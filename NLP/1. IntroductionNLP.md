# Введение в NLP: лингвистические основы и архитектура обработки языка

Представьте, что вы открываете книгу на незнакомом языке. Первое, что вы замечаете — это символы (буквы, иероглифы). Потом понимаете, что символы складываются в слова, слова в предложения, предложения передают смысл. Если бы вы учили этот язык, вы бы проходили ровно те же уровни: алфавит, грамматика, синтаксис, значение слов, контекст использования.

Natural Language Processing — это попытка научить компьютер понимать язык так же, как мы. Но если человек учится интуитивно через миллионы примеров, то машина должна иметь чёткую структуру: правила, модели, данные. И вот здесь начинается самое интересное.

---

## Что такое естественный язык и почему он сложен

**Естественный язык** — это система коммуникации, выработанная людьми естественным путем, в отличие от искусственных языков (программирования, математики). Это текст, речь, жесты — любая последовательность символов или звуков, имеющая смысл.

Пример простого предложения: *"Мама мыла раму"*.

Разложим его:
- **Токены:** `{Мама, мыла, раму}` — отдельные значимые единицы
- **Морфология:** *мыла* — глагол прошедшего времени женского рода
- **Синтаксис:** подлежащее (*Мама*) + сказуемое (*мыла*) + дополнение (*раму*)
- **Семантика:** описание действия в прошлом

Кажется просто? Теперь усложним: *"Мама мыла окно в саду"*. Что в саду — окно или процесс мытья? Человек понимает из контекста, но компьютер должен явно разрешить эту неоднозначность.

> [!IMPORTANT]
> Язык — это не просто набор правил. Это динамическая система, где смысл зависит от контекста, культуры, времени и даже интонации. Задача NLP — формализовать эту сложность так, чтобы машина могла работать с ней алгоритмически.

---

## Уровни языковой обработки: от символов к смыслу

Каждая задача NLP опирается на правила разных уровней. Это как слои луковицы: каждый следующий уровень строится на предыдущем.

### 1. Графематический уровень

**Задача:** разделить текст на слова, предложения, абзацы.

Звучит тривиально? Попробуйте разделить китайский текст без пробелов: "今天天气很好" — где границы слов? Или немецкое слово *Donaudampfschifffahrtsgesellschaftskapitän* (капитан дунайского пароходства) — это одно слово или несколько?

**Примеры:**
- Сегментация предложений (sentence boundary detection)
- Токенизация (разбиение на слова)
- Обработка аббревиатур, чисел, дат, URL

```python
# Простейшая токенизация по пробелам
text = "Привет, мир! Как дела?"
tokens = text.split()
# ['Привет,', 'мир!', 'Как', 'дела?']
# Проблема: пунктуация слилась со словами
```

### 2. Морфологический уровень

**Задача:** понять структуру слова и его грамматические характеристики.

Морфология изучает, как слова изменяются: склонение, спряжение, префиксы, суффиксы.

Возьмём слово *"бежали"*:
- Лемма (базовая форма): *бежать*
- Часть речи: глагол
- Время: прошедшее
- Число: множественное

Для английского это проще (*running* → *run*), для русского сложнее (16 падежей, спряжения, рода). Для финского или турецкого (агглютинативные языки) — ещё сложнее.

**Основные задачи:**
- **Лемматизация:** *конца* → *конец*, *бежали* → *бежать*
- **Стеммирование:** грубое отсечение окончаний (*конца* → *кон*)
- **POS-tagging:** определение части речи (noun, verb, adjective)

> [!NOTE]
> Для русского языка морфология критична. Словоформа "банка" может быть существительным (тара), формой слова "банк" (финансовое учреждение) или глаголом (от "банить"). Без морфологического анализа невозможно понять смысл.

### 3. Синтаксический уровень

**Задача:** понять структуру предложения — как слова связаны друг с другом.

Два основных подхода:
- **Dependency parsing (зависимости):** дерево связей между словами
- **Constituency parsing (составляющие):** иерархическое разбиение на фразы

Пример: *"Кот сидел на окне"*

Зависимости:
```
сидел (ROOT)
  ├─ кот (subject)
  └─ на (preposition)
      └─ окне (object)
```

Почему это важно? Потому что синтаксис помогает понять **кто**, **что делает**, **с чем**. В вопросно-ответных системах без этого не обойтись.

**Сложности:**
- В русском языке свободный порядок слов: *"Мама мыла раму"* = *"Раму мыла мама"*
- В английском фиксированный: *"The cat ate the mouse"* ≠ *"The mouse ate the cat"*

### 4. Семантический уровень

**Задача:** извлечь смысл, а не только форму.

На этом уровне мы понимаем:
- **WSD (Word Sense Disambiguation):** какое значение слова имеется в виду?
  - *bank* = берег или банк?
  - *коса* = инструмент, причёска или ландшафт?

- **NER (Named Entity Recognition):** выделить имена, даты, организации
  - *"Иван Петров работает в Google с 2020 года"*
  - → `[PERSON: Иван Петров]`, `[ORG: Google]`, `[DATE: 2020]`

- **SRL (Semantic Role Labeling):** кто агент, пациент, инструмент?
  - *"Мама (агент) мыла (действие) раму (пациент) губкой (инструмент)"*

**Формула WSD (классический подход):**

$$\hat{s} = \arg\max_{s_i \in S} P(s_i \mid \text{context})$$

где $s_i$ — один из возможных смыслов слова, $\text{context}$ — соседние слова, синтаксические связи, глобальные признаки документа.

### 5. Прагматический уровень

**Задача:** понять намерение и контекст использования.

Это высший уровень. Здесь мы отвечаем на вопросы:
- Какова цель высказывания? (утверждение, вопрос, просьба, угроза)
- Что автор хочет сообщить неявно? (сарказм, ирония)
- Какой стиль речи? (формальный, неформальный, технический)

Пример:
- *"Не могли бы вы закрыть окно?"* — грамматически вопрос, прагматически — просьба.
- *"Отличная работа!"* — может быть похвалой или саркастической критикой (зависит от контекста).

---

## Классификация задач NLP: три больших направления

Все задачи NLP можно разделить на три категории:

### 1. Анализ (Understanding)

**Цель:** извлечь информацию и понять текст.

**Примеры задач:**
- **Классификация текста:** спам/не-спам, тема новости, язык документа
- **Sentiment Analysis:** позитивный/негативный/нейтральный отзыв
- **NER:** извлечение имён, дат, организаций
- **Question Answering:** система отвечает на вопросы по тексту
- **POS-tagging и синтаксический парсинг**

### 2. Извлечение информации (Information Extraction)

**Цель:** структурировать неструктурированный текст.

**Примеры:**
- **Relation Extraction:** кто с кем связан?
  - *"Илон Маск основал SpaceX"* → `[Илон Маск, founded, SpaceX]`
- **Event Extraction:** что произошло, когда, где?
- **Knowledge Graph Construction:** построение графа знаний из текстов
- **Table/Form Filling:** автоматическое заполнение полей из документов

### 3. Генерация (Generation)

**Цель:** создать новый текст.

**Примеры:**
- **Machine Translation:** перевод с одного языка на другой
- **Summarization:** автоматическое резюмирование документов
- **Text Generation:** дописывание текста, генерация описаний
- **Dialogue Systems:** чат-боты, голосовые ассистенты
- **Data-to-Text:** генерация отчётов из таблиц

> [!TIP]
> При выборе задачи для изучения начинайте с анализа (классификация, NER). Это фундамент. Генерация — самая сложная категория, требует глубокого понимания всех уровней языка.

---

## Лингвистические явления, усложняющие обработку

Почему NLP так сложен? Потому что язык полон исключений, неоднозначностей и контекстных зависимостей.

### 1. Омонимия и полисемия

**Омонимия:** одна форма, разные несвязанные значения.
- *bank*: берег реки vs финансовое учреждение
- *коса*: инструмент vs причёска vs ландшафт

**Полисемия:** одно слово, несколько связанных значений.
- *ручка*: часть двери, письменный инструмент, детская рука

Разрешение требует контекста: соседние слова, тема документа, синтаксические связи.

### 2. Анафора и кореференция

**Анафора:** местоимения, указывающие на предыдущие упоминания.

*"Маша пришла в офис. Она села за стол."* — *Она* = Маша.

*"Компания объявила результаты. Её акции выросли."* — *Её* = компании.

Система должна связать местоимение с упоминаемой сущностью (**coreference resolution**).

### 3. Многословные выражения (MWE)

Фразы, которые нужно трактовать как единое целое:
- *по крайней мере*, *в конце концов* (русский)
- *in spite of*, *with respect to* (английский)

Обычная токенизация разобьёт их на отдельные слова, потеряв смысл.

### 4. Порядок слов и морфологическое богатство

**Фиксированный порядок (английский):**
- *"The dog bites the man"* ≠ *"The man bites the dog"*

**Свободный порядок (русский):**
- *"Собака укусила человека"* = *"Человека укусила собака"* (акцент меняется, смысл нет)

**Морфологически богатые языки** (русский, финский, турецкий) имеют десятки форм одного слова, что требует сложной лемматизации и подсловной токенизации.

### 5. Кросс-лингвальные различия

**Агглютинация (турецкий, финский):**
- Слово = корень + цепочка аффиксов
- Одно слово может быть целым предложением

**Отсутствие пробелов (китайский, японский):**
- Сегментация слов — отдельная нетривиальная задача

**Код-свитчинг (переключение языков внутри предложения):**
- *"Я хочу coffee и немного работы"*
- Требует многоязычных моделей

> [!WARNING]
> Не существует универсального пайплайна для всех языков! То, что работает для английского, может полностью сломаться на китайском или арабском. Всегда учитывайте специфику языка при проектировании системы.

---

## Пайплайн обработки текста: от сырых данных к результату

Любая NLP-задача проходит через несколько этапов обработки. Это **pipeline** (конвейер).

### Классический пайплайн (модульный подход)

```
[Сырой текст]
     ↓
[Сегментация предложений]
     ↓
[Токенизация]
     ↓
[Морфологический анализ: POS, лемматизация]
     ↓
[Синтаксический парсинг]
     ↓
[Семантический анализ: NER, WSD]
     ↓
[Агрегация и извлечение признаков]
     ↓
[Решение задачи: классификация / генерация / извлечение]
```

**Пример:**

Входной текст: *"Мама мыла раму. Потом мы пошли гулять."*

1. **Сегментация:** `["Мама мыла раму.", "Потом мы пошли гулять."]`
2. **Токенизация:** `["Мама", "мыла", "раму", ".", "Потом", "мы", "пошли", "гулять", "."]`
3. **Морфология:** `[("Мама", NOUN), ("мыла", VERB), ("раму", NOUN), ...]`
4. **Лемматизация:** `["мама", "мыть", "рама", ...]`
5. **Синтаксис:** построение дерева зависимостей
6. **Задача:** например, извлечь субъект-действие-объект → `(Мама, мыла, раму)`

### Современный end-to-end подход

С появлением глубокого обучения (особенно трансформеров) многие этапы объединены в одну модель.

**Например, BERT:**
- На вход: токены текста
- На выход: представления, которые можно использовать для любой задачи

Преимущества:
- Меньше ошибок распространения между этапами
- Лучшая производительность за счёт совместного обучения

Недостатки:
- Меньше интерпретируемости
- Требуется больше данных и вычислений

> [!NOTE]
> В промышленных системах часто используют гибридный подход: классические модули (токенизация, морфология) + нейросетевые компоненты (эмбеддинги, классификаторы). Это даёт баланс между качеством, скоростью и интерпретируемостью.

---

## Кросс-лингвальные особенности: почему язык имеет значение

Языки сильно различаются по структуре, и это влияет на выбор методов.

### Типология языков

| Тип | Примеры | Особенности | Влияние на NLP |
|-----|---------|-------------|----------------|
| Изолирующие | Китайский, вьетнамский | Нет словоизменения, фиксированный порядок | Сложная сегментация слов |
| Флективные | Русский, латынь | Много форм слова, свободный порядок | Требуется мощная морфология |
| Агглютинативные | Турецкий, финский | Цепочки аффиксов | Подсловная токенизация критична |
| Аналитические | Английский | Мало форм, фиксированный порядок | Проще всего для базовых методов |

### Практические последствия

**Для русского языка:**
- Обязательна качественная лемматизация (pymorphy2, mystem)
- Токенизация должна учитывать составные слова
- Word embeddings выигрывают от подсловных методов (FastText)

**Для китайского:**
- Первый этап — word segmentation (jieba, Stanford Segmenter)
- Иероглифы несут больше информации, чем латинские буквы
- Character-level модели часто эффективны

**Для английского:**
- Относительно простая морфология
- Большинство открытых датасетов и моделей
- Хороший starting point для изучения

> [!TIP]
> При работе с низкоресурсными языками используйте мультиязычные модели (mBERT, XLM-R) и transfer learning с английского. Это даёт существенный буст качества даже при малом количестве размеченных данных.

---

## Практика: проектирование пайплайна

Давайте спроектируем простой пайплайн для классификации новостей на русском языке.

**Задача:** определить тему новости (политика, спорт, технологии, культура).

### Шаг 1: Препроцессинг

```python
import re
from nltk.tokenize import sent_tokenize, word_tokenize
from pymystem3 import Mystem

# Инициализация лемматизатора
mystem = Mystem()

def preprocess(text):
    # 1. Очистка
    text = re.sub(r'http\S+', '', text)  # удалить URL
    text = re.sub(r'[^\w\s]', ' ', text)  # удалить пунктуацию
    text = text.lower()
    
    # 2. Токенизация и лемматизация
    lemmas = mystem.lemmatize(text)
    lemmas = [l for l in lemmas if l.strip() and not l.isspace()]
    
    return lemmas

text = "Президент России подписал указ о новой реформе."
print(preprocess(text))
# ['президент', 'россия', 'подписать', 'указ', 'о', 'новый', 'реформа']
```

### Шаг 2: Векторизация

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Корпус новостей (пример)
corpus = [
    "президент подписать указ реформа",
    "спортсмен выиграть золотой медаль олимпиада",
    "новый смартфон выйти рынок технология",
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
```

### Шаг 3: Классификация

```python
from sklearn.linear_model import LogisticRegression

# Обучение (в реальности нужна размеченная выборка)
y = [0, 1, 2]  # 0=политика, 1=спорт, 2=технологии
clf = LogisticRegression()
clf.fit(X, y)

# Предсказание
new_text = preprocess("футболист забить гол")
new_vec = vectorizer.transform([" ".join(new_text)])
print(clf.predict(new_vec))  # [1] = спорт
```

### Шаг 4: Оценка и мониторинг

```python
from sklearn.metrics import classification_report

# На тестовой выборке
y_true = [0, 1, 2, 0]
y_pred = clf.predict(X_test)
print(classification_report(y_true, y_pred))
```

> [!IMPORTANT]
> Этот пример упрощён для демонстрации. В production системе нужны: обработка OOV слов, кэширование лемматизации, батчинг, версионирование модели, мониторинг drift, A/B тестирование изменений.

---

## Когда использовать классические vs нейросетевые методы

### Классический пайплайн (правила + ML)

**Подходит когда:**
- Малые данные (< 10k примеров)
- Нужна интерпретируемость
- Строгие требования к латентности
- Специфичный домен с явными правилами

**Инструменты:** regex, spaCy, NLTK, scikit-learn, pymorphy2

### Нейросетевой подход (трансформеры, LLMs)

**Подходит когда:**
- Большие данные (> 100k примеров)
- Нужна максимальная точность
- Сложные задачи (генерация, QA)
- Доступны GPU для инференса

**Инструменты:** BERT, GPT, T5, Transformers (Hugging Face)

### Гибридный подход (рекомендуется)

- Классическая препроцессинг (токенизация, очистка)
- Нейросетевые эмбеддинги
- Классические или нейросетевые головы в зависимости от задачи
- Правила для постобработки и валидации

---

## Резюме

NLP — это многоуровневая дисциплина, где каждая задача требует понимания лингвистических основ. Мы прошли от простых символов до сложных семантических и прагматических задач.

**Ключевые выводы:**
- Язык структурирован по уровням: графематика → морфология → синтаксис → семантика → прагматика
- Задачи NLP делятся на анализ, извлечение информации и генерацию
- Лингвистические явления (омонимия, анафора, MWE) требуют контекстного анализа
- Разные языки нуждаются в разных подходах
- Пайплайн обработки — это фундамент любой NLP-системы

В следующих лекциях мы подробно разберём каждый этап: токенизацию, эмбеддинги, модели и практические задачи. Пока же запомните: **понимание языковой структуры — это ключ к построению эффективных NLP-систем**.