{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Практика: Введение в NLP\n",
        "\n",
        "В этой тетради мы пройдём основные концепции NLP на практике:\n",
        "- Токенизация\n",
        "- Морфологический анализ\n",
        "- Построение пайплайна\n",
        "- Классификация текстов\n",
        "\n",
        "Убедитесь, что установлены необходимые библиотеки:\n",
        "```\n",
        "pip install nltk pymystem3 pymorphy2 scikit-learn\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Токенизация и сегментация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Примеры текстов\n",
        "text_ru = \"\"\"Привет, мир! Это первый пример. NLP очень интересен.\n",
        "Машинное обучение меняет нашу жизнь.\"\"\"\n",
        "\n",
        "text_en = \"\"\"Hello, world! This is the first example. NLP is very interesting.\n",
        "Machine learning is changing our lives.\"\"\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"СЕГМЕНТАЦИЯ НА ПРЕДЛОЖЕНИЯ\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nРусский текст:\")\n",
        "sentences_ru = sent_tokenize(text_ru, language='russian')\n",
        "for i, sent in enumerate(sentences_ru, 1):\n",
        "    print(f\"{i}. {sent}\")\n",
        "\n",
        "print(\"\\nАнглийский текст:\")\n",
        "sentences_en = sent_tokenize(text_en, language='english')\n",
        "for i, sent in enumerate(sentences_en, 1):\n",
        "    print(f\"{i}. {sent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ТОКЕНИЗАЦИЯ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Простая токенизация по пробелам\n",
        "simple_tokens = text_ru.split()\n",
        "print(f\"\\nПростая токенизация (split):\")\n",
        "print(f\"Токены: {simple_tokens}\")\n",
        "print(f\"Количество: {len(simple_tokens)}\")\n",
        "print(\"⚠️ Проблема: пунктуация слилась со словами\")\n",
        "\n",
        "# Более правильная токенизация\n",
        "print(\"\\nТокенизация с разделением пунктуации:\")\n",
        "tokens_ru = word_tokenize(text_ru, language='russian')\n",
        "print(f\"Токены: {tokens_ru[:20]}...\")  # Первые 20\n",
        "print(f\"Количество: {len(tokens_ru)}\")\n",
        "\n",
        "print(\"\\nАнглийская токенизация:\")\n",
        "tokens_en = word_tokenize(text_en)\n",
        "print(f\"Токены: {tokens_en[:20]}...\")\n",
        "print(f\"Количество: {len(tokens_en)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пользовательская токенизация с regex\n",
        "print(\"\\nОсовещение токенизации с regex:\")\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    # Разделяем по пробелам, но сохраняем пунктуацию\n",
        "    # Паттерн: слово или одиночный символ\n",
        "    pattern = r\"\\b\\w+\\b|[.,!?;:-]\"\n",
        "    tokens = re.findall(pattern, text, re.UNICODE)\n",
        "    return tokens\n",
        "\n",
        "custom_tokens = custom_tokenize(text_ru.lower())\n",
        "print(f\"Custom токены: {custom_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Морфологический анализ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Для русского языка используем pymorphy2\n",
        "import pymorphy2\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Примеры слов\n",
        "words = ['бежали', 'конца', 'банка', 'красивая', 'работают']\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ЛЕММАТИЗАЦИЯ И МОРФОЛОГИЧЕСКИЙ АНАЛИЗ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for word in words:\n",
        "    p = morph.parse(word)[0]\n",
        "    print(f\"\\nСлово: '{word}'\")\n",
        "    print(f\"  Лемма: {p.normal_form}\")\n",
        "    print(f\"  Часть речи: {p.tag.POS}\")\n",
        "    print(f\"  Грамм. признаки: {p.tag}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Лемматизация полного текста\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ЛЕММАТИЗАЦИЯ ТЕКСТА\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    # Токенизируем\n",
        "    tokens = word_tokenize(text.lower(), language='russian')\n",
        "    # Лемматизируем\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():  # Только буквы\n",
        "            parsed = morph.parse(token)[0]\n",
        "            lemmas.append(parsed.normal_form)\n",
        "        else:\n",
        "            lemmas.append(token)  # Числа, пунктуация как есть\n",
        "    return lemmas\n",
        "\n",
        "original = \"Мама мыла раму. Дети бежали на площадку.\"\n",
        "lemmatized = lemmatize_text(original)\n",
        "\n",
        "print(f\"\\nОригинал: {original}\")\n",
        "print(f\"Леммы: {' '.join(lemmatized)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Анализ одной словоформы с несколькими вариантами\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ОМОНИМИЯ: СЛОВО С НЕСКОЛЬКИМИ ЗНАЧЕНИЯМИ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "ambiguous_word = 'банка'\n",
        "parses = morph.parse(ambiguous_word)\n",
        "\n",
        "print(f\"\\nВсе возможные разборы слова '{ambiguous_word}':\")\n",
        "for i, p in enumerate(parses, 1):\n",
        "    print(f\"{i}. Лемма: {p.normal_form:15} | Часть речи: {p.tag.POS:10} | Полный тег: {p.tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Построение простого пайплайна"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ПОЛНЫЙ ПАЙПЛАЙН ПРЕДОБРАБОТКИ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class TextPipeline:\n",
        "    def __init__(self, language='russian'):\n",
        "        self.language = language\n",
        "        self.morph = pymorphy2.MorphAnalyzer()\n",
        "        if language == 'russian':\n",
        "            self.stop_words = stopwords.words('russian')\n",
        "        else:\n",
        "            self.stop_words = stopwords.words('english')\n",
        "    \n",
        "    def process(self, text):\n",
        "        \"\"\"Полная обработка текста\"\"\"\n",
        "        # 1. Сегментация на предложения\n",
        "        sentences = sent_tokenize(text, language=self.language)\n",
        "        \n",
        "        result = {\n",
        "            'original': text,\n",
        "            'sentences': sentences,\n",
        "            'tokens': [],\n",
        "            'lemmas': [],\n",
        "            'cleaned_tokens': []\n",
        "        }\n",
        "        \n",
        "        # 2. Токенизация и лемматизация\n",
        "        all_tokens = word_tokenize(text.lower(), language=self.language)\n",
        "        result['tokens'] = all_tokens\n",
        "        \n",
        "        # 3. Лемматизация\n",
        "        lemmas = []\n",
        "        for token in all_tokens:\n",
        "            if token.isalpha():\n",
        "                lemma = self.morph.parse(token)[0].normal_form\n",
        "                lemmas.append(lemma)\n",
        "            else:\n",
        "                lemmas.append(token)\n",
        "        result['lemmas'] = lemmas\n",
        "        \n",
        "        # 4. Удаление стоп-слов и пунктуации\n",
        "        cleaned = [w for w in lemmas if w.isalpha() and w not in self.stop_words]\n",
        "        result['cleaned_tokens'] = cleaned\n",
        "        \n",
        "        return result\n",
        "\n",
        "# Использование\n",
        "pipeline = TextPipeline('russian')\n",
        "\n",
        "test_text = \"\"\"Машинное обучение и обработка естественного языка — это мощные инструменты.\n",
        "Они помогают нам понимать и анализировать текст.\"\"\"\n",
        "\n",
        "result = pipeline.process(test_text)\n",
        "\n",
        "print(f\"\\nОригинальный текст:\\n{result['original']}\")\n",
        "print(f\"\\nПредложений: {len(result['sentences'])}\")\n",
        "for i, sent in enumerate(result['sentences'], 1):\n",
        "    print(f\"  {i}. {sent}\")\n",
        "\n",
        "print(f\"\\nВсего токенов: {len(result['tokens'])}\")\n",
        "print(f\"Первые 10 токенов: {result['tokens'][:10]}\")\n",
        "\n",
        "print(f\"\\nОчищенные токены (без стоп-слов): {len(result['cleaned_tokens'])}\")\n",
        "print(f\"Токены: {result['cleaned_tokens']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Классификация текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"КЛАССИФИКАЦИЯ НОВОСТЕЙ НА РУССКОМ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Синтетический корпус для демонстрации\n",
        "# В реальности нужна большая размеченная выборка\n",
        "train_texts = [\n",
        "    \"Президент подписал новый указ о реформе образования\",\n",
        "    \"Правительство приняло бюджет на следующий год\",\n",
        "    \"Депутаты голосовали за законопроект\",\n",
        "    \"Российская сборная выиграла золотую медаль на олимпиаде\",\n",
        "    \"Спортсмены готовятся к чемпионату мира\",\n",
        "    \"Футболист забил решающий гол в матче\",\n",
        "    \"Новая модель смартфона выходит на рынок\",\n",
        "    \"Компания объявила о разработке искусственного интеллекта\",\n",
        "    \"Ученые создали инновационную технологию\",\n",
        "]\n",
        "\n",
        "train_labels = [\n",
        "    0, 0, 0,  # политика\n",
        "    1, 1, 1,  # спорт\n",
        "    2, 2, 2,  # технология\n",
        "]\n",
        "\n",
        "# Векторизация\n",
        "vectorizer = TfidfVectorizer(lowercase=True, analyzer='char', ngram_range=(2, 3))\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "print(f\"\\nМатрица TF-IDF формы: {X_train.shape}\")\n",
        "print(f\"Число признаков: {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "# Обучение классификатора\n",
        "clf = LogisticRegression(random_state=42, max_iter=200)\n",
        "clf.fit(X_train, train_labels)\n",
        "\n",
        "print(\"\\nМодель обучена!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Тестирование\n",
        "test_texts = [\n",
        "    \"Депутаты обсуждают новый закон\",  # Политика\n",
        "    \"Спортсмен выиграл чемпионат\",     # Спорт\n",
        "    \"Новая технология в смартфоне\",    # Технология\n",
        "]\n",
        "\n",
        "X_test = vectorizer.transform(test_texts)\n",
        "predictions = clf.predict(X_test)\n",
        "probabilities = clf.predict_proba(X_test)\n",
        "\n",
        "label_names = ['Политика', 'Спорт', 'Технология']\n",
        "\n",
        "print(\"\\nПредсказания для тестовых текстов:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for text, pred, proba in zip(test_texts, predictions, probabilities):\n",
        "    print(f\"\\nТекст: {text}\")\n",
        "    print(f\"Предсказание: {label_names[pred]} (уверенность: {proba[pred]:.2%})\")\n",
        "    print(f\"Вероятности по классам:\")\n",
        "    for label, prob in zip(label_names, proba):\n",
        "        bar = '█' * int(prob * 30)\n",
        "        print(f\"  {label:15} {prob:6.2%} {bar}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# На обучающей выборке\n",
        "predictions_train = clf.predict(X_train)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"МЕТРИКИ КАЧЕСТВА НА ОБУЧАЮЩЕЙ ВЫБОРКЕ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nОбщая точность: {(predictions_train == train_labels).mean():.2%}\")\n",
        "print(\"\\nОтчёт по классам:\")\n",
        "print(classification_report(train_labels, predictions_train, target_names=label_names))\n",
        "\n",
        "print(\"\\nМатрица ошибок:\")\n",
        "cm = confusion_matrix(train_labels, predictions_train)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Практические задачи"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ЗАДАЧА 1: Анализ сложности омонимии\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Найдите слова, которые имеют несколько разборов\n",
        "ambiguous_words = ['коса', 'замок', 'мука', 'хлеб', 'роман']\n",
        "\n",
        "print(\"\\nАнализ омонимичных слов:\")\n",
        "for word in ambiguous_words:\n",
        "    parses = morph.parse(word)\n",
        "    print(f\"\\n'{word}': {len(parses)} вариант(ов)\")\n",
        "    for i, p in enumerate(parses[:3], 1):  # Первые 3\n",
        "        print(f\"  {i}. {p.normal_form} ({p.tag.POS})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ЗАДАЧА 2: Сравнение токенизаций\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "complex_text = \"Mr. Smith e-mailed the file at 5:00 p.m. to john@example.com\"\n",
        "\n",
        "# Попробуем разные подходы\n",
        "simple = complex_text.split()\n",
        "nltk_tokens = word_tokenize(complex_text)\n",
        "\n",
        "print(f\"\\nТекст: {complex_text}\")\n",
        "print(f\"\\nПростая токенизация (split): {len(simple)} токенов\")\n",
        "print(simple)\n",
        "print(f\"\\nNLTK токенизация: {len(nltk_tokens)} токенов\")\n",
        "print(nltk_tokens)\n",
        "print(\"\\nЗамечание: NLTK лучше разделяет пунктуацию\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ЗАДАЧА 3: Создание собственного токенизатора\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def advanced_tokenize(text):\n",
        "    \"\"\"\n",
        "    Продвинутая токенизация с обработкой:\n",
        "    - Email адресов\n",
        "    - URL\n",
        "    - Хэштегов\n",
        "    - Чисел и денежных сумм\n",
        "    \"\"\"\n",
        "    # Сохраняем специальные токены\n",
        "    tokens = []\n",
        "    \n",
        "    # Паттерны\n",
        "    patterns = [\n",
        "        (r'https?://\\S+', 'URL'),\n",
        "        (r'\\S+@\\S+', 'EMAIL'),\n",
        "        (r'#\\w+', 'HASHTAG'),\n",
        "        (r'\\$?\\d+[.,]\\d+', 'NUMBER'),\n",
        "        (r'\\b\\w+\\b', 'WORD'),\n",
        "        (r'[.,:;!?]', 'PUNCT'),\n",
        "    ]\n",
        "    \n",
        "    from itertools import chain\n",
        "    combined_pattern = '|'.join(f'(?P<{name}>{pat})' for pat, name in patterns)\n",
        "    \n",
        "    for match in re.finditer(combined_pattern, text):\n",
        "        token = match.group()\n",
        "        token_type = match.lastgroup\n",
        "        tokens.append((token, token_type))\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "test = \"Check my email at john@example.com or visit https://example.com. Price is $19.99 #NLP\"\n",
        "result = advanced_tokenize(test)\n",
        "\n",
        "print(f\"\\nТекст: {test}\")\n",
        "print(f\"\\nТокены с типами:\")\n",
        "for token, token_type in result:\n",
        "    print(f\"  {token:20} → {token_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ЗАДАЧА 4: Влияние стоп-слов на классификацию\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_text = \"Это очень интересный и важный документ о машинном обучении\"\n",
        "\n",
        "tokens = word_tokenize(sample_text.lower(), language='russian')\n",
        "stop_words_ru = stopwords.words('russian')\n",
        "\n",
        "print(f\"\\nОригинальные токены: {[t for t in tokens if t.isalpha()]}\")\n",
        "\n",
        "# С удалением стоп-слов\n",
        "filtered = [t for t in tokens if t.isalpha() and t not in stop_words_ru]\n",
        "print(f\"Без стоп-слов: {filtered}\")\n",
        "\n",
        "removed = [t for t in tokens if t.isalpha() and t in stop_words_ru]\n",
        "print(f\"\\nУдалены стоп-слова: {removed}\")\n",
        "print(f\"\\nОригинальных слов: {sum(1 for t in tokens if t.isalpha())}\")\n",
        "print(f\"Значимых слов: {len(filtered)}\")\n",
        "print(f\"Удалено: {len(removed)} ({len(removed)/sum(1 for t in tokens if t.isalpha()):.1%})\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
