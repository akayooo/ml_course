{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Практика: Введение в NLP\n",
        "\n",
        "В этом ноутбуке мы пройдём основные концепции NLP на практике:\n",
        "- Токенизация\n",
        "- Морфологический анализ\n",
        "- Построение пайплайна\n",
        "- Классификация текстов\n",
        "\n",
        "Убедитесь, что установлены необходимые библиотеки:\n",
        "```\n",
        "pip install nltk pymystem3 pymorphy2 scikit-learn\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Токенизация и сегментация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "СЕГМЕНТАЦИЯ НА ПРЕДЛОЖЕНИЯ\n",
            "============================================================\n",
            "\n",
            "Русский текст:\n",
            "1. Привет, мир!\n",
            "2. Это первый пример.\n",
            "3. NLP очень интересен.\n",
            "4. Машинное обучение меняет нашу жизнь.\n",
            "\n",
            "Английский текст:\n",
            "1. Hello, world!\n",
            "2. This is the first example.\n",
            "3. NLP is very interesting.\n",
            "4. Machine learning is changing our lives.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Примеры текстов\n",
        "text_ru = \"\"\"Привет, мир! Это первый пример. NLP очень интересен.\n",
        "Машинное обучение меняет нашу жизнь.\"\"\"\n",
        "\n",
        "text_en = \"\"\"Hello, world! This is the first example. NLP is very interesting.\n",
        "Machine learning is changing our lives.\"\"\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"СЕГМЕНТАЦИЯ НА ПРЕДЛОЖЕНИЯ\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nРусский текст:\")\n",
        "sentences_ru = sent_tokenize(text_ru, language='russian')\n",
        "for i, sent in enumerate(sentences_ru, 1):\n",
        "    print(f\"{i}. {sent}\")\n",
        "\n",
        "print(\"\\nАнглийский текст:\")\n",
        "sentences_en = sent_tokenize(text_en, language='english')\n",
        "for i, sent in enumerate(sentences_en, 1):\n",
        "    print(f\"{i}. {sent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ТОКЕНИЗАЦИЯ\n",
            "============================================================\n",
            "\n",
            "Простая токенизация (split):\n",
            "Токены: ['Привет,', 'мир!', 'Это', 'первый', 'пример.', 'NLP', 'очень', 'интересен.', 'Машинное', 'обучение', 'меняет', 'нашу', 'жизнь.']\n",
            "Количество: 13\n",
            "⚠️ Проблема: пунктуация слилась со словами\n",
            "\n",
            "Токенизация с разделением пунктуации:\n",
            "Токены: ['Привет', ',', 'мир', '!', 'Это', 'первый', 'пример', '.', 'NLP', 'очень', 'интересен', '.', 'Машинное', 'обучение', 'меняет', 'нашу', 'жизнь', '.']...\n",
            "Количество: 18\n",
            "\n",
            "Английская токенизация:\n",
            "Токены: ['Hello', ',', 'world', '!', 'This', 'is', 'the', 'first', 'example', '.', 'NLP', 'is', 'very', 'interesting', '.', 'Machine', 'learning', 'is', 'changing', 'our']...\n",
            "Количество: 22\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ТОКЕНИЗАЦИЯ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Простая токенизация по пробелам\n",
        "simple_tokens = text_ru.split()\n",
        "print(f\"\\nПростая токенизация (split):\")\n",
        "print(f\"Токены: {simple_tokens}\")\n",
        "print(f\"Количество: {len(simple_tokens)}\")\n",
        "print(\"⚠️ Проблема: пунктуация слилась со словами\")\n",
        "\n",
        "# Более правильная токенизация\n",
        "print(\"\\nТокенизация с разделением пунктуации:\")\n",
        "tokens_ru = word_tokenize(text_ru, language='russian')\n",
        "print(f\"Токены: {tokens_ru[:20]}...\")  # Первые 20\n",
        "print(f\"Количество: {len(tokens_ru)}\")\n",
        "\n",
        "print(\"\\nАнглийская токенизация:\")\n",
        "tokens_en = word_tokenize(text_en)\n",
        "print(f\"Токены: {tokens_en[:20]}...\")\n",
        "print(f\"Количество: {len(tokens_en)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Осовещение токенизации с regex:\n",
            "Custom токены: ['привет', ',', 'мир', '!', 'это', 'первый', 'пример', '.', 'nlp', 'очень', 'интересен', '.', 'машинное', 'обучение', 'меняет', 'нашу', 'жизнь', '.']\n"
          ]
        }
      ],
      "source": [
        "# Пользовательская токенизация с regex\n",
        "print(\"\\nОсовещение токенизации с regex:\")\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    # Разделяем по пробелам, но сохраняем пунктуацию\n",
        "    # Паттерн: слово или одиночный символ\n",
        "    pattern = r\"\\b\\w+\\b|[.,!?;:-]\"\n",
        "    tokens = re.findall(pattern, text, re.UNICODE)\n",
        "    return tokens\n",
        "\n",
        "custom_tokens = custom_tokenize(text_ru.lower())\n",
        "print(f\"Custom токены: {custom_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Морфологический анализ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ЛЕММАТИЗАЦИЯ И МОРФОЛОГИЧЕСКИЙ АНАЛИЗ\n",
            "============================================================\n",
            "\n",
            "Слово: 'бежали'\n",
            "  Лемма: бежать\n",
            "  Часть речи: VERB\n",
            "  Грамм. признаки: VERB,perf,intr plur,past,indc\n",
            "\n",
            "Слово: 'конца'\n",
            "  Лемма: конец\n",
            "  Часть речи: NOUN\n",
            "  Грамм. признаки: NOUN,inan,masc sing,gent\n",
            "\n",
            "Слово: 'банка'\n",
            "  Лемма: банк\n",
            "  Часть речи: NOUN\n",
            "  Грамм. признаки: NOUN,inan,masc sing,gent\n",
            "\n",
            "Слово: 'красивая'\n",
            "  Лемма: красивый\n",
            "  Часть речи: ADJF\n",
            "  Грамм. признаки: ADJF,Qual femn,sing,nomn\n",
            "\n",
            "Слово: 'работают'\n",
            "  Лемма: работать\n",
            "  Часть речи: VERB\n",
            "  Грамм. признаки: VERB,impf,intr plur,3per,pres,indc\n"
          ]
        }
      ],
      "source": [
        "# Для русского языка используем pymorphy2\n",
        "import pymorphy2\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Примеры слов\n",
        "words = ['бежали', 'конца', 'банка', 'красивая', 'работают']\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ЛЕММАТИЗАЦИЯ И МОРФОЛОГИЧЕСКИЙ АНАЛИЗ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for word in words:\n",
        "    p = morph.parse(word)[0]\n",
        "    print(f\"\\nСлово: '{word}'\")\n",
        "    print(f\"  Лемма: {p.normal_form}\")\n",
        "    print(f\"  Часть речи: {p.tag.POS}\")\n",
        "    print(f\"  Грамм. признаки: {p.tag}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ЛЕММАТИЗАЦИЯ ТЕКСТА\n",
            "============================================================\n",
            "\n",
            "Оригинал: Мама мыла раму. Дети бежали на площадку.\n",
            "Леммы: мама мыло рама . ребёнок бежать на площадка .\n"
          ]
        }
      ],
      "source": [
        "# Лемматизация полного текста\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ЛЕММАТИЗАЦИЯ ТЕКСТА\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    # Токенизируем\n",
        "    tokens = word_tokenize(text.lower(), language='russian')\n",
        "    # Лемматизируем\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():  # Только буквы\n",
        "            parsed = morph.parse(token)[0]\n",
        "            lemmas.append(parsed.normal_form)\n",
        "        else:\n",
        "            lemmas.append(token)  # Числа, пунктуация как есть\n",
        "    return lemmas\n",
        "\n",
        "original = \"Мама мыла раму. Дети бежали на площадку.\"\n",
        "lemmatized = lemmatize_text(original)\n",
        "\n",
        "print(f\"\\nОригинал: {original}\")\n",
        "print(f\"Леммы: {' '.join(lemmatized)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ОМОНИМИЯ: СЛОВО С НЕСКОЛЬКИМИ ЗНАЧЕНИЯМИ\n",
            "============================================================\n",
            "\n",
            "Все возможные разборы слова 'банка':\n",
            "1. Лемма: банк            | Часть речи: NOUN       | Полный тег: NOUN,inan,masc sing,gent\n",
            "2. Лемма: банка           | Часть речи: NOUN       | Полный тег: NOUN,inan,femn sing,nomn\n"
          ]
        }
      ],
      "source": [
        "# Анализ одной словоформы с несколькими вариантами\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ОМОНИМИЯ: СЛОВО С НЕСКОЛЬКИМИ ЗНАЧЕНИЯМИ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "ambiguous_word = 'банка'\n",
        "parses = morph.parse(ambiguous_word)\n",
        "\n",
        "print(f\"\\nВсе возможные разборы слова '{ambiguous_word}':\")\n",
        "for i, p in enumerate(parses, 1):\n",
        "    print(f\"{i}. Лемма: {p.normal_form:15} | Часть речи: {p.tag.POS:10} | Полный тег: {p.tag}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Построение простого пайплайна"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ПОЛНЫЙ ПАЙПЛАЙН ПРЕДОБРАБОТКИ\n",
            "============================================================\n",
            "\n",
            "Оригинальный текст:\n",
            "Машинное обучение и обработка естественного языка — это мощные инструменты.\n",
            "Они помогают нам понимать и анализировать текст.\n",
            "\n",
            "Предложений: 2\n",
            "  1. Машинное обучение и обработка естественного языка — это мощные инструменты.\n",
            "  2. Они помогают нам понимать и анализировать текст.\n",
            "\n",
            "Всего токенов: 19\n",
            "Первые 10 токенов: ['машинное', 'обучение', 'и', 'обработка', 'естественного', 'языка', '—', 'это', 'мощные', 'инструменты']\n",
            "\n",
            "Очищенные токены (без стоп-слов): 12\n",
            "Токены: ['машинный', 'обучение', 'обработка', 'естественный', 'язык', 'это', 'мощный', 'инструмент', 'помогать', 'понимать', 'анализировать', 'текст']\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ПОЛНЫЙ ПАЙПЛАЙН ПРЕДОБРАБОТКИ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class TextPipeline:\n",
        "    def __init__(self, language='russian'):\n",
        "        self.language = language\n",
        "        self.morph = pymorphy2.MorphAnalyzer()\n",
        "        if language == 'russian':\n",
        "            self.stop_words = stopwords.words('russian')\n",
        "        else:\n",
        "            self.stop_words = stopwords.words('english')\n",
        "    \n",
        "    def process(self, text):\n",
        "        \"\"\"Полная обработка текста\"\"\"\n",
        "        # 1. Сегментация на предложения\n",
        "        sentences = sent_tokenize(text, language=self.language)\n",
        "        \n",
        "        result = {\n",
        "            'original': text,\n",
        "            'sentences': sentences,\n",
        "            'tokens': [],\n",
        "            'lemmas': [],\n",
        "            'cleaned_tokens': []\n",
        "        }\n",
        "        \n",
        "        # 2. Токенизация и лемматизация\n",
        "        all_tokens = word_tokenize(text.lower(), language=self.language)\n",
        "        result['tokens'] = all_tokens\n",
        "        \n",
        "        # 3. Лемматизация\n",
        "        lemmas = []\n",
        "        for token in all_tokens:\n",
        "            if token.isalpha():\n",
        "                lemma = self.morph.parse(token)[0].normal_form\n",
        "                lemmas.append(lemma)\n",
        "            else:\n",
        "                lemmas.append(token)\n",
        "        result['lemmas'] = lemmas\n",
        "        \n",
        "        # 4. Удаление стоп-слов и пунктуации\n",
        "        cleaned = [w for w in lemmas if w.isalpha() and w not in self.stop_words]\n",
        "        result['cleaned_tokens'] = cleaned\n",
        "        \n",
        "        return result\n",
        "\n",
        "# Использование\n",
        "pipeline = TextPipeline('russian')\n",
        "\n",
        "test_text = \"\"\"Машинное обучение и обработка естественного языка — это мощные инструменты.\n",
        "Они помогают нам понимать и анализировать текст.\"\"\"\n",
        "\n",
        "result = pipeline.process(test_text)\n",
        "\n",
        "print(f\"\\nОригинальный текст:\\n{result['original']}\")\n",
        "print(f\"\\nПредложений: {len(result['sentences'])}\")\n",
        "for i, sent in enumerate(result['sentences'], 1):\n",
        "    print(f\"  {i}. {sent}\")\n",
        "\n",
        "print(f\"\\nВсего токенов: {len(result['tokens'])}\")\n",
        "print(f\"Первые 10 токенов: {result['tokens'][:10]}\")\n",
        "\n",
        "print(f\"\\nОчищенные токены (без стоп-слов): {len(result['cleaned_tokens'])}\")\n",
        "print(f\"Токены: {result['cleaned_tokens']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Классификация текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "КЛАССИФИКАЦИЯ НОВОСТЕЙ НА РУССКОМ\n",
            "============================================================\n",
            "\n",
            "Матрица TF-IDF формы: (9, 514)\n",
            "Число признаков: 514\n",
            "\n",
            "Модель обучена!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"КЛАССИФИКАЦИЯ НОВОСТЕЙ НА РУССКОМ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Синтетический корпус для демонстрации\n",
        "# В реальности нужна большая размеченная выборка\n",
        "train_texts = [\n",
        "    \"Президент подписал новый указ о реформе образования\",\n",
        "    \"Правительство приняло бюджет на следующий год\",\n",
        "    \"Депутаты голосовали за законопроект\",\n",
        "    \"Российская сборная выиграла золотую медаль на олимпиаде\",\n",
        "    \"Спортсмены готовятся к чемпионату мира\",\n",
        "    \"Футболист забил решающий гол в матче\",\n",
        "    \"Новая модель смартфона выходит на рынок\",\n",
        "    \"Компания объявила о разработке искусственного интеллекта\",\n",
        "    \"Ученые создали инновационную технологию\",\n",
        "]\n",
        "\n",
        "train_labels = [\n",
        "    0, 0, 0,  # политика\n",
        "    1, 1, 1,  # спорт\n",
        "    2, 2, 2,  # технология\n",
        "]\n",
        "\n",
        "# Векторизация\n",
        "vectorizer = TfidfVectorizer(lowercase=True, analyzer='char', ngram_range=(2, 3))\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "print(f\"\\nМатрица TF-IDF формы: {X_train.shape}\")\n",
        "print(f\"Число признаков: {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "# Обучение классификатора\n",
        "clf = LogisticRegression(random_state=42, max_iter=200)\n",
        "clf.fit(X_train, train_labels)\n",
        "\n",
        "print(\"\\nМодель обучена!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Предсказания для тестовых текстов:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Текст: Депутаты обсуждают новый закон\n",
            "Предсказание: Политика (уверенность: 43.72%)\n",
            "Вероятности по классам:\n",
            "  Политика        43.72% █████████████\n",
            "  Спорт           27.97% ████████\n",
            "  Технология      28.31% ████████\n",
            "\n",
            "Текст: Спортсмен выиграл чемпионат\n",
            "Предсказание: Спорт (уверенность: 45.77%)\n",
            "Вероятности по классам:\n",
            "  Политика        26.66% ███████\n",
            "  Спорт           45.77% █████████████\n",
            "  Технология      27.57% ████████\n",
            "\n",
            "Текст: Новая технология в смартфоне\n",
            "Предсказание: Технология (уверенность: 44.06%)\n",
            "Вероятности по классам:\n",
            "  Политика        26.77% ████████\n",
            "  Спорт           29.17% ████████\n",
            "  Технология      44.06% █████████████\n"
          ]
        }
      ],
      "source": [
        "# Тестирование\n",
        "test_texts = [\n",
        "    \"Депутаты обсуждают новый закон\",  # Политика\n",
        "    \"Спортсмен выиграл чемпионат\",     # Спорт\n",
        "    \"Новая технология в смартфоне\",    # Технология\n",
        "]\n",
        "\n",
        "X_test = vectorizer.transform(test_texts)\n",
        "predictions = clf.predict(X_test)\n",
        "probabilities = clf.predict_proba(X_test)\n",
        "\n",
        "label_names = ['Политика', 'Спорт', 'Технология']\n",
        "\n",
        "print(\"\\nПредсказания для тестовых текстов:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for text, pred, proba in zip(test_texts, predictions, probabilities):\n",
        "    print(f\"\\nТекст: {text}\")\n",
        "    print(f\"Предсказание: {label_names[pred]} (уверенность: {proba[pred]:.2%})\")\n",
        "    print(f\"Вероятности по классам:\")\n",
        "    for label, prob in zip(label_names, proba):\n",
        "        bar = '█' * int(prob * 30)\n",
        "        print(f\"  {label:15} {prob:6.2%} {bar}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "МЕТРИКИ КАЧЕСТВА НА ОБУЧАЮЩЕЙ ВЫБОРКЕ\n",
            "============================================================\n",
            "\n",
            "Общая точность: 100.00%\n",
            "\n",
            "Отчёт по классам:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Политика       1.00      1.00      1.00         3\n",
            "       Спорт       1.00      1.00      1.00         3\n",
            "  Технология       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00         9\n",
            "   macro avg       1.00      1.00      1.00         9\n",
            "weighted avg       1.00      1.00      1.00         9\n",
            "\n",
            "\n",
            "Матрица ошибок:\n",
            "[[3 0 0]\n",
            " [0 3 0]\n",
            " [0 0 3]]\n"
          ]
        }
      ],
      "source": [
        "# На обучающей выборке\n",
        "predictions_train = clf.predict(X_train)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"МЕТРИКИ КАЧЕСТВА НА ОБУЧАЮЩЕЙ ВЫБОРКЕ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nОбщая точность: {(predictions_train == train_labels).mean():.2%}\")\n",
        "print(\"\\nОтчёт по классам:\")\n",
        "print(classification_report(train_labels, predictions_train, target_names=label_names))\n",
        "\n",
        "print(\"\\nМатрица ошибок:\")\n",
        "cm = confusion_matrix(train_labels, predictions_train)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Практические задачи"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ЗАДАЧА 1: Анализ сложности омонимии\n",
            "============================================================\n",
            "\n",
            "Анализ омонимичных слов:\n",
            "\n",
            "'коса': 3 вариант(ов)\n",
            "  1. коса (NOUN)\n",
            "  2. кос (NOUN)\n",
            "  3. косой (ADJS)\n",
            "\n",
            "'замок': 3 вариант(ов)\n",
            "  1. замок (NOUN)\n",
            "  2. замок (NOUN)\n",
            "  3. замокнуть (VERB)\n",
            "\n",
            "'мука': 1 вариант(ов)\n",
            "  1. мука (NOUN)\n",
            "\n",
            "'хлеб': 2 вариант(ов)\n",
            "  1. хлеб (NOUN)\n",
            "  2. хлеб (NOUN)\n",
            "\n",
            "'роман': 12 вариант(ов)\n",
            "  1. роман (NOUN)\n",
            "  2. роман (NOUN)\n",
            "  3. роман (NOUN)\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"ЗАДАЧА 1: Анализ сложности омонимии\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Найдите слова, которые имеют несколько разборов\n",
        "ambiguous_words = ['коса', 'замок', 'мука', 'хлеб', 'роман']\n",
        "\n",
        "print(\"\\nАнализ омонимичных слов:\")\n",
        "for word in ambiguous_words:\n",
        "    parses = morph.parse(word)\n",
        "    print(f\"\\n'{word}': {len(parses)} вариант(ов)\")\n",
        "    for i, p in enumerate(parses[:3], 1):  # Первые 3\n",
        "        print(f\"  {i}. {p.normal_form} ({p.tag.POS})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ЗАДАЧА 2: Сравнение токенизаций\n",
            "============================================================\n",
            "\n",
            "Текст: Mr. Smith e-mailed the file at 5:00 p.m. to john@example.com\n",
            "\n",
            "Простая токенизация (split): 10 токенов\n",
            "['Mr.', 'Smith', 'e-mailed', 'the', 'file', 'at', '5:00', 'p.m.', 'to', 'john@example.com']\n",
            "\n",
            "NLTK токенизация: 12 токенов\n",
            "['Mr.', 'Smith', 'e-mailed', 'the', 'file', 'at', '5:00', 'p.m.', 'to', 'john', '@', 'example.com']\n",
            "\n",
            "Замечание: NLTK лучше разделяет пунктуацию\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ЗАДАЧА 2: Сравнение токенизаций\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "complex_text = \"Mr. Smith e-mailed the file at 5:00 p.m. to john@example.com\"\n",
        "\n",
        "# Попробуем разные подходы\n",
        "simple = complex_text.split()\n",
        "nltk_tokens = word_tokenize(complex_text)\n",
        "\n",
        "print(f\"\\nТекст: {complex_text}\")\n",
        "print(f\"\\nПростая токенизация (split): {len(simple)} токенов\")\n",
        "print(simple)\n",
        "print(f\"\\nNLTK токенизация: {len(nltk_tokens)} токенов\")\n",
        "print(nltk_tokens)\n",
        "print(\"\\nЗамечание: NLTK лучше разделяет пунктуацию\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ЗАДАЧА 3: Создание собственного токенизатора\n",
            "============================================================\n",
            "\n",
            "Текст: Check my email at john@example.com or visit https://example.com. Price is $19.99 #NLP\n",
            "\n",
            "Токены с типами:\n",
            "  Check                → WORD\n",
            "  my                   → WORD\n",
            "  email                → WORD\n",
            "  at                   → WORD\n",
            "  john@example.com     → EMAIL\n",
            "  or                   → WORD\n",
            "  visit                → WORD\n",
            "  https://example.com. → URL\n",
            "  Price                → WORD\n",
            "  is                   → WORD\n",
            "  $19.99               → NUMBER\n",
            "  #NLP                 → HASHTAG\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ЗАДАЧА 3: Создание собственного токенизатора\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def advanced_tokenize(text):\n",
        "    \"\"\"\n",
        "    Продвинутая токенизация с обработкой:\n",
        "    - Email адресов\n",
        "    - URL\n",
        "    - Хэштегов\n",
        "    - Чисел и денежных сумм\n",
        "    \"\"\"\n",
        "    # Сохраняем специальные токены\n",
        "    tokens = []\n",
        "    \n",
        "    # Паттерны\n",
        "    patterns = [\n",
        "        (r'https?://\\S+', 'URL'),\n",
        "        (r'\\S+@\\S+', 'EMAIL'),\n",
        "        (r'#\\w+', 'HASHTAG'),\n",
        "        (r'\\$?\\d+[.,]\\d+', 'NUMBER'),\n",
        "        (r'\\b\\w+\\b', 'WORD'),\n",
        "        (r'[.,:;!?]', 'PUNCT'),\n",
        "    ]\n",
        "    \n",
        "    from itertools import chain\n",
        "    combined_pattern = '|'.join(f'(?P<{name}>{pat})' for pat, name in patterns)\n",
        "    \n",
        "    for match in re.finditer(combined_pattern, text):\n",
        "        token = match.group()\n",
        "        token_type = match.lastgroup\n",
        "        tokens.append((token, token_type))\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "test = \"Check my email at john@example.com or visit https://example.com. Price is $19.99 #NLP\"\n",
        "result = advanced_tokenize(test)\n",
        "\n",
        "print(f\"\\nТекст: {test}\")\n",
        "print(f\"\\nТокены с типами:\")\n",
        "for token, token_type in result:\n",
        "    print(f\"  {token:20} → {token_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ЗАДАЧА 4: Влияние стоп-слов на классификацию\n",
            "============================================================\n",
            "\n",
            "Оригинальные токены: ['это', 'очень', 'интересный', 'и', 'важный', 'документ', 'о', 'машинном', 'обучении']\n",
            "Без стоп-слов: ['это', 'очень', 'интересный', 'важный', 'документ', 'машинном', 'обучении']\n",
            "\n",
            "Удалены стоп-слова: ['и', 'о']\n",
            "\n",
            "Оригинальных слов: 9\n",
            "Значимых слов: 7\n",
            "Удалено: 2 (22.2%)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ЗАДАЧА 4: Влияние стоп-слов на классификацию\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_text = \"Это очень интересный и важный документ о машинном обучении\"\n",
        "\n",
        "tokens = word_tokenize(sample_text.lower(), language='russian')\n",
        "stop_words_ru = stopwords.words('russian')\n",
        "\n",
        "print(f\"\\nОригинальные токены: {[t for t in tokens if t.isalpha()]}\")\n",
        "\n",
        "# С удалением стоп-слов\n",
        "filtered = [t for t in tokens if t.isalpha() and t not in stop_words_ru]\n",
        "print(f\"Без стоп-слов: {filtered}\")\n",
        "\n",
        "removed = [t for t in tokens if t.isalpha() and t in stop_words_ru]\n",
        "print(f\"\\nУдалены стоп-слова: {removed}\")\n",
        "print(f\"\\nОригинальных слов: {sum(1 for t in tokens if t.isalpha())}\")\n",
        "print(f\"Значимых слов: {len(filtered)}\")\n",
        "print(f\"Удалено: {len(removed)} ({len(removed)/sum(1 for t in tokens if t.isalpha()):.1%})\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
