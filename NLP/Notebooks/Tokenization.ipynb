{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –ü—Ä–∞–∫—Ç–∏–∫–∞: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "–í —ç—Ç–æ–π —Ç–µ—Ç—Ä–∞–¥–∏ –º—ã –ø—Ä–æ–π–¥—ë–º:\n",
        "- –†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
        "- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏ –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞\n",
        "- –°—Ç–µ–º–º–∏–Ω–≥ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é\n",
        "- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: BoW, TF-IDF, BM25, N-–≥—Ä–∞–º–º—ã\n",
        "- –†–∞–±–æ—Ç—É —Å OOV —Å–ª–æ–≤–∞–º–∏\n",
        "\n",
        "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:\n",
        "```\n",
        "pip install nltk pymorphy2 scikit-learn rank-bm25 unicodedata\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –ú–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "1. –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò\n",
            "======================================================================\n",
            "\n",
            "–¢–µ–∫—Å—Ç:\n",
            "Mr. Smith e-mailed john@example.com at 5:00 p.m. regarding the project.\n",
            "It's a very important matter! What's the status?\n",
            "\n",
            "1. split() ‚Äî 18 —Ç–æ–∫–µ–Ω–æ–≤:\n",
            "   ['Mr.', 'Smith', 'e-mailed', 'john@example.com', 'at', '5:00', 'p.m.', 'regarding', 'the', 'project.', \"It's\", 'a', 'very', 'important', 'matter!', \"What's\", 'the', 'status?']\n",
            "   ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞: –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è –∏ –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã –Ω–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã\n",
            "\n",
            "2. NLTK word_tokenize() ‚Äî 25 —Ç–æ–∫–µ–Ω–æ–≤:\n",
            "   ['Mr.', 'Smith', 'e-mailed', 'john', '@', 'example.com', 'at', '5:00', 'p.m.', 'regarding', 'the', 'project', '.', 'It', \"'s\", 'a', 'very', 'important', 'matter', '!', 'What', \"'s\", 'the', 'status', '?']\n",
            "   ‚úì –•–æ—Ä–æ—à–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
            "\n",
            "3. Regex (word + punct) ‚Äî 35 —Ç–æ–∫–µ–Ω–æ–≤:\n",
            "   ['mr', '.', 'smith', 'e', '-', 'mailed', 'john', 'example', '.', 'com', 'at', '5', '00', 'p', '.', 'm', '.', 'regarding', 'the', 'project', '.', 'it', \"'\", 's', 'a', 'very', 'important', 'matter', '!', 'what', \"'\", 's', 'the', 'status', '?']\n",
            "   ‚úì –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º, —á—Ç–æ —Å—á–∏—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω–æ–º\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymorphy2\n",
        "from collections import Counter\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"1. –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –°–ª–æ–∂–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ —Å–ª—É—á–∞—è–º–∏\n",
        "complex_text = \"\"\"Mr. Smith e-mailed john@example.com at 5:00 p.m. regarding the project.\n",
        "It's a very important matter! What's the status?\"\"\"\n",
        "\n",
        "print(f\"\\n–¢–µ–∫—Å—Ç:\\n{complex_text}\\n\")\n",
        "\n",
        "# 1. –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ–±–µ–ª–∞–º\n",
        "simple = complex_text.split()\n",
        "print(f\"1. split() ‚Äî {len(simple)} —Ç–æ–∫–µ–Ω–æ–≤:\")\n",
        "print(f\"   {simple}\")\n",
        "print(\"   ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞: –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è –∏ –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã –Ω–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã\\n\")\n",
        "\n",
        "# 2. NLTK word_tokenize\n",
        "nltk_tokens = word_tokenize(complex_text)\n",
        "print(f\"2. NLTK word_tokenize() ‚Äî {len(nltk_tokens)} —Ç–æ–∫–µ–Ω–æ–≤:\")\n",
        "print(f\"   {nltk_tokens}\")\n",
        "print(\"   ‚úì –•–æ—Ä–æ—à–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\\n\")\n",
        "\n",
        "# 3. Regex —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "regex_tokens = re.findall(r\"\\b\\w+\\b|[\\w.,!?;'-]\", complex_text.lower())\n",
        "print(f\"3. Regex (word + punct) ‚Äî {len(regex_tokens)} —Ç–æ–∫–µ–Ω–æ–≤:\")\n",
        "print(f\"   {regex_tokens}\")\n",
        "print(\"   ‚úì –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º, —á—Ç–æ —Å—á–∏—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω–æ–º\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "2. –ü–û–î–°–õ–û–í–ù–ê–Ø –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø: BPE (Byte Pair Encoding)\n",
            "======================================================================\n",
            "\n",
            "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ 'quick': ['q', 'u', 'i', 'c', 'k']\n",
            "BPE —Ç–æ–∫–µ–Ω—ã: ['quick</w>']\n",
            "\n",
            "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ 'independence': ['i', 'n', 'd', 'e', 'p', 'e', 'n', 'd', 'e', 'n', 'c', 'e']\n",
            "BPE —Ç–æ–∫–µ–Ω—ã: ['i', 'n', 'd', 'e', 'p', 'e', 'n', 'd', 'e', 'n', 'c', 'e', '</w>']\n",
            "\n",
            "‚úì BPE —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞ –Ω–∞ –±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø–æ–¥—Å–ª–æ–≤–∞\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"2. –ü–û–î–°–õ–û–í–ù–ê–Ø –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø: BPE (Byte Pair Encoding)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "class SimpleBPE:\n",
        "    \"\"\"–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\"\"\"\n",
        "    \n",
        "    def __init__(self, num_merges=100):\n",
        "        self.num_merges = num_merges\n",
        "        self.word_freqs = {}\n",
        "        self.bpe_ranks = {}\n",
        "        self.vocab = {}\n",
        "    \n",
        "    def _get_stats(self, vocab):\n",
        "        \"\"\"–ü–æ–¥—Å—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤\"\"\"\n",
        "        pairs = {}\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word.split()\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pair = (symbols[i], symbols[i + 1])\n",
        "                pairs[pair] = pairs.get(pair, 0) + freq\n",
        "        return pairs\n",
        "    \n",
        "    def _merge_vocab(self, pair, vocab):\n",
        "        \"\"\"–û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–∞–º—É—é —á–∞—Å—Ç—É—é –ø–∞—Ä—É –≤–æ –≤—Å–µ—Ö —Å–ª–æ–≤–∞—Ö\"\"\"\n",
        "        new_vocab = {}\n",
        "        bigram = ' '.join(pair)\n",
        "        replacement = ''.join(pair)\n",
        "        \n",
        "        for word in vocab:\n",
        "            new_word = word.replace(bigram, replacement)\n",
        "            new_vocab[new_word] = vocab[word]\n",
        "        \n",
        "        return new_vocab\n",
        "    \n",
        "    def train(self, texts):\n",
        "        \"\"\"–û–±—É—á–∏—Ç—å BPE –Ω–∞ –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–æ–≤\"\"\"\n",
        "        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤\n",
        "        for text in texts:\n",
        "            for word in text.split():\n",
        "                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–æ–º, —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–º –Ω–∞ —Å–∏–º–≤–æ–ª—ã\n",
        "                word_lower = word.lower()\n",
        "                char_word = ' '.join(list(word_lower)) + ' </w>'\n",
        "                self.word_freqs[char_word] = self.word_freqs.get(char_word, 0) + 1\n",
        "        \n",
        "        vocab = {word: freq for word, freq in self.word_freqs.items()}\n",
        "        \n",
        "        # –í—ã–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∂–∏\n",
        "        for i in range(self.num_merges):\n",
        "            pairs = self._get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "            \n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            self.bpe_ranks[best_pair] = i\n",
        "            vocab = self._merge_vocab(best_pair, vocab)\n",
        "        \n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "        self.vocab = set(vocab.keys())\n",
        "    \n",
        "    def tokenize(self, word):\n",
        "        \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ–º BPE –∫ —Å–ª–æ–≤—É\"\"\"\n",
        "        word_lower = word.lower()\n",
        "        word = ' '.join(list(word_lower)) + ' </w>'\n",
        "        \n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ –º–µ—Ä–∂–∏\n",
        "        for pair, rank in sorted(self.bpe_ranks.items(), key=lambda x: x[1]):\n",
        "            bigram = ' '.join(pair)\n",
        "            replacement = ''.join(pair)\n",
        "            word = word.replace(bigram, replacement)\n",
        "        \n",
        "        return word.split()\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "texts = [\n",
        "    \"the quick brown fox jumps\",\n",
        "    \"the lazy dog\",\n",
        "    \"the quick brown fox\",\n",
        "]\n",
        "\n",
        "bpe = SimpleBPE(num_merges=50)\n",
        "bpe.train(texts)\n",
        "\n",
        "print(f\"\\n–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ 'quick': {list('quick')}\")\n",
        "print(f\"BPE —Ç–æ–∫–µ–Ω—ã: {bpe.tokenize('quick')}\")\n",
        "\n",
        "print(f\"\\n–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ 'independence': {list('independence')}\")\n",
        "print(f\"BPE —Ç–æ–∫–µ–Ω—ã: {bpe.tokenize('independence')}\")\n",
        "print(\"\\n‚úì BPE —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞ –Ω–∞ –±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø–æ–¥—Å–ª–æ–≤–∞\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "–ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\n",
            "======================================================================\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:\n",
            "\n",
            "–û—Ä–∏–≥–∏–Ω–∞–ª:    'HELLO World! —ç—Ç–æ –¢–ï–ö–°–¢.'\n",
            "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: 'hello world! —ç—Ç–æ —Ç–µ–∫—Å—Ç.'\n",
            "\n",
            "–û—Ä–∏–≥–∏–Ω–∞–ª:    'caf√© na√Øve r√©sum√©'\n",
            "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: 'cafe naive resume'\n",
            "\n",
            "–û—Ä–∏–≥–∏–Ω–∞–ª:    '<p>HTML &amp; encoded text</p>'\n",
            "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: 'html & encoded text'\n",
            "\n",
            "–û—Ä–∏–≥–∏–Ω–∞–ª:    '   –õ–∏—à–Ω–∏–µ    –ø—Ä–æ–±–µ–ª—ã   –≤–µ–∑–¥–µ  '\n",
            "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: '–ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤–µ–∑–¥–µ'\n",
            "\n",
            "–û—Ä–∏–≥–∏–Ω–∞–ª:    '–ø—Ä–∏–≤–µ—Ç –ü–†–ò–í–ï–¢ –ü—Ä–ò–≤–ï—Ç'\n",
            "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: '–ø—Ä–∏–≤–µ—Ç –ø—Ä–∏–≤–µ—Ç –ø—Ä–∏–≤–µ—Ç'\n"
          ]
        }
      ],
      "source": [
        "import unicodedata\n",
        "import html\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"–ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
        "texts_to_normalize = [\n",
        "    \"HELLO World! —ç—Ç–æ –¢–ï–ö–°–¢.\",\n",
        "    \"caf√© na√Øve r√©sum√©\",  # –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–∫–∏\n",
        "    \"<p>HTML &amp; encoded text</p>\",  # HTML\n",
        "    \"   –õ–∏—à–Ω–∏–µ    –ø—Ä–æ–±–µ–ª—ã   –≤–µ–∑–¥–µ  \",  # –ø—Ä–æ–±–µ–ª—ã\n",
        "    \"–ø—Ä–∏–≤–µ—Ç –ü–†–ò–í–ï–¢ –ü—Ä–ò–≤–ï—Ç\",  # —Ä–µ–≥–∏—Å—Ç—Ä\n",
        "]\n",
        "\n",
        "class TextNormalizer:\n",
        "    \"\"\"–ü–æ–ª–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def normalize(text, lowercase=True, remove_accents=True, \n",
        "                  remove_html=True, clean_whitespace=True):\n",
        "        \"\"\"\n",
        "        –ü–æ–ª–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:\n",
        "        - –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–≥–∏—Å—Ç—Ä–∞\n",
        "        - –£–¥–∞–ª–µ–Ω–∏–µ –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–∫–æ–≤\n",
        "        - HTML –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "        - –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤\n",
        "        \"\"\"\n",
        "        # 1. –£–¥–∞–ª–µ–Ω–∏–µ HTML\n",
        "        if remove_html:\n",
        "            text = html.unescape(text)\n",
        "            text = re.sub(r'<[^>]+>', '', text)\n",
        "        \n",
        "        # 2. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "        if lowercase:\n",
        "            text = text.lower()\n",
        "        \n",
        "        # 3. –£–¥–∞–ª–µ–Ω–∏–µ –¥–∏–∞–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–∫–æ–≤ (accents)\n",
        "        if remove_accents:\n",
        "            text = unicodedata.normalize('NFD', text)\n",
        "            text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n",
        "        \n",
        "        # 4. –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤\n",
        "        if clean_whitespace:\n",
        "            text = re.sub(r'\\s+', ' ', text)  # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã ‚Üí –æ–¥–∏–Ω\n",
        "            text = text.strip()  # –Ω–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü\n",
        "        \n",
        "        return text\n",
        "\n",
        "normalizer = TextNormalizer()\n",
        "\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:\")\n",
        "for text in texts_to_normalize:\n",
        "    normalized = normalizer.normalize(text)\n",
        "    print(f\"\\n–û—Ä–∏–≥–∏–Ω–∞–ª:    '{text}'\")\n",
        "    print(f\"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: '{normalized}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. –°—Ç–µ–º–º–∏–Ω–≥ vs –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "–°–¢–ï–ú–ú–ò–ù–ì VS –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø\n",
            "======================================================================\n",
            "\n",
            "–ê–ù–ì–õ–ò–ô–°–ö–ò–ô –Ø–ó–´–ö:\n",
            "----------------------------------------------------------------------\n",
            "–°–ª–æ–≤–æ                –°—Ç–µ–º–º–∏–Ω–≥             –†–µ–∑—É–ª—å—Ç–∞—Ç           \n",
            "----------------------------------------------------------------------\n",
            "running              run                 \n",
            "runs                 run                 \n",
            "ran                  ran                 \n",
            "runner               runner              \n",
            "working              work                \n",
            "works                work                \n",
            "worked               work                \n",
            "\n",
            "–†–£–°–°–ö–ò–ô –Ø–ó–´–ö:\n",
            "----------------------------------------------------------------------\n",
            "–°–ª–æ–≤–æ                –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è        \n",
            "----------------------------------------------------------------------\n",
            "–±–µ–∂–∞–ª–∏               –±–µ–∂–∞—Ç—å              \n",
            "–±–µ–≥—É—â–∏–π              –±–µ–∂–∞—Ç—å              \n",
            "–±–µ–∂–∞–ª                –±–µ–∂–∞—Ç—å              \n",
            "–±–µ–≥—É–Ω                –±–µ–≥—É–Ω               \n",
            "—Ä–∞–±–æ—Ç–∞—é—Ç             —Ä–∞–±–æ—Ç–∞—Ç—å            \n",
            "—Ä–∞–±–æ—á–∏–π              —Ä–∞–±–æ—á–∏–π             \n",
            "—Ä–∞–±–æ—Ç–æ–π              —Ä–∞–±–æ—Ç–∞              \n",
            "\n",
            "======================================================================\n",
            "–°–†–ê–í–ù–ï–ù–ò–ï:\n",
            "======================================================================\n",
            "\n",
            "–°–¢–ï–ú–ú–ò–ù–ì (Stemming):\n",
            "  + –ë—ã—Å—Ç—Ä—ã–π (–ø—Ä–æ—Å—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ—Ç—Å–µ—á–µ–Ω–∏—è)\n",
            "  - –ú–æ–∂–µ—Ç –¥–∞—Ç—å –Ω–µ —Ä–µ–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ (–Ω–∞–ø—Ä. 'run' + 'ing' ‚Üí 'runn')\n",
            "  - –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —á–∞—Å—Ç—å —Ä–µ—á–∏\n",
            "\n",
            "–õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø (Lemmatization):\n",
            "  + –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ, —Å–ª–æ–≤–∞—Ä–Ω–æ–µ —Å–ª–æ–≤–æ\n",
            "  + –£—á–∏—Ç—ã–≤–∞–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é\n",
            "  - –ú–µ–¥–ª–µ–Ω–Ω–µ–µ (—Ç—Ä–µ–±—É–µ—Ç —Å–ª–æ–≤–∞—Ä—è –∏ –∞–Ω–∞–ª–∏–∑–∞)\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"–°–¢–ï–ú–ú–ò–ù–ì VS –õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤–æ—Ñ–æ—Ä–º\n",
        "words_ru = ['–±–µ–∂–∞–ª–∏', '–±–µ–≥—É—â–∏–π', '–±–µ–∂–∞–ª', '–±–µ–≥—É–Ω', '—Ä–∞–±–æ—Ç–∞—é—Ç', '—Ä–∞–±–æ—á–∏–π', '—Ä–∞–±–æ—Ç–æ–π']\n",
        "words_en = ['running', 'runs', 'ran', 'runner', 'working', 'works', 'worked']\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤\n",
        "stemmer_en = SnowballStemmer('english')\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "print(\"\\n–ê–ù–ì–õ–ò–ô–°–ö–ò–ô –Ø–ó–´–ö:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'–°–ª–æ–≤–æ':<20} {'–°—Ç–µ–º–º–∏–Ω–≥':<20} {'–†–µ–∑—É–ª—å—Ç–∞—Ç':<20}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for word in words_en:\n",
        "    stem = stemmer_en.stem(word)\n",
        "    print(f\"{word:<20} {stem:<20}\")\n",
        "\n",
        "print(\"\\n–†–£–°–°–ö–ò–ô –Ø–ó–´–ö:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'–°–ª–æ–≤–æ':<20} {'–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è':<20}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for word in words_ru:\n",
        "    lemma = morph.parse(word)[0].normal_form\n",
        "    print(f\"{word:<20} {lemma:<20}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"–°–†–ê–í–ù–ï–ù–ò–ï:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n–°–¢–ï–ú–ú–ò–ù–ì (Stemming):\")\n",
        "print(\"  + –ë—ã—Å—Ç—Ä—ã–π (–ø—Ä–æ—Å—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ—Ç—Å–µ—á–µ–Ω–∏—è)\")\n",
        "print(\"  - –ú–æ–∂–µ—Ç –¥–∞—Ç—å –Ω–µ —Ä–µ–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ (–Ω–∞–ø—Ä. 'run' + 'ing' ‚Üí 'runn')\")\n",
        "print(\"  - –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —á–∞—Å—Ç—å —Ä–µ—á–∏\")\n",
        "print(\"\\n–õ–ï–ú–ú–ê–¢–ò–ó–ê–¶–ò–Ø (Lemmatization):\")\n",
        "print(\"  + –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ, —Å–ª–æ–≤–∞—Ä–Ω–æ–µ —Å–ª–æ–≤–æ\")\n",
        "print(\"  + –£—á–∏—Ç—ã–≤–∞–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é\")\n",
        "print(\"  - –ú–µ–¥–ª–µ–Ω–Ω–µ–µ (—Ç—Ä–µ–±—É–µ—Ç —Å–ª–æ–≤–∞—Ä—è –∏ –∞–Ω–∞–ª–∏–∑–∞)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ OOV (Out-of-Vocabulary) —Å–ª–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "–û–ë–†–ê–ë–û–¢–ö–ê OOV (Out-of-Vocabulary) –°–õ–û–í\n",
            "======================================================================\n",
            "\n",
            "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: 13 —Ç–æ–∫–µ–Ω–æ–≤\n",
            "\n",
            "–°–ª–æ–≤–∞—Ä—å: {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3, 'the': 4, 'quick': 5, 'fox': 6, 'jumps': 7, 'dog': 8, 'brown': 9, 'lazy': 10, 'sleeps': 11, 'over': 12}\n",
            "\n",
            "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: the UNKNOWN_WORD fox ANOTHER_UNKNOWN\n",
            "–ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω: [4, 1, 6, 1]\n",
            "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω: the <UNK> fox <UNK>\n",
            "\n",
            "–ó–∞–º–µ—á–∞–Ω–∏–µ: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ (UNKNOWN_WORD, ANOTHER_UNKNOWN) ‚Üí <UNK>\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"–û–ë–†–ê–ë–û–¢–ö–ê OOV (Out-of-Vocabulary) –°–õ–û–í\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "class SimpleVocabulary:\n",
        "    \"\"\"–°–ª–æ–≤–∞—Ä—å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π OOV —Å–ª–æ–≤\"\"\"\n",
        "    \n",
        "    def __init__(self, min_freq=2, max_vocab_size=1000):\n",
        "        self.min_freq = min_freq\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.word2id = {}  # —Å–ª–æ–≤–æ ‚Üí ID\n",
        "        self.id2word = {}  # ID ‚Üí —Å–ª–æ–≤–æ\n",
        "        self.word_freq = Counter()\n",
        "        \n",
        "        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "        self.pad_token = '<PAD>'\n",
        "        self.unk_token = '<UNK>'\n",
        "        self.sos_token = '<SOS>'\n",
        "        self.eos_token = '<EOS>'\n",
        "        \n",
        "        self._add_special_tokens()\n",
        "    \n",
        "    def _add_special_tokens(self):\n",
        "        special_tokens = [self.pad_token, self.unk_token, self.sos_token, self.eos_token]\n",
        "        for token in special_tokens:\n",
        "            idx = len(self.word2id)\n",
        "            self.word2id[token] = idx\n",
        "            self.id2word[idx] = token\n",
        "    \n",
        "    def build(self, texts):\n",
        "        \"\"\"–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∏–∑ –∫–æ—Ä–ø—É—Å–∞\"\"\"\n",
        "        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤\n",
        "        for text in texts:\n",
        "            tokens = text.lower().split()\n",
        "            self.word_freq.update(tokens)\n",
        "        \n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è >= min_freq\n",
        "        for word, freq in self.word_freq.most_common(self.max_vocab_size):\n",
        "            if freq >= self.min_freq:\n",
        "                idx = len(self.word2id)\n",
        "                self.word2id[word] = idx\n",
        "                self.id2word[idx] = word\n",
        "    \n",
        "    def encode(self, text):\n",
        "        \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ IDs, –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ ‚Üí UNK\"\"\"\n",
        "        tokens = text.lower().split()\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            if token in self.word2id:\n",
        "                ids.append(self.word2id[token])\n",
        "            else:\n",
        "                ids.append(self.word2id[self.unk_token])\n",
        "        return ids\n",
        "    \n",
        "    def decode(self, ids):\n",
        "        \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å IDs –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç\"\"\"\n",
        "        tokens = [self.id2word.get(id, self.unk_token) for id in ids]\n",
        "        return ' '.join(tokens)\n",
        "    \n",
        "    def size(self):\n",
        "        return len(self.word2id)\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "train_texts = [\n",
        "    'the quick brown fox jumps',\n",
        "    'the lazy dog sleeps',\n",
        "    'quick fox jumps over dog',\n",
        "]\n",
        "\n",
        "vocab = SimpleVocabulary(min_freq=1, max_vocab_size=100)\n",
        "vocab.build(train_texts)\n",
        "\n",
        "print(f\"\\n–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab.size()} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
        "print(f\"\\n–°–ª–æ–≤–∞—Ä—å: {vocab.word2id}\")\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ OOV\n",
        "test_text = \"the UNKNOWN_WORD fox ANOTHER_UNKNOWN\"\n",
        "encoded = vocab.encode(test_text)\n",
        "decoded = vocab.decode(encoded)\n",
        "\n",
        "print(f\"\\n–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: {test_text}\")\n",
        "print(f\"–ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω: {encoded}\")\n",
        "print(f\"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω: {decoded}\")\n",
        "print(\"\\n–ó–∞–º–µ—á–∞–Ω–∏–µ: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ (UNKNOWN_WORD, ANOTHER_UNKNOWN) ‚Üí <UNK>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: BoW, TF-IDF, N-–≥—Ä–∞–º–º—ã"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "–ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í: Bag of Words –∏ TF-IDF\n",
            "======================================================================\n",
            "\n",
            "1. BAG OF WORDS (BoW)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "–ú–∞—Ç—Ä–∏—Ü–∞ BoW —Ñ–æ—Ä–º—ã: (3, 12)\n",
            "–°–ª–æ–≤–∞—Ä—å: ['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'log' 'mat' 'on' 'pets' 'sat' 'the']\n",
            "\n",
            "–ú–∞—Ç—Ä–∏—Ü–∞ (–¥–æ–∫—É–º–µ–Ω—Ç—ã √ó —Å–ª–æ–≤–∞):\n",
            "[[0 0 1 0 0 0 0 1 1 0 1 2]\n",
            " [0 0 0 0 1 0 1 0 1 0 1 2]\n",
            " [1 1 0 1 0 1 0 0 0 1 0 0]]\n",
            "\n",
            "‚úì BoW –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "2. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "–ú–∞—Ç—Ä–∏—Ü–∞ TF-IDF —Ñ–æ—Ä–º—ã: (3, 12)\n",
            "\n",
            "–ú–∞—Ç—Ä–∏—Ü–∞ (–¥–æ–∫—É–º–µ–Ω—Ç—ã √ó —Å–ª–æ–≤–∞):\n",
            "[[0.         0.         0.42755362 0.         0.         0.\n",
            "  0.         0.42755362 0.32516555 0.         0.32516555 0.6503311 ]\n",
            " [0.         0.         0.         0.         0.42755362 0.\n",
            "  0.42755362 0.         0.32516555 0.         0.32516555 0.6503311 ]\n",
            " [0.4472136  0.4472136  0.         0.4472136  0.         0.4472136\n",
            "  0.         0.         0.         0.4472136  0.         0.        ]]\n",
            "\n",
            "–§–æ—Ä–º—É–ª—ã:\n",
            "  TF(t, d) = count(t in d) / total_words(d)\n",
            "  IDF(t) = log(total_docs / docs_with_t)\n",
            "  TF-IDF(t, d) = TF(t, d) √ó IDF(t)\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä: —Å–ª–æ–≤–æ 'the' –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ 1\n",
            "  TF = 2/6 = 0.333\n",
            "  IDF = log(3/3) = 0.000\n",
            "  TF-IDF = 0.650\n",
            "  üí° –ù–∏–∑–∫–æ–µ TF-IDF, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤–µ–∑–¥–µ (–Ω–∏–∑–∫–∞—è IDF)\n",
            "\n",
            "–ü—Ä–∏–º–µ—Ä: —Å–ª–æ–≤–æ 'mat' –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ 1\n",
            "  TF = 1/6 = 0.167\n",
            "  IDF = log(3/1) = 1.099\n",
            "  TF-IDF = 0.428\n",
            "  üí° –í—ã—Å–æ–∫–æ–µ TF-IDF, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ª–æ–≤–æ —Ä–µ–¥–∫–æ–µ (–≤—ã—Å–æ–∫–∞—è IDF)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import math\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"–ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í: Bag of Words –∏ TF-IDF\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –ú–∞–ª–µ–Ω—å–∫–∏–π –∫–æ—Ä–ø—É—Å\n",
        "documents = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog sat on the log\",\n",
        "    \"cats and dogs are pets\",\n",
        "]\n",
        "\n",
        "# 1. Bag of Words (Count Vectorizer)\n",
        "print(\"\\n1. BAG OF WORDS (BoW)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(f\"\\n–ú–∞—Ç—Ä–∏—Ü–∞ BoW —Ñ–æ—Ä–º—ã: {X_bow.shape}\")\n",
        "print(f\"–°–ª–æ–≤–∞—Ä—å: {bow_vectorizer.get_feature_names_out()}\")\n",
        "\n",
        "print(\"\\n–ú–∞—Ç—Ä–∏—Ü–∞ (–¥–æ–∫—É–º–µ–Ω—Ç—ã √ó —Å–ª–æ–≤–∞):\")\n",
        "print(X_bow.toarray())\n",
        "print(\"\\n‚úì BoW –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ\")\n",
        "\n",
        "# 2. TF-IDF\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"2. TF-IDF (Term Frequency - Inverse Document Frequency)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(f\"\\n–ú–∞—Ç—Ä–∏—Ü–∞ TF-IDF —Ñ–æ—Ä–º—ã: {X_tfidf.shape}\")\n",
        "print(\"\\n–ú–∞—Ç—Ä–∏—Ü–∞ (–¥–æ–∫—É–º–µ–Ω—Ç—ã √ó —Å–ª–æ–≤–∞):\")\n",
        "print(X_tfidf.toarray())\n",
        "\n",
        "print(\"\\n–§–æ—Ä–º—É–ª—ã:\")\n",
        "print(\"  TF(t, d) = count(t in d) / total_words(d)\")\n",
        "print(\"  IDF(t) = log(total_docs / docs_with_t)\")\n",
        "print(\"  TF-IDF(t, d) = TF(t, d) √ó IDF(t)\")\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑: —Å–ª–æ–≤–æ 'the' –≤ –ø–µ—Ä–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ\n",
        "word_idx = list(bow_vectorizer.get_feature_names_out()).index('the')\n",
        "tf_value = X_bow[0, word_idx] / X_bow[0].sum()\n",
        "idf_value = math.log(3 / 3)  # –≤—Å–µ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–º–µ—é—Ç 'the'\n",
        "\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä: —Å–ª–æ–≤–æ 'the' –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ 1\")\n",
        "print(f\"  TF = {X_bow[0, word_idx]}/6 = {tf_value:.3f}\")\n",
        "print(f\"  IDF = log(3/3) = {idf_value:.3f}\")\n",
        "print(f\"  TF-IDF = {X_tfidf[0, word_idx]:.3f}\")\n",
        "print(\"  üí° –ù–∏–∑–∫–æ–µ TF-IDF, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤–µ–∑–¥–µ (–Ω–∏–∑–∫–∞—è IDF)\")\n",
        "\n",
        "# –°–ª–æ–≤–æ 'mat' –≤ –ø–µ—Ä–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ (—Ä–µ–¥–∫–æ–µ)\n",
        "word_idx = list(bow_vectorizer.get_feature_names_out()).index('mat')\n",
        "tf_value = X_bow[0, word_idx] / X_bow[0].sum()\n",
        "idf_value = math.log(3 / 1)  # —Ç–æ–ª—å–∫–æ 1 –¥–æ–∫—É–º–µ–Ω—Ç –∏–º–µ–µ—Ç 'mat'\n",
        "\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä: —Å–ª–æ–≤–æ 'mat' –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ 1\")\n",
        "print(f\"  TF = {X_bow[0, word_idx]}/6 = {tf_value:.3f}\")\n",
        "print(f\"  IDF = log(3/1) = {idf_value:.3f}\")\n",
        "print(f\"  TF-IDF = {X_tfidf[0, word_idx]:.3f}\")\n",
        "print(\"  üí° –í—ã—Å–æ–∫–æ–µ TF-IDF, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ª–æ–≤–æ —Ä–µ–¥–∫–æ–µ (–≤—ã—Å–æ–∫–∞—è IDF)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "3. N-–ì–†–ê–ú–ú–´ (N-grams)\n",
            "======================================================================\n",
            "\n",
            "–¢–µ–∫—Å—Ç: 'the quick brown fox jumps over the lazy dog'\n",
            "\n",
            "1-–≥—Ä–∞–º–º—ã: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "\n",
            "2-–≥—Ä–∞–º–º—ã: ['the quick', 'quick brown', 'brown fox', 'fox jumps', 'jumps over', 'over the', 'the lazy', 'lazy dog']\n",
            "\n",
            "3-–≥—Ä–∞–º–º—ã: ['the quick brown', 'quick brown fox', 'brown fox jumps', 'fox jumps over', 'jumps over the', 'over the lazy', 'the lazy dog']\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "N-–≥—Ä–∞–º–º—ã —á–µ—Ä–µ–∑ TfidfVectorizer:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "–ë–∏–≥—Ä–∞–º–º—ã (ngram_range=(2,2)):\n",
            "  ['brown dog', 'brown fox', 'lazy brown', 'quick brown', 'the lazy', 'the quick']\n",
            "\n",
            "–£–Ω–∏–≥—Ä–∞–º–º—ã + –ë–∏–≥—Ä–∞–º–º—ã (ngram_range=(1,2)):\n",
            "  ['brown', 'brown dog', 'brown fox', 'dog', 'fox', 'lazy', 'lazy brown', 'quick', 'quick brown', 'the', 'the lazy', 'the quick']\n",
            "\n",
            "–ú–∞—Ç—Ä–∏—Ü–∞ —Ñ–æ—Ä–º—ã: (2, 12)\n",
            "  (–¥–æ–∫—É–º–µ–Ω—Ç—ã: 2, –ø—Ä–∏–∑–Ω–∞–∫–∏: 12)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"3. N-–ì–†–ê–ú–ú–´ (N-grams)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "text = \"the quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "def get_ngrams(text, n):\n",
        "    \"\"\"–ò–∑–≤–ª–µ—á—å n-–≥—Ä–∞–º–º—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
        "    tokens = text.split()\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngram = ' '.join(tokens[i:i+n])\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "print(f\"\\n–¢–µ–∫—Å—Ç: '{text}'\")\n",
        "\n",
        "# –Æ–Ω–∏–≥—Ä–∞–º–º—ã (1-–≥—Ä–∞–º–º—ã)\n",
        "unigrams = get_ngrams(text, 1)\n",
        "print(f\"\\n1-–≥—Ä–∞–º–º—ã: {unigrams}\")\n",
        "\n",
        "# –ë–∏–≥—Ä–∞–º–º—ã (2-–≥—Ä–∞–º–º—ã)\n",
        "bigrams = get_ngrams(text, 2)\n",
        "print(f\"\\n2-–≥—Ä–∞–º–º—ã: {bigrams}\")\n",
        "\n",
        "# –¢—Ä–∏–≥—Ä–∞–º–º—ã (3-–≥—Ä–∞–º–º—ã)\n",
        "trigrams = get_ngrams(text, 3)\n",
        "print(f\"\\n3-–≥—Ä–∞–º–º—ã: {trigrams}\")\n",
        "\n",
        "# N-–≥—Ä–∞–º–º—ã —á–µ—Ä–µ–∑ sklearn\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"N-–≥—Ä–∞–º–º—ã —á–µ—Ä–µ–∑ TfidfVectorizer:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "documents = [\n",
        "    \"the quick brown fox\",\n",
        "    \"the lazy brown dog\",\n",
        "]\n",
        "\n",
        "# –¢–æ–ª—å–∫–æ –±–∏–≥—Ä–∞–º–º—ã\n",
        "bigram_vectorizer = TfidfVectorizer(ngram_range=(2, 2))\n",
        "X_bigrams = bigram_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"\\n–ë–∏–≥—Ä–∞–º–º—ã (ngram_range=(2,2)):\")\n",
        "print(f\"  {list(bigram_vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "# –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —É–Ω–∏–≥—Ä–º–º –∏ –±–∏–≥—Ä–∞–º–º\n",
        "combined_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "X_combined = combined_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"\\n–£–Ω–∏–≥—Ä–∞–º–º—ã + –ë–∏–≥—Ä–∞–º–º—ã (ngram_range=(1,2)):\")\n",
        "print(f\"  {list(combined_vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "print(f\"\\n–ú–∞—Ç—Ä–∏—Ü–∞ —Ñ–æ—Ä–º—ã: {X_combined.shape}\")\n",
        "print(f\"  (–¥–æ–∫—É–º–µ–Ω—Ç—ã: {X_combined.shape[0]}, –ø—Ä–∏–∑–Ω–∞–∫–∏: {X_combined.shape[1]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. BM25 –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "BM25 –î–õ–Ø –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –ü–û–ò–°–ö–ê\n",
            "======================================================================\n",
            "\n",
            "–ö–æ—Ä–ø—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n",
            "  1. machine learning is a subset of artificial intelligence\n",
            "  2. deep learning uses neural networks with multiple layers\n",
            "  3. natural language processing analyzes text data\n",
            "  4. computer vision processes visual information\n",
            "  5. neural networks are inspired by biological neurons\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê BM25:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "–ó–∞–ø—Ä–æ—Å: 'neural networks learning'\n",
            "  –î–æ–∫—É–º–µ–Ω—Ç   | –û—Ü–µ–Ω–∫–∞ BM25 | –†–µ–∑—É–ª—å—Ç–∞—Ç\n",
            "  ----------------------------------------\n",
            "  Doc 1     |     0.3117 | ‚ñà‚ñà‚ñà\n",
            "  Doc 2     |     0.9352 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "  Doc 3     |     0.0000 | \n",
            "  Doc 4     |     0.0000 | \n",
            "  Doc 5     |     0.6642 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "\n",
            "–ó–∞–ø—Ä–æ—Å: 'text processing language'\n",
            "  –î–æ–∫—É–º–µ–Ω—Ç   | –û—Ü–µ–Ω–∫–∞ BM25 | –†–µ–∑—É–ª—å—Ç–∞—Ç\n",
            "  ----------------------------------------\n",
            "  Doc 1     |     0.0000 | \n",
            "  Doc 2     |     0.0000 | \n",
            "  Doc 3     |     3.4801 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "  Doc 4     |     0.0000 | \n",
            "  Doc 5     |     0.0000 | \n",
            "\n",
            "–ó–∞–ø—Ä–æ—Å: 'artificial intelligence'\n",
            "  –î–æ–∫—É–º–µ–Ω—Ç   | –û—Ü–µ–Ω–∫–∞ BM25 | –†–µ–∑—É–ª—å—Ç–∞—Ç\n",
            "  ----------------------------------------\n",
            "  Doc 1     |     2.0356 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            "  Doc 2     |     0.0000 | \n",
            "  Doc 3     |     0.0000 | \n",
            "  Doc 4     |     0.0000 | \n",
            "  Doc 5     |     0.0000 | \n",
            "\n",
            "–§–æ—Ä–º—É–ª–∞ BM25:\n",
            "  –£—á–∏—Ç—ã–≤–∞–µ—Ç:\n",
            "    1. –ß–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (TF)\n",
            "    2. –û–±—Ä–∞—Ç–Ω—É—é —á–∞—Å—Ç–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (IDF)\n",
            "    3. –î–ª–∏–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)\n",
            "    4. –ù–∞—Å—ã—â–µ–Ω–∏–µ (diminishing returns –¥–ª—è —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤)\n",
            "  ‚úì –õ—É—á—à–µ —á–µ–º TF-IDF –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞\n"
          ]
        }
      ],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"BM25 –î–õ–Ø –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –ü–û–ò–°–ö–ê\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –ö–æ—Ä–ø—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "documents = [\n",
        "    \"machine learning is a subset of artificial intelligence\",\n",
        "    \"deep learning uses neural networks with multiple layers\",\n",
        "    \"natural language processing analyzes text data\",\n",
        "    \"computer vision processes visual information\",\n",
        "    \"neural networks are inspired by biological neurons\",\n",
        "]\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º\n",
        "tokenized_docs = [doc.lower().split() for doc in documents]\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º BM25\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "# –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã\n",
        "queries = [\n",
        "    \"neural networks learning\",\n",
        "    \"text processing language\",\n",
        "    \"artificial intelligence\",\n",
        "]\n",
        "\n",
        "print(\"\\n–ö–æ—Ä–ø—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"  {i}. {doc}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "print(\"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê BM25:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for query in queries:\n",
        "    tokenized_query = query.lower().split()\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    \n",
        "    print(f\"\\n–ó–∞–ø—Ä–æ—Å: '{query}'\")\n",
        "    print(\"  –î–æ–∫—É–º–µ–Ω—Ç   | –û—Ü–µ–Ω–∫–∞ BM25 | –†–µ–∑—É–ª—å—Ç–∞—Ç\")\n",
        "    print(\"  \" + \"-\" * 40)\n",
        "    \n",
        "    for i, score in enumerate(scores, 1):\n",
        "        bar = '‚ñà' * int(score * 10)\n",
        "        print(f\"  Doc {i}     | {score:10.4f} | {bar}\")\n",
        "\n",
        "print(\"\\n–§–æ—Ä–º—É–ª–∞ BM25:\")\n",
        "print(\"  –£—á–∏—Ç—ã–≤–∞–µ—Ç:\")\n",
        "print(\"    1. –ß–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (TF)\")\n",
        "print(\"    2. –û–±—Ä–∞—Ç–Ω—É—é —á–∞—Å—Ç–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (IDF)\")\n",
        "print(\"    3. –î–ª–∏–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)\")\n",
        "print(\"    4. –ù–∞—Å—ã—â–µ–Ω–∏–µ (diminishing returns –¥–ª—è —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤)\")\n",
        "print(\"  ‚úì –õ—É—á—à–µ —á–µ–º TF-IDF –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "–ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –ó–ê–î–ê–ß–ò –ò –í–´–ó–û–í–´\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "–ó–ê–î–ê–ß–ê 1: –í—ã–±–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\n",
            "======================================================================\n",
            "\n",
            "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–∞—Ç—Ä–∏—Ü:\n",
            "  BoW             ‚Üí (5, 18) (–¥–æ–∫—É–º–µ–Ω—Ç—ã √ó –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
            "  TF-IDF          ‚Üí (5, 18) (–¥–æ–∫—É–º–µ–Ω—Ç—ã √ó –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
            "  Bigram—ã         ‚Üí (5, 15) (–¥–æ–∫—É–º–µ–Ω—Ç—ã √ó –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
            "\n",
            "–¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n",
            "  Doc 1: –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç(0.52), –ø–∞—Ä–ª–∞–º–µ–Ω—Ç(0.52), –º–∏–Ω–∏—Å—Ç—Ä(0.52)\n",
            "  Doc 2: —á–µ–º–ø–∏–æ–Ω–∞—Ç(0.52), —Ñ—É—Ç–±–æ–ª(0.52), –æ–ª–∏–º–ø–∏–∞–¥–∞(0.52)\n",
            "  Doc 3: —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è(0.50), –ø—Ä–æ–≥—Ä–∞–º–º–∞(0.50), –∏–Ω—Ç–µ—Ä–Ω–µ—Ç(0.50)\n",
            "  Doc 4: –≤—ã–±–æ—Ä—ã(0.52), –≥–æ–ª–æ—Å(0.52), –∑–∞–∫–æ–Ω(0.52)\n",
            "  Doc 5: —Å—Ç–∞–¥–∏–æ–Ω(0.52), –∏–≥—Ä–æ–∫(0.52), –∫–æ–º–∞–Ω–¥–∞(0.52)\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"–ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –ó–ê–î–ê–ß–ò –ò –í–´–ó–û–í–´\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –ó–ê–î–ê–ß–ê 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–π –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"–ó–ê–î–ê–ß–ê 1: –í—ã–±–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# –ö–æ—Ä–ø—É—Å –Ω–æ–≤–æ—Å—Ç–µ–π (–ø—Ä–∏–º–µ—Ä)\n",
        "news_docs = [\n",
        "    \"–ø–æ–ª–∏—Ç–∏–∫–∞ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –º–∏–Ω–∏—Å—Ç—Ä –ø–∞—Ä–ª–∞–º–µ–Ω—Ç\",\n",
        "    \"—Å–ø–æ—Ä—Ç —Ñ—É—Ç–±–æ–ª –æ–ª–∏–º–ø–∏–∞–¥–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç\",\n",
        "    \"—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –∫–æ–º–ø—å—é—Ç–µ—Ä –ø—Ä–æ–≥—Ä–∞–º–º–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç\",\n",
        "    \"–ø–æ–ª–∏—Ç–∏–∫–∞ –≤—ã–±–æ—Ä—ã –≥–æ–ª–æ—Å –∑–∞–∫–æ–Ω\",\n",
        "    \"—Å–ø–æ—Ä—Ç –∫–æ–º–∞–Ω–¥–∞ –∏–≥—Ä–æ–∫ —Å—Ç–∞–¥–∏–æ–Ω\",\n",
        "]\n",
        "\n",
        "# –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã\n",
        "approaches = {\n",
        "    'BoW': CountVectorizer(),\n",
        "    'TF-IDF': TfidfVectorizer(),\n",
        "    'Bigram—ã': TfidfVectorizer(ngram_range=(2, 2)),\n",
        "}\n",
        "\n",
        "print(\"\\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–∞—Ç—Ä–∏—Ü:\")\n",
        "for name, vectorizer in approaches.items():\n",
        "    X = vectorizer.fit_transform(news_docs)\n",
        "    print(f\"  {name:15} ‚Üí {X.shape} (–¥–æ–∫—É–º–µ–Ω—Ç—ã √ó –ø—Ä–∏–∑–Ω–∞–∫–∏)\")\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑: –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–∂–Ω—ã?\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vec.fit_transform(news_docs)\n",
        "\n",
        "print(\"\\n–¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:\")\n",
        "feature_names = tfidf_vec.get_feature_names_out()\n",
        "for doc_idx in range(len(news_docs)):\n",
        "    top_indices = X_tfidf[doc_idx].toarray()[0].argsort()[-3:][::-1]\n",
        "    top_features = [feature_names[i] for i in top_indices]\n",
        "    top_scores = X_tfidf[doc_idx].toarray()[0][top_indices]\n",
        "    print(f\"  Doc {doc_idx + 1}: {', '.join([f'{f}({s:.2f})' for f, s in zip(top_features, top_scores)])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "–ó–ê–î–ê–ß–ê 2: –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã\n",
            "======================================================================\n",
            "\n",
            "–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:\n",
            "  –ö–ª–∞—Å—Å 1 (–ø–æ–∑–∏—Ç–∏–≤): 15 –ø—Ä–∏–º–µ—Ä–æ–≤ (88.2%)\n",
            "  –ö–ª–∞—Å—Å 0 (–Ω–µ–≥–∞—Ç–∏–≤): 2 –ø—Ä–∏–º–µ—Ä–æ–≤ (11.8%)\n",
            "  –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: 7.5:1\n",
            "\n",
            "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     –ù–µ–≥–∞—Ç–∏–≤       1.00      1.00      1.00         2\n",
            "     –ü–æ–∑–∏—Ç–∏–≤       1.00      1.00      1.00        15\n",
            "\n",
            "    accuracy                           1.00        17\n",
            "   macro avg       1.00      1.00      1.00        17\n",
            "weighted avg       1.00      1.00      1.00        17\n",
            "\n",
            "\n",
            "–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: class_weight='balanced' –¥–∞–µ—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å —Ä–µ–¥–∫–æ–º—É –∫–ª–∞—Å—Å—É\n"
          ]
        }
      ],
      "source": [
        "# –ó–ê–î–ê–ß–ê 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"–ó–ê–î–ê–ß–ê 2: –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# –°–∏–º—É–ª–∏—Ä—É–µ–º –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä\n",
        "positive_docs = [\n",
        "    \"–æ—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω\",\n",
        "    \"—Å—É–ø–µ—Ä –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –≤—Å–µ–º\",\n",
        "    \"–ª—É—á—à–∏–π –≤—ã–±–æ—Ä –∫–æ–≥–¥–∞ –ª–∏–±–æ\",\n",
        "] * 5  # –º–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "\n",
        "negative_docs = [\n",
        "    \"—É–∂–∞—Å–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\",\n",
        "    \"—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω —Å–æ–≤—Å–µ–º\",\n",
        "]  # –º–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
        "\n",
        "all_docs = positive_docs + negative_docs\n",
        "labels = [1] * len(positive_docs) + [0] * len(negative_docs)\n",
        "\n",
        "print(\"\\n–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "print(f\"  –ö–ª–∞—Å—Å 1 (–ø–æ–∑–∏—Ç–∏–≤): {sum(labels)} –ø—Ä–∏–º–µ—Ä–æ–≤ ({sum(labels)/len(labels)*100:.1f}%)\")\n",
        "print(f\"  –ö–ª–∞—Å—Å 0 (–Ω–µ–≥–∞—Ç–∏–≤): {len(labels)-sum(labels)} –ø—Ä–∏–º–µ—Ä–æ–≤ ({(1-sum(labels)/len(labels))*100:.1f}%)\")\n",
        "print(f\"  –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {sum(labels)/(len(labels)-sum(labels)):.1f}:1\")\n",
        "\n",
        "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(all_docs)\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ\n",
        "clf = LogisticRegression(class_weight='balanced')  # –í–∞–∂–Ω–æ! balanced weights\n",
        "clf.fit(X, labels)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "y_pred = clf.predict(X)\n",
        "\n",
        "print(\"\\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ:\")\n",
        "print(classification_report(labels, y_pred, target_names=['–ù–µ–≥–∞—Ç–∏–≤', '–ü–æ–∑–∏—Ç–∏–≤']))\n",
        "\n",
        "print(\"\\n–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: class_weight='balanced' –¥–∞–µ—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å —Ä–µ–¥–∫–æ–º—É –∫–ª–∞—Å—Å—É\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "–ó–ê–î–ê–ß–ê 3: PMI –∏ NPMI –¥–ª—è –∫–æ–ª–ª–æ–∫–∞—Ü–∏–π\n",
            "======================================================================\n",
            "\n",
            "–¢–æ–ø 10 –∫–æ–ª–ª–æ–∫–∞—Ü–∏–π –ø–æ PMI:\n",
            "–ö–æ–ª–ª–æ–∫–∞—Ü–∏—è                            PMI       NPMI  –ß–∞—Å—Ç–æ—Ç–∞\n",
            "----------------------------------------------------------------------\n",
            "systems + use                      3.9512     1.5405        2\n",
            "learn + networks                   3.9512     1.5405        2\n",
            "learn + patterns.                  3.9512     1.5405        2\n",
            "is + powerful.                     3.2581     1.2702        2\n",
            "networks + patterns.               3.2581     1.0000        1\n",
            "and + deep                         3.2581     1.2702        2\n",
            "are + related.                     3.2581     1.0000        1\n",
            "networks. + neural                 3.0758     1.8656        5\n",
            "machine + powerful.                2.8526     1.1122        2\n",
            "neural + use                       2.8526     1.1122        2\n",
            "\n",
            "–í—ã—Å–æ–∫–∏–π PMI ‚Üí —Å–ª–æ–≤–∞ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ (–∫–æ–ª–ª–æ–∫–∞—Ü–∏—è)\n",
            "–ù–∏–∑–∫–∏–π/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π PMI ‚Üí —Å–ª–æ–≤–∞ –∏–∑–±–µ–≥–∞—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞ (–∞–Ω—Ç–∏–∞—Å—Å–æ—Ü–∏–∞—Ü–∏—è)\n"
          ]
        }
      ],
      "source": [
        "# –ó–ê–î–ê–ß–ê 3: PMI –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–ª–æ–∫–∞—Ü–∏–π\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"–ó–ê–î–ê–ß–ê 3: PMI –∏ NPMI –¥–ª—è –∫–æ–ª–ª–æ–∫–∞—Ü–∏–π\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def compute_pmi_npmi(tokens, window_size=2):\n",
        "    \"\"\"\n",
        "    PMI (Pointwise Mutual Information) = log(P(w1, w2) / (P(w1) * P(w2)))\n",
        "    NPMI (Normalized PMI) = PMI / -log(P(w1, w2))\n",
        "    \"\"\"\n",
        "    from itertools import combinations\n",
        "    from collections import defaultdict\n",
        "    \n",
        "    vocab_freq = Counter(tokens)  # P(w)\n",
        "    pair_freq = defaultdict(int)  # P(w1, w2)\n",
        "    \n",
        "    # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã (–æ–∫–Ω–æ —Å–ª–æ–≤)\n",
        "    for i in range(len(tokens) - window_size + 1):\n",
        "        window = tokens[i:i+window_size]\n",
        "        for pair in combinations(sorted(set(window)), 2):\n",
        "            pair_freq[pair] += 1\n",
        "    \n",
        "    total = sum(vocab_freq.values())\n",
        "    pmi_scores = {}\n",
        "    \n",
        "    for (w1, w2), count in pair_freq.items():\n",
        "        p_pair = count / total\n",
        "        p_w1 = vocab_freq[w1] / total\n",
        "        p_w2 = vocab_freq[w2] / total\n",
        "        \n",
        "        pmi = math.log(p_pair / (p_w1 * p_w2))\n",
        "        npmi = pmi / -math.log(p_pair) if p_pair > 0 else 0\n",
        "        \n",
        "        pmi_scores[(w1, w2)] = (pmi, npmi, count)\n",
        "    \n",
        "    return pmi_scores\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤\n",
        "text = \"\"\"machine learning is powerful. machine learning systems use neural networks.\n",
        "deep learning is neural networks. neural networks learn patterns.\n",
        "machine learning and deep learning are related.\"\"\"\n",
        "\n",
        "tokens = text.lower().split()\n",
        "pmi_scores = compute_pmi_npmi(tokens, window_size=3)\n",
        "\n",
        "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ PMI\n",
        "sorted_pmi = sorted(pmi_scores.items(), key=lambda x: x[1][0], reverse=True)\n",
        "\n",
        "print(\"\\n–¢–æ–ø 10 –∫–æ–ª–ª–æ–∫–∞—Ü–∏–π –ø–æ PMI:\")\n",
        "print(f\"{'–ö–æ–ª–ª–æ–∫–∞—Ü–∏—è':<30} {'PMI':>10} {'NPMI':>10} {'–ß–∞—Å—Ç–æ—Ç–∞':>8}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for (w1, w2), (pmi, npmi, count) in sorted_pmi[:10]:\n",
        "    print(f\"{w1 + ' + ' + w2:<30} {pmi:10.4f} {npmi:10.4f} {count:8}\")\n",
        "\n",
        "print(\"\\n–í—ã—Å–æ–∫–∏–π PMI ‚Üí —Å–ª–æ–≤–∞ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ (–∫–æ–ª–ª–æ–∫–∞—Ü–∏—è)\")\n",
        "print(\"–ù–∏–∑–∫–∏–π/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π PMI ‚Üí —Å–ª–æ–≤–∞ –∏–∑–±–µ–≥–∞—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞ (–∞–Ω—Ç–∏–∞—Å—Å–æ—Ü–∏–∞—Ü–∏—è)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
