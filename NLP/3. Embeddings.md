# Векторные представления слов: от Word2Vec к современным эмбеддингам

Представьте, что вы хотите рассказать компьютеру, что слово "король" похоже на слово "королева". Как вы это сделаете? Численно?

Один из самых красивых результатов в NLP — это то, что мы можем представить слова в виде **векторов в многомерном пространстве**, где **близость соответствует семантическому сходству**.

$$\text{king} - \text{man} + \text{woman} \approx \text{queen}$$

Это не метафора. Это реальная математика. Эта лекция разберёт, как это работает и почему.

---

## Мотивация: от дискретных слов к непрерывным векторам

### Проблема дискретного представления

До эмбеддингов слова представляли как:

**One-hot encoding:**

```
"cat"   = [1, 0, 0, 0, ...]
"dog"   = [0, 1, 0, 0, ...]
"kitten" = [0, 0, 1, 0, ...]
```

**Проблемы:**

1. **Ортогональность:** все слова одинаково далеки друг от друга. "cat" и "kitten" не похожи (косинусное сходство = 0).
2. **Размерность:** словарь из 100k слов = вектор размера 100k, почти весь нулевой.
3. **Нет семантики:** числовое расстояние не означает семантическое сходство.

### Решение: непрерывные векторы

Идея эмбеддингов: **представить слово как вектор в 50- или 300-мерном пространстве, где близкие слова имеют близкие векторы.**

```
cat    = [0.2, -0.5, 0.1, ..., 0.3]    (50 или 300 компонент)
kitten = [0.1, -0.4, 0.2, ..., 0.4]    ← похожи на "cat"
dog    = [-0.3, 0.6, -0.1, ..., 0.2]   ← отличаются от "cat"
```

Косинусное сходство:

$$\cos\left(\text{cat}, \text{kitten}\right) = \frac{\vec{\text{cat}} \cdot \vec{\text{kitten}}}{\|\vec{\text{cat}}\| \cdot \|\vec{\text{kitten}}\|} \approx 0.95$$

$$\cos\left(\text{cat}, \text{dog}\right) \approx 0.3$$

Теперь компьютер понимает, что "cat" и "kitten" похожи.

> [!IMPORTANT]
> Эмбеддинги — это один из самых важных прорывов в NLP. Они позволили перейти от дискретных, вручную кодируемых признаков к **выученным непрерывным представлениям**, которые захватывают семантику.

---

## Word2Vec: революция в эмбеддингах

В 2013 году исследователи из Google (Mikolov et al.) представили **Word2Vec** — метод, который привел к революции в NLP.

**Ключевая идея:** "You shall know a word by the company it keeps" (Wirth, 1957).

Слово определяется теми словами, которые его окружают.

### Skip-gram архитектура

**Задача:** предсказать контекстные слова, зная целевое слово.

Дано предложение: *"The quick brown fox jumps"*

С окном size = 2 пары (target, context):

```
quick → [the, brown]      (из контекста слова "quick")
brown → [quick, fox]
fox   → [brown, jumps]
...
```

**Skip-gram сеть:**

```
Вход: слово "quick"
  ↓
Эмбеддинг: W[quick] (50-мерный вектор)
  ↓
Линейный слой: h = W[quick]
  ↓
Output layer: вероятности всех слов словаря
  ↓
Выход: P("the" | "quick"), P("brown" | "quick"), P("jumps" | "quick"), ...
```

**Математика:**

Для каждой пары (target $w_t$, context $w_c$):

$$P(w_c | w_t) = \frac{\exp(\vec{w_c} \cdot \vec{w_t})}{\sum_{w=1}^{V} \exp(\vec{w} \cdot \vec{w_t})}$$

Loss (перекрёстная энтропия):

$$L = -\log P(w_c | w_t) = -\vec{w_c} \cdot \vec{w_t} + \log\sum_{w=1}^{V} \exp(\vec{w} \cdot \vec{w_t})$$

Обучаем методом SGD, обновляя эмбеддинги:

$$\vec{w_t} \leftarrow \vec{w_t} - \alpha \frac{\partial L}{\partial \vec{w_t}}$$

**Результат:** после обучения на большом корпусе, слова с похожим контекстом получают похожие векторы.

> [!TIP]
> Skip-gram хорошо работает на **редких словах** (они встречаются в специфичном контексте). CBOW хорошо работает на **частых словах** (они встречаются везде).

### CBOW: Continuous Bag of Words

**Обратная задача:** предсказать слово, зная контекст.

```
[the, brown] → predict "quick"
[quick, fox] → predict "brown"
```

**Архитектура:**

```
Вход: [the, brown]
  ↓
Эмбеддинги: [W[the], W[brown]]
  ↓
Усреднение: h = (W[the] + W[brown]) / 2
  ↓
Output layer
  ↓
Выход: P(quick | context)
```

**Различия Skip-gram vs CBOW:**

| Аспект | Skip-gram | CBOW |
|--------|-----------|------|
| Задача | target → context | context → target |
| Скорость | Медленнее (V выходов) | Быстрее (1 выход) |
| Редкие слова | ⭐⭐⭐ | ⭐ |
| Частые слова | ⭐⭐ | ⭐⭐⭐ |
| Требуемые данные | Больше | Меньше |

**Интуиция:**

- **Skip-gram:** "Если я вижу 'king', я должен предсказать 'queen', 'prince', 'royal'". Сильно штрафуется, если вектор 'king' далеко от этих слов. → Редкие слова получают отличные представления.
- **CBOW:** "Если я вижу 'queen', 'prince', 'royal' вместе, они должны указывать на 'king'". Менее строго для редких слов. → Частые слова получают отличные представления.

### Оптимизация: Negative Sampling и Hierarchical Softmax

**Проблема:** полный softmax имеет O(V) complexity, где V = размер словаря (часто 100k+).

Вычислить для каждого примера:

$$\frac{\exp(\vec{w_c} \cdot \vec{w_t})}{\sum_{w=1}^{V} \exp(\vec{w} \cdot \vec{w_t})}$$

слишком дорого.

#### Negative Sampling

**Идея:** вместо предсказания всех слов, предсказываем:

1. **Positive pair** (целевая пара): предсказать, что это положительная пара
2. **Negative samples** (случайные слова): предсказать, что это отрицательные пары

**Loss становится:**

$$L = -\log \sigma(\vec{w_c} \cdot \vec{w_t}) - \sum_{i=1}^{k} \log \sigma(-\vec{w_i} \cdot \vec{w_t})$$

где:
- $\sigma(x) = \frac{1}{1 + e^{-x}}$ — сигмоид
- Первый член: максимизируем сходство positive пары
- Второй член: минимизируем сходство с k случайными словами (negative samples)

**Преимущества:**

- O(k) вместо O(V), где k ~ 5-20 (k << V)
- Практически не теряем качество
- Можем выбирать negative samples умно (частые слова штрафуются больше)

#### Иерархический Softmax

Альтернатива negative sampling: организуем словарь как бинарное дерево (Huffman tree).

```
                    root
                   /    \
               node1      node2
              /    \      /    \
           cat   dog   queen   king
```

Для каждого слова — путь от корня (log V операций вместо V).

> [!NOTE]
> На практике Negative Sampling быстрее и часто дает лучше результаты. Иерархический Softmax более сложен в реализации.

---

## GloVe: глобальный контекст

Word2Vec fokusiуется на **локальном контексте** (слова в окне).

**GloVe** (Global Vectors) объединяет **локальный контекст** (как Word2Vec) с **глобальной статистикой** (матрица совместного появления слов).

### Матрица совместного появления

$$X_{ij} = \text{количество раз слово j встречается в контексте слова i}$$

Пример (корпус: "King queen prince royal"):

```
       king  queen  prince  royal
king    0     2      1       1
queen   2     0      1       1
prince  1     1      0       1
royal   1     1      1       0
```

Word2Vec использует эту информацию неявно (через контекстные пары).
GloVe использует её явно.

### Функция потерь GloVe

$$L = \sum_{i,j} f(X_{ij}) (\vec{w_i} \cdot \vec{w_j} - \log X_{ij})^2$$

где $f(X_{ij})$ — весовая функция (игнорирует редкие пары, не переобучается на частых).

**Интуиция:**

- Скалярное произведение $\vec{w_i} \cdot \vec{w_j}$ должно быть близко к $\log X_{ij}$
- Если слова часто встречаются вместе: $X_{ij}$ большой → $\vec{w_i} \cdot \vec{w_j}$ большой
- Эмбеддинги отражают **глобальную структуру** корпуса

**GloVe vs Word2Vec:**

| Критерий | Word2Vec | GloVe |
|----------|----------|-------|
| Скорость обучения | Быстро | Медленнее |
| Использует глобальную статистику | Неявно | Явно |
| Качество на малых данных | Хорошо | Лучше |
| Интерпретируемость | Хорошо | Лучше (матрица X явно видна) |
| На практике | Word2Vec часто достаточно | GloVe лучше для аналитики |

---

## FastText: подсловные эмбеддинги

**Проблема Word2Vec:** что делать с OOV (out-of-vocabulary) словами?

```python
model = Word2Vec([...])
vec = model.wv["magnetohydrodynamic"]  # KeyError: слова нет в словаре
```

### Идея FastText

**Решение:** представить слово не как атомарную единицу, а как **сумму подсловных эмбеддингов**.

Слово: "magnetohydrodynamic"

**Подслова (n-граммы):**

$$\text{n-граммы} = [mag, mag, agn, gne, net, ..., nic, ic> ]$$

(3-граммы для примера)

**Эмбеддинг слова:**

$$\vec{w_{\text{word}}} = \frac{1}{|S|} \sum_{g \in S} \vec{w_g}$$

где $S$ — множество подслов, $\vec{w_g}$ — эмбеддинг подслова.

**Преимущества:**

1. **OOV обработка:** опечатка "magneetohydrodynamic" всё ещё содержит большинство подслов → похожий вектор
2. **Морфология:** слова с похожей структурой автоматически похожи
3. **Редкие слова:** даже если слово встречается один раз, его подслова встречаются часто

### Практическое применение

```python
from gensim.models import FastText

model = FastText(sentences, min_n=3, max_n=6, vector_size=100)

# Теперь работает даже для OOV слов
vec = model.wv["magnetohydrodynamic"]  ✓
vec = model.wv["misspeled"]             ✓ (похож на "misspelled")
```

**FastText особенно полезен для:**
- Морфологически богатых языков (русский, финский, турецкий)
- Предотвращения опечаток
- Дефектных данных с шумом

> [!WARNING]
> FastText медленнее и требует больше памяти, чем Word2Vec. Используйте только если OOV слова частые или морфология важна.

---

## Оценка качества эмбеддингов

### Intrinsic evaluation: внутренняя оценка

#### 1. Аналогии

**Семантические аналогии:**

```
paris : france :: tokyo : japan
king : queen :: prince : princess
```

**Синтаксические аналогии:**

```
run : running :: walk : walking
big : bigger :: tall : taller
```

**Как работает:**

Найти слово $d$ такое что:

$$\vec{d} \approx \vec{c} - \vec{a} + \vec{b}$$

```python
# king : queen = prince : ?
model.wv.most_similar(positive=['queen', 'prince'], negative=['king'])
# → princess
```

**Метрика:** процент корректных аналогий на датасете.

#### 2. Ближайшие соседи и сходство

**Ближайшие соседи:** для слова найти топ-k похожих слов вручную оцените, имеют ли они смысл.

**Сходство:** дана пара слов, люди оценивают сходство (1-10). Коррелируем с косинусным сходством эмбеддингов.

```
Датасет (SimLex-999, Rare Word, WordSim-353):
monkey  baboon    → human rating: 8.3 (очень похожи)
model.wv.similarity('monkey', 'baboon') = 0.85  ✓ (высокий score)

knife   fork      → human rating: 5.9
model.wv.similarity('knife', 'fork') = 0.61  ✓ (средний score)
```

**Метрика:** корреляция Спирмана между человеческими оценками и моделью.

### Extrinsic evaluation: внешняя оценка

Используйте эмбеддинги в реальной задаче и оцените качество.

#### Классификация текстов

```python
# Дан текст, его эмбеддинг = среднее эмбеддингов слов
doc_embedding = mean([model.wv[word] for word in doc])

# Обучаем классификатор
clf = LogisticRegression()
clf.fit(embeddings, labels)

# Оцениваем на test выборке
accuracy = clf.score(X_test, y_test)
```

#### Clustering

```python
# Кластеризуем слова по эмбеддингам
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=10)
clusters = kmeans.fit_predict(embeddings)

# Оцениваем качество кластеров (NMI, ARI)
```

> [!TIP]
> Intrinsic evaluation быстрое (пара минут), но не гарантирует success в real task. Extrinsic evaluation медленнее, но более надежно показывает, работает ли модель.

---

## Практические советы

### Выбор между методами

$$\text{Выбор} = \begin{cases}
\text{Skip-gram} & \text{если: большой корпус, редкие слова важны} \\
\text{CBOW} & \text{если: маленький корпус, нужно быстро} \\
\text{FastText} & \text{если: OOV слова, морфология важна (русский)} \\
\text{GloVe} & \text{если: хотите глобальный контекст, аналитику}
\end{cases}$$

### Гиперпараметры

- **vector_size** (50-300): больше = лучше качество, но медленнее, требует больше данных
- **window** (2-10): маленький = синтаксис, большой = тематика
- **min_count** (1-5): удаляет редкие слова, уменьшает словарь
- **negative** (5-25): больше = лучше, но медленнее
- **epochs** (5-30): обычно 10-20

### Обучение

```python
# Рекомендуемый pipeline
from gensim.models import Word2Vec

model = Word2Vec(
    tokenized_sentences,
    sg=1,                    # Skip-gram
    vector_size=100,         # 100-мерные векторы
    window=5,                # контекст ±5 слов
    min_count=2,             # слова встречаются >= 2 раза
    workers=4,               # параллелизм
    negative=10,             # 10 negative samples
    epochs=20,               # 20 эпох обучения
    seed=42                  # reproducibility
)

# Оценка
print(model.wv.most_similar('king', topn=5))
print(model.wv.similarity('king', 'queen'))
```

### Common gotchas

**1. Недостаточно данных**
- Эмбеддинги требуют большого корпуса (>= 1M слов рекомендуется)
- Для маленьких данных используйте pre-trained модели

**2. Плохая токенизация**
- Плохая токенизация → плохие эмбеддинги
- Всегда проверяйте примеры токенов

**3. Забыли нормализацию**
```python
# ❌ Плохо
sentences = [doc.split() for doc in raw_texts]

# ✓ Хорошо
sentences = [[token.lower() for token in word_tokenize(doc)]
             for doc in raw_texts]
```

**4. Переобучение на малых данных**
- Используйте pre-trained эмбеддинги (Word2Vec русский, FastText)
- Или fine-tune их на вашем домене

---

## От Word2Vec к современности

Word2Vec (2013) был революцией, но с тех пор появилось много улучшений:

- **GloVe** (2014): объединяет локальный и глобальный контекст
- **FastText** (2016): подсловные эмбеддинги
- **ELMo** (2018): контекстные эмбеддинги (разные для разных контекстов)
- **BERT** (2018): двунаправленные трансформеры
- **GPT** (2018+): авторегрессивные трансформеры

Ключевое улучшение: **контекстность**. Word2Vec дает один вектор для слова. BERT дает разные векторы в зависимости от контекста:

```
"bank" (река) vs "bank" (финансовое) → разные векторы в BERT
vs Word2Vec → один вектор для обоих
```

Но **Word2Vec и FastText остаются быстрым, надежным baseline'ом**, который часто достаточен для простых задач.

---

## Резюме

**Ключевые идеи:**

1. **Слова как векторы:** близость соответствует семантическому сходству
2. **Skip-gram:** предсказываем контекст, зная слово → хорошо для редких слов
3. **CBOW:** предсказываем слово, зная контекст → быстро для частых слов
4. **Negative Sampling:** эффективная оптимизация (O(k) вместо O(V))
5. **GloVe:** объединяет локальный и глобальный контекст
6. **FastText:** подсловные эмбеддинги для OOV и морфологии
7. **Оценка:** intrinsic (аналогии, сходство) и extrinsic (задачи)

**Для практики:**

```python
# Быстрый старт
from gensim.models import Word2Vec

# Для русского или OOV слов: используйте FastText
from gensim.models import FastText
model = FastText(sentences, vector_size=100, min_n=3, max_n=6)

# Оценка
print(model.wv.most_similar('machine'))
print(model.wv.similarity('machine', 'learning'))
```

Word2Vec и его вариации остаются фундаментом современного NLP. Понимание этих методов критично для глубокого обучения в NLP.