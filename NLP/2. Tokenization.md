# Предобработка, токенизация и извлечение признаков

От сырого текста к структурированным данным, готовым для машинного обучения, — это крайне нетривиальный процесс. Текст полон шума: HTML-теги, опечатки, разные регистры, диакритические знаки, пунктуация. Первая задача NLP — привести текст в порядок. Вторая — преобразовать его в числовое представление, которое может обработать модель.

В этой лекции мы разберём три ключевых этапа: токенизацию (разбиение на единицы), нормализацию (приведение к единому виду) и извлечение признаков (преобразование в векторы). Эти этапы отвечают на три вопроса:

1. **Как разбить текст на единицы обработки?** (токенизация)
2. **Как сделать текст чистым и унифицированным?** (нормализация)
3. **Как преобразовать текст в числа?** (извлечение признаков)

---

## Часть 1: Токенизация — разбиение текста на единицы

### Что такое токен и почему это не так просто, как кажется

**Токен** — это атомарная единица обработки текста. Обычно это слово, но может быть символ, подслово или даже фраза.

На первый взгляд, токенизация тривиальна: разделяем по пробелам. Но попробуйте разобраться:

```
Текст: "Mr. Smith e-mailed john@example.com at 5:00 p.m. to discuss it's a deal!"

Наивный split():
['Mr.', 'Smith', 'e-mailed', 'john@example.com', 'at', '5:00', 'p.m.', 'to', 'discuss', "it's", 'a', 'deal!']
                                                      ↑ Проблема: точка после p.m.

Результат: точка отделилась от 'p.m.', что неправильно!
```

Точки в аббревиатурах, апострофы в сокращениях, дефисы в сложных словах, email адреса, URL — всё это требует осторожности.

> [!IMPORTANT]
> Выбор метода токенизации влияет на качество всей системы. Неправильная токенизация распространяется через весь конвейер, загрязняя данные для обучения модели.

### Классические методы токенизации

#### 1. Простая токенизация по пробелам

```python
text = "Hello world!"
tokens = text.split()  # ['Hello', 'world!']
```

**Когда подходит:** никогда. Только для демонстраций.

**Проблемы:**
- Пунктуация слипается со словами
- Апострофы в сокращениях не разбираются
- Email адреса и URL разбиваются неправильно

#### 2. Regex-токенизация

```python
import re

text = "Mr. Smith e-mailed john@example.com"
tokens = re.findall(r'\b\w+\b|\S+', text)
# ['Mr', 'Smith', 'e', 'mailed', 'john@example.com']
```

**Преимущества:**
- Контролируем, что считать токеном
- Можем обработать специальные случаи (email, URL, числа)

**Недостатки:**
- Сложно написать универсальный regex
- Специфичен для каждого языка

**Практический пример:**

```python
import re

def advanced_tokenize(text):
    """Токенизация с обработкой специальных случаев"""
    patterns = [
        (r'https?://\S+', 'URL'),
        (r'\S+@\S+', 'EMAIL'),
        (r'\d+[.,]\d+', 'NUMBER'),  # 123.45 или 123,45
        (r"[A-Z][a-z]+", 'WORD'),   # capitalized words
        (r'[a-z]+', 'WORD'),        # lowercase words
        (r'[.,!?;:-]', 'PUNCT'),    # punctuation
    ]
    
    tokens = []
    for match in re.finditer('|'.join(p[0] for p in patterns), text):
        tokens.append(match.group())
    
    return tokens

tokens = advanced_tokenize("Visit https://example.com or email john@example.com")
# ['Visit', 'https://example.com', 'or', 'email', 'john@example.com']
```

#### 3. NLTK и spaCy: готовые решения

```python
from nltk.tokenize import word_tokenize
from nltk.data import find

text = "Mr. Smith said, 'Hello!'"

# NLTK
tokens = word_tokenize(text)
# ['Mr.', 'Smith', 'said', ',', "'", 'Hello', '!', "'"]

# spaCy (более продвинуто)
import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp(text)
tokens = [token.text for token in doc]
# ['Mr.', 'Smith', 'said', ',', "'", 'Hello', '!', "'"]
```

**Преимущества:**
- Учитывают грамматику языка
- Обрабатывают аббревиатуры
- Встроенная поддержка множества языков

**Недостатки:**
- Нужно загружать модели
- Медленнее, чем regex

> [!TIP]
> Для production систем используйте готовые решения (spaCy, NLTK). Для экспериментов и специфичных задач — пишите свои regex. Главное — **всегда тестируйте на реальных данных вашего домена**.

### Подсловная токенизация: когда слово — это не слово

Представьте, что у вас есть слово $\text{"magnetohydrodynamic"}$. Редкое слово, его нет в большинстве словарей. Но если мы разобьём его на подслова: $\text{[magneto][hydro][dynamic]}$ — каждое подслово знакомо моделям.

**Подсловная токенизация** разбивает редкие слова на более частые подслова. Это решает проблему OOV (Out-of-Vocabulary) слов и снижает размер словаря.

#### BPE (Byte Pair Encoding)

Алгоритм BPE работает так:

1. Начинаем со всех символов в словаре
2. Считаем частоту всех пар соседних символов/подслов
3. Заменяем самую частую пару на одну единицу
4. Повторяем N раз (N — гиперпараметр)

**Пример:**

```
Корпус: "dog", "cat", "cats"

Шаг 0 (символы):
  d o g: 1
  c a t: 1
  c a t s: 1

Шаг 1 (самая частая пара = "ca" встречается 2 раза):
  d o g: 1
  ca t: 1
  ca ts: 1

Шаг 2 (самая частая пара = "ca t" встречается в "cats"):
  d o g: 1
  ca t: 1
  cat s: 1

...и так далее
```

**Результат:** редкие слова представлены последовательностью частых подслов.

**Формула отработки:**

$$\text{tokenize}(\text{word}) = \text{apply learned merges}$$

где learned merges — это операции слияния, которые мы выучили на корпусе.

#### WordPiece, SentencePiece и Unigram LM

Существуют варианты BPE:

- **WordPiece** (BERT): выбирает пары на основе вероятности, а не частоты
- **SentencePiece** (T5, XLM-R): работает на уровне байтов, независим от языка
- **Unigram LM** (MeCab): probabilistic approach с динамическим программированием

Ключевая идея всех методов: **разбить редкие слова на частые подслова**.

```python
# Пример с transformers (BERT)
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

text = "The quick brown fox"
tokens = tokenizer.tokenize(text)
# ['the', 'quick', 'brown', 'fox']

# На редком слове:
text2 = "magnetohydrodynamic"
tokens2 = tokenizer.tokenize(text2)
# ['[UNK]'] или разбит на подслова, в зависимости от версии BERT
```

> [!WARNING]
> При переходе между токенизаторами (например, BERT vs GPT) **всегда проверяйте, как они токенизируют вашу доменную лексику**. Ошибка здесь может стоить часов отладки.

### OOV (Out-of-Vocabulary) слова: что делать с неизвестным?

Дан текст: *"The company XYZ123 announced UNKNOWN_WORD"*

Если слова `XYZ123` и `UNKNOWN_WORD` не в нашем словаре, что делать?

#### Стратегии обработки OOV

**1. Специальный токен `<UNK>`**

```python
vocab = {'the': 1, 'company': 2, 'announced': 3, '<UNK>': 0}
text = "The company XYZ123 announced"
tokens = [vocab.get(word, vocab['<UNK>']) for word in text.lower().split()]
# [1, 2, 0, 3]  ← XYZ123 → 0
```

Простая, но теряем информацию.

**2. Backoff к подсловам**

```python
# Если слова нет, разбиваем на символы или подслова
def tokenize_with_backoff(word, vocab, subword_vocab):
    if word in vocab:
        return [word]
    else:
        # Пробуем подслова
        for i in range(len(word), 0, -1):
            subword = word[:i]
            if subword in subword_vocab:
                rest = word[i:]
                return [subword] + tokenize_with_backoff(rest, vocab, subword_vocab)
        # Если ничего не нашли, символы
        return list(word)

tokens = tokenize_with_backoff('XYZ123', vocab, {})
# ['X', 'Y', 'Z', '1', '2', '3']
```

Сохраняет больше информации, используется в BERT и других моделях.

**3. Similarity-based lookup**

```python
from difflib import get_close_matches

def tokenize_with_similarity(word, vocab):
    if word in vocab:
        return word
    else:
        # Ищем похожее слово
        matches = get_close_matches(word, vocab.keys(), n=1, cutoff=0.6)
        if matches:
            return matches[0]
        else:
            return '<UNK>'

tokenize_with_similarity('anounced', vocab)  # → 'announced' (опечатка исправлена)
```

Хорошо для опечаток, но может быть ненадёжным.

---

## Часть 2: Нормализация — приведение текста в порядок

### Что нормализовать и почему

Текст, собранный из интернета, полон грязи:
- Разный регистр: "HELLO", "Hello", "hello"
- Диакритические знаки: "café" vs "cafe"
- HTML сущности: "&amp;", "&lt;"
- Множественные пробелы
- Невидимые символы Unicode

Нормализация приводит текст к единообразному виду, чтобы модель могла сосредоточиться на смысле, а не на форме.

### Пошаговая нормализация

```python
import unicodedata
import re
import html

def normalize_text(text, lowercase=True, remove_accents=True, 
                   remove_html=True, clean_whitespace=True):
    """Полная нормализация текста"""
    
    # 1. HTML декодирование
    if remove_html:
        text = html.unescape(text)  # "&amp;" → "&"
        text = re.sub(r'<[^>]+>', '', text)  # Удалить теги
    
    # 2. Регистр
    if lowercase:
        text = text.lower()
    
    # 3. Диакритические знаки
    if remove_accents:
        # NFD = разложение (café = c + a + f + e + ◌́)
        text = unicodedata.normalize('NFD', text)
        # Удалляем диакритику (категория 'Mn')
        text = ''.join(c for c in text 
                      if unicodedata.category(c) != 'Mn')
    
    # 4. Пробелы
    if clean_whitespace:
        text = re.sub(r'\s+', ' ', text)  # множественные → один
        text = text.strip()  # начало/конец
    
    return text

# Примеры
normalize_text("<p>Café naïve   RÉSUMÉ</p>")
# "cafe naive resume"
```

### Стемминг и лемматизация: нужны ли обе?

Хотя обе техники сводят словоформы к базовому виду, они делают это по-разному.

#### Стемминг: грубое отсечение окончаний

```python
from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer('english')

words = ['running', 'runs', 'ran', 'runner']
stems = [stemmer.stem(w) for w in words]
# ['run', 'run', 'ran', 'runner']
                          ↑ Не идеально!
```

**Алгоритм:** простые правила вроде "отсечь -ing, -ed, -s"

**Преимущества:**
- Очень быстро (просто regex)
- Не требует словаря
- Работает даже для редких слов

**Недостатки:**
- Может дать не реальное слово ("comput" для "computing")
- Язык-специфичен (нужны разные правила для разных языков)

#### Лемматизация: поиск базовой формы

```python
import pymorphy2

morph = pymorphy2.MorphAnalyzer()

words = ['бежали', 'бегущий', 'бегун']
lemmas = [morph.parse(w)[0].normal_form for w in words]
# ['бежать', 'бегущий', 'бегун']
               ↑ Не сводит к одной лемме
```

**Алгоритм:** словарь + морфологический анализ

**Преимущества:**
- Возвращает реальное слово
- Учитывает морфологию и контекст

**Недостатки:**
- Медленнее (требует словаря)
- Может быть неоднозначной ("завод" = завод или завели?)

#### Когда что использовать?

$$\text{Выбор} = \begin{cases}
\text{Стемминг} & \text{если: быстро нужно, данных мало, язык простой} \\
\text{Лемматизация} & \text{если: качество важно, есть хороший словарь}
\end{cases}$$

> [!TIP]
> Для русского языка **всегда используйте лемматизацию**. Морфология русского слишком сложная для стемминга. Инструменты: pymorphy2 (быстро), mystem (более точно, требует сервер).

---

## Часть 3: Извлечение признаков — от текста к числам

### Bag of Words: текст как набор слов

**Идея:** забыли о порядке слов, считаем только их частоту.

```python
doc1 = "cat sat on mat"
doc2 = "dog sat on log"

# Словарь: {cat: 1, sat: 2, on: 3, mat: 4, dog: 5, log: 6}

# BoW для doc1: [1, 2, 1, 1, 0, 0]  (кот встречается 1 раз, сидел 1 раз, на 1 раз, коврик 1 раз, собака 0, бревно 0)
# BoW для doc2: [0, 1, 1, 0, 1, 1]
```

**Математика:**

$$\text{BoW}_d = [c(w_1, d), c(w_2, d), \ldots, c(w_V, d)]$$

где $c(w_i, d)$ — количество раз слово $w_i$ встречается в документе $d$, а $V$ — размер словаря.

**Преимущества:**
- Просто и быстро
- Работает как baseline

**Недостатки:**
- Теряется информация о порядке слов
- Частые слова ("the", "and") доминируют
- Не учитывает семантику

### TF-IDF: взвешивание по важности

**Проблема BoW:** слово "the" встречается везде и ничего не говорит о содержании.

**Решение:** взвешиваем слова по редкости.

$$\text{TF-IDF}(t, d) = \underbrace{\text{TF}(t, d)}_{\text{частота в документе}} \times \underbrace{\text{IDF}(t)}_{\text{редкость в корпусе}}$$

где:

$$\text{TF}(t, d) = \frac{\text{count}(t \text{ in } d)}{\text{total words in } d}$$

$$\text{IDF}(t) = \log\left(\frac{\text{total docs}}{|\{d : t \in d\}|}\right)$$

**Интуиция:**
- TF высокий для частых в документе слов
- IDF высокий для редких в корпусе слов
- TF×IDF высокий для слов, которые частые в конкретном документе, но редкие в корпусе

**Пример:**

| Слово | Документ 1 | Документ 2 | Документ 3 |
|-------|-----------|-----------|-----------|
| "the" | 3 | 4 | 2 | IDF = log(3/3) = 0 → TF-IDF ≈ 0 |
| "cat" | 2 | 0 | 0 | IDF = log(3/1) ≈ 1.1 → TF-IDF ≈ 0.33 |
| "dog" | 0 | 3 | 0 | IDF = log(3/1) ≈ 1.1 → TF-IDF ≈ 0.5 |

Видно: "cat" и "dog" получают высокий вес (редкие, информативные), "the" получает нулевой вес (частое, бесполезное).

```python
from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    "cat sat on mat",
    "dog sat on log",
    "cat and dog",
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(documents)

# X — матрица (документы × слова) с TF-IDF весами
print(X.toarray())
# [[0.57 0.57 0.57 0.57 0.   0.  ]
#  [0.   0.58 0.58 0.   0.58 0.58]
#  [0.71 0.   0.   0.   0.71 0.71]]
```

### N-граммы: контекст через соседние слова

**Проблема TF-IDF:** не учитывает порядок и контекст.

```
"not good" vs "good"
TF-IDF одинаков для слова "good", хотя смысл противоположный.
```

**Решение:** используем N-граммы (последовательности N слов).

$$\text{unigrams (1-граммы)} = [w_1, w_2, w_3, \ldots]$$

$$\text{bigrams (2-граммы)} = [(w_1, w_2), (w_2, w_3), \ldots]$$

$$\text{trigrams (3-граммы)} = [(w_1, w_2, w_3), (w_2, w_3, w_4), \ldots]$$

**Примеры:**

```python
text = "the quick brown fox"

unigrams = ["the", "quick", "brown", "fox"]
bigrams = [("the", "quick"), ("quick", "brown"), ("brown", "fox")]
trigrams = [("the", "quick", "brown"), ("quick", "brown", "fox")]
```

**Можно использовать символьные N-граммы:**

```python
word = "cat"
char_bigrams = [("c", "a"), ("a", "t")]
char_trigrams = [("c", "a", "t")]
```

Это помогает при сложной морфологии (русский язык, опечатки).

```python
# Биграммы через TfidfVectorizer
vectorizer = TfidfVectorizer(ngram_range=(2, 2))
X = vectorizer.fit_transform(documents)

# Комбо: уни- и биграммы
vectorizer = TfidfVectorizer(ngram_range=(1, 2))
X = vectorizer.fit_transform(documents)
```

**Когда нужны N-граммы:**
- Вы хотите ловить фразы ("New York", "not good")
- Морфология сложная (русский, турецкий)
- Вас интересуют опечатки (символьные N-граммы)

**Осторожность:**
- N-граммы растут экспоненциально: bigrams = $V^2$, trigrams = $V^3$
- Большая матрица = медленнее, больше памяти

> [!NOTE]
> На практике часто используют комбинацию: TF-IDF + unigramы + биграммы. Это даёт хороший баланс между информативностью и размером.

### BM25: для поиска и ранжирования

TF-IDF хороша, но есть проблема: длинные документы доминируют (больше слов = выше TF).

**BM25** (Best Matching 25) — это формула поиска, которая исправляет эту проблему.

$$\text{BM25}(q, d) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{\text{TF}(q_i, d) \cdot (k_1 + 1)}{\text{TF}(q_i, d) + k_1 \cdot \left(1 - b + b \cdot \frac{|d|}{|d_{\text{avg}}|}\right)}$$

где:
- $q$ — поисковый запрос
- $d$ — документ
- $k_1$ — параметр насыщения (обычно 1.5)
- $b$ — параметр нормализации длины (обычно 0.75)
- $|d|$ — длина документа
- $|d_{\text{avg}}|$ — средняя длина документов

**Интуиция:**
1. IDF штрафует частые слова (как TF-IDF)
2. TF насыщается (после 3-4 вхождений дополнительные не помогают)
3. Нормализация по длине (длинные документы не получают несправедливое преимущество)

**Пример:**

```python
from rank_bm25 import BM25Okapi

documents = [
    "machine learning is powerful",
    "deep learning neural networks",
    "natural language processing",
]

# Токенизируем
tokenized_docs = [doc.lower().split() for doc in documents]

# BM25
bm25 = BM25Okapi(tokenized_docs)

# Поиск
query = "neural networks learning"
query_tokens = query.lower().split()
scores = bm25.get_scores(query_tokens)

# scores = [0.45, 2.15, 0.0]  ← doc 2 получает высший балл
```

**Когда использовать BM25:**
- Информационный поиск (Google-like)
- Ранжирование документов по релевантности
- Когда качество поиска критично

**Vs TF-IDF:**
- TF-IDF: быстро, просто, математически стройно
- BM25: лучше на практике, особенно на длинных документах

> [!IMPORTANT]
> BM25 — это de facto стандарт для поиска. Используется в Elasticsearch, Apache Lucene и других production системах. Если вы построили поиск на TF-IDF и он работает хуже, чем ожидалось, **первое, что нужно попробовать — это BM25**.

### PMI и NPMI: выделение коллокаций

Иногда нам нужно понять, какие слова часто встречаются вместе не просто потому, что оба частые, но потому что они как-то связаны.

**Пример:**
- "machine learning" — естественная коллокация
- "the cat" — не коллокация (слово "the" встречается везде)

**PMI (Pointwise Mutual Information):**

$$\text{PMI}(x, y) = \log\left(\frac{P(x, y)}{P(x) \cdot P(y)}\right)$$

**Интуиция:**
- Если $P(x, y) = P(x) \cdot P(y)$ → они независимы → PMI = 0
- Если $P(x, y) > P(x) \cdot P(y)$ → они связаны → PMI > 0
- Если $P(x, y) < P(x) \cdot P(y)$ → они избегают друг друга → PMI < 0

**NPMI (Normalized PMI):**

$$\text{NPMI}(x, y) = \frac{\text{PMI}(x, y)}{-\log P(x, y)}$$

Нормализация приводит значения в диапазон [-1, 1].

**Пример вычисления:**

```
Корпус: "machine learning", "machine learning", "deep learning", "the cat"
Всего 4 фразы.

P(machine) = 2/4 = 0.5
P(learning) = 3/4 = 0.75
P(machine, learning) = 2/4 = 0.5

PMI(machine, learning) = log(0.5 / (0.5 × 0.75)) = log(0.5 / 0.375) ≈ 0.29

P(the) = 1/4 = 0.25
P(cat) = 1/4 = 0.25
P(the, cat) = 1/4 = 0.25

PMI(the, cat) = log(0.25 / (0.25 × 0.25)) = log(4) ≈ 1.39

Замечание: PMI(the, cat) > PMI(machine, learning) 
потому что они редкие, но всегда встречаются вместе.
```

**Выделение коллокаций на практике:**

```python
from collections import defaultdict
import math

def extract_collocations(tokens, window_size=2, top_n=20):
    """Найти топ коллокации по PMI"""
    
    vocab_freq = Counter(tokens)
    pair_freq = defaultdict(int)
    
    # Считаем совместные частоты
    for i in range(len(tokens) - window_size + 1):
        window = tokens[i:i+window_size]
        for j, w1 in enumerate(window):
            for w2 in window[j+1:]:
                pair = tuple(sorted([w1, w2]))
                pair_freq[pair] += 1
    
    total = sum(vocab_freq.values())
    pmi_scores = {}
    
    for (w1, w2), count in pair_freq.items():
        p_pair = count / total
        p_w1 = vocab_freq[w1] / total
        p_w2 = vocab_freq[w2] / total
        
        pmi = math.log(p_pair / (p_w1 * p_w2))
        pmi_scores[(w1, w2)] = pmi
    
    return sorted(pmi_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]

# Использование
collocations = extract_collocations(tokens)
# [('machine', 'learning'), ('deep', 'learning'), ...]
```

---

## Практические рекомендации и тонкости

### Выбор метода токенизации

$$\text{Токенизация} = \begin{cases}
\text{NLTK/spaCy} & \text{Язык: английский, стандартные задачи} \\
\text{Regex} & \text{Домен специфичный, кастомная логика} \\
\text{BPE/SentencePiece} & \text{Нейросети, multilingual, редкие слова}
\end{cases}$$

### Выбор метода признаков

| Метод | Скорость | Качество | Когда использовать |
|-------|----------|----------|-------------------|
| BoW | ⚡⚡⚡ | ⭐ | Baseline, очень быстро |
| TF-IDF | ⚡⚡ | ⭐⭐⭐ | Классификация, поиск базовый |
| N-граммы | ⚡ | ⭐⭐⭐⭐ | Морфология важна, фразы важны |
| BM25 | ⚡⚡ | ⭐⭐⭐⭐ | Ранжирование, поиск |
| Word embeddings | ⚡ | ⭐⭐⭐⭐ | Модели глубокого обучения |

### Несбалансированные классы

Если у вас 95% позитивных примеров и 5% негативных, модель будет предсказывать только позитив.

**Решения:**
- `class_weight='balanced'` в sklearn
- Переsampling: oversampling меньшего класса, undersampling большего
- Пороги решения (для вероятностей)
- Метрики: используйте F1, PR-curve, не accuracy

```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(class_weight='balanced')
clf.fit(X_train, y_train)
```

### Pipeline: от текста к модели

```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Весь конвейер в одном объекте
clf = Pipeline([
    ('tfidf', TfidfVectorizer(ngram_range=(1, 2), max_features=5000)),
    ('lr', LogisticRegression(class_weight='balanced'))
])

# Обучение
clf.fit(X_train, y_train)

# Предсказание
y_pred = clf.predict(X_test)
```

> [!TIP]
> Используйте Pipeline для reproducibility и избежания data leakage. Всегда обучайте параметры vectorizer (в т.ч. размер словаря) только на обучающей выборке.

---

## Резюме

**Токенизация:**
- Просто split() никогда не подходит
- Используйте NLTK/spaCy для стандартных задач
- Пишите regex для специфичных доменов
- BPE для редких слов и нейросетей

**Нормализация:**
- Lowercase, HTML, диакритику, пробелы
- Лемматизируйте (не стеммируйте) для русского
- Тестируйте на реальных данных вашего домена

**Извлечение признаков:**
- BoW = baseline, но часто недостаточно
- TF-IDF = хороший выбор по умолчанию
- N-граммы = когда контекст важен
- BM25 = лучше всего для поиска
- PMI = для коллокаций и фраз

Следующий шаг — работа с эмбеддингами, которые захватывают семантику слов. Но сначала эти методы дают хороший baseline для понимания текста.

