# Подготовка признаков (Feature Engineering) и очистка данных

> Здесь собраны практические и теоретические основы подготовки данных: кодирование категориальных признаков, работа с выбросами и пропусками, кросс-валидация для отбора признаков, а также современные методы feature selection. Интуиция вперемешку со строгими формулами и фишками.

---

## 1) Что такое Feature Engineering

**Определение.** Feature engineering это извлечение, комбинирование и преобразование исходных данных для создания более информативных признаков, повышающих качество и устойчивость моделей.

**Три главные задачи:**
- Извлечение информации из сложных полей: например, из `1990-08-12 09:16:00` получить год, месяц, день недели, час, выходной.
- Комбинирование признаков: разница между событиями, агрегаты по группам, интеракции признаков.
- Преобразование признаков: масштабирование, кодирование категорий, нелинейные трансформации.

> **Tip.** Думайте о признаках как о гипотезах. Каждый новый признак должен иметь бизнес-смысл и проверяться на валидации.

---

## 2) Категориальные признаки: основные и продвинутые кодировки

### 2.1 Базовые подходы
- **Integer encoding**: каждому уникальному значению присваивается свой номер. Риск псевдо-упорядоченности.
- **One-hot encoding**: бинарная индикаторная матрица категорий. Хорош для низкой кардинальности, увеличивает размерность при высокой.

```python
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=True)
ct = ColumnTransformer([("cat", ohe, cat_cols)], remainder="passthrough")

pipe = Pipeline([("ct", ct), ("clf", LogisticRegression(max_iter=1000))])
pipe.fit(X_train, y_train)
```

> **Warning.** One-hot на признаках с высокой кардинальностью может взорвать память и привести к переобучению. Рассмотрите альтернативы ниже.

### 2.2 Высокая кардинальность
- **Hashing trick**: отображение категорий в фиксированное число бинов при помощи хеш-функции.
- **Count encoding**: замена категории на её частоту или лог-частоту.
- **Target encoding**: замена категории на статистику по целевой: среднее, сглаженное среднее.
- **Leave-one-out encoding**: как target encoding, но исключает текущий объект из статистики.
- **CatBoost encoding**: упорядоченная цель с шумом для борьбы с утечкой.

```python
# Пример целевого кодирования с перекрестной схемой для борьбы с утечкой
import numpy as np
import pandas as pd
from sklearn.model_selection import KFold

def target_encode(train_col, y, n_splits=5, smoothing=10.0):
    global_mean = y.mean()
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    te = pd.Series(index=train_col.index, dtype=float)
    for tr, val in kf.split(train_col):
        tr_idx, val_idx = train_col.index[tr], train_col.index[val]
        stats = y.iloc[tr].groupby(train_col.iloc[tr]).agg(["mean","count"])
        means = (stats["mean"] * stats["count"] + global_mean * smoothing) / (stats["count"] + smoothing)
        te.iloc[val] = train_col.iloc[val].map(means).fillna(global_mean)
    return te.fillna(global_mean)
```

> **Tip.** При target encoding используйте кросс-валидационную схему вычисления статистик, шум и сглаживание. Никогда не считайте среднее по целевой на всей обучающей выборке без разбиения это утечка.

---

## 3) Работа с выбросами

**Выбросы** это наблюдения, резко отличающиеся от основной массы данных. Они могут быть ошибками измерений, редкими событиями или ценной аномалией.

### 3.1 Классические критерии
- **IQR**: точки за пределами интервала
$$
[Q1 - 1.5 \cdot IQR,\; Q3 + 1.5 \cdot IQR]
$$
считаются выбросами, где $IQR = Q3 - Q1$.
- **Z-score**: $|z| > 3$ часто трактуют как выброс; для тяжелых хвостов лучше использовать **robust z** на основе медианы и $MAD$:
$$
z_i^{rob} = \frac{x_i - \operatorname{median}(X)}{1.4826 \cdot MAD},\quad MAD = \operatorname{median}(|X - \operatorname{median}(X)|).
$$

### 3.2 Модели и визуализации
- **Isolation Forest, LOF, One-Class SVM** для выявления аномалий.
- Визуализации: boxplot, scatter, QQ-plot, сравнение с медианой и квартилями.

### 3.3 Стратегии обработки
- Удаление очевидных ошибок, если можно доказать некорректность.
- **Winsorization**: обрезание хвостов на уровне перцентилей.
- Трансформации: $\log(1+x)$, Box-Cox, Yeo-Johnson.
- Отдельное моделирование аномалий и учет их экономического смысла.

> **Note.** Сначала разберитесь в природе выбросов. Иногда именно они предсказывают редкие и важные события.

---

## 4) Отсутствующие данные

### 4.1 Типы пропусков
- **MCAR** данные пропускаются полностью случайно.
- **MAR** вероятность пропуска зависит от наблюдаемых признаков.
- **MNAR** пропуски зависят от ненаблюдаемого значения признака.

### 4.2 Базовые стратегии
- Удаление строк при очень малой доле пропусков и большом объеме данных.
- Заполнение константой, средним, медианой, модой плюс индикатор пропуска.

### 4.3 Продвинутые методы
- **KNN imputer** для локальной похожести.
- **Iterative/MICE** множественная имputation с регрессорами по циклу.
- **MissForest** имputation деревьями.
- Для временных рядов: forward/backward fill, сглаживание, модельные прогнозы.

```python
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer

num_imp = IterativeImputer(random_state=42)
cat_imp = SimpleImputer(strategy="most_frequent")
```

> **Warning.** Никогда не подбирайте параметры импьютеров на полном датасете. Все трансформации обучайте только на трейне, применяйте к валидации и тесту через Pipeline или ColumnTransformer.

---

## 5) Валидация и кросс-валидация для Feature Selection

- **KFold** разбиение на $k$ частей.
- **StratifiedKFold** при несбалансированных классах.
- **GroupKFold** когда группы нельзя пересекать.
- **TimeSeriesSplit** если есть порядок во времени.
- **RepeatedKFold** для сглаживания дисперсии оценки.
- **Nested CV** для честной оценки подбора гиперпараметров и фич.

> **Tip.** Всегда проверяйте утечку: временной порядок, пересечение пользователей между фолдами, обработку целевых статистик. Лучше более строгая схема, чем завышенная метрика.

---

## 6) Современные методы отбора признаков

### 6.1 Фильтры
- Корреляция, ANOVA F, $\chi^2$, взаимная информация. Быстры и масштабируемы, но игнорируют взаимодействия.

### 6.2 Встроенные методы
- **L1/Lasso**, **Elastic Net** для линейных моделей.
- Важности в деревьях и бустингах по снижению impurity.
- **Permutation importance** оценка падения метрики при перетасовке признака.

### 6.3 Обёртки
- **RFE** рекурсивное исключение признаков, часто с **RFECV**.
- Последовательный выбор вперед или назад **SFS/SBS**.

### 6.4 Интерпретируемость и устойчивость
- **SHAP** локальные и глобальные эффекты признаков на предсказание.
- **Boruta** сравнение с теневыми признаками на базе Random Forest.
- **Stability selection** повторная выборка и выбор признаков, которые устойчиво отбираются.

```python
from sklearn.feature_selection import RFE, RFECV, SelectFromModel, mutual_info_classif
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

est = LogisticRegression(penalty="l1", solver="saga", max_iter=5000)
rfecv = RFECV(estimator=est, step=1, cv=StratifiedKFold(5), scoring="roc_auc", n_jobs=-1)
rfecv.fit(X_train, y_train)
selected_mask = rfecv.support_
```

> **Note.** Перестраивайте отбор признаков внутри каждой обучающей итерации CV. Иначе оценка качества будет смещена вверх из-за утечки информации.

---

## 7) Практический пайплайн

1. Разбейте данные на трейн и валидацию.
2. Опишите схемы импьютации численных и категориальных признаков.
3. Выберите кодировщик категорий с учетом кардинальности.
4. Масштабируйте численные признаки при необходимости.
5. Отберите признаки фильтрами и или встроенными методами внутри CV.
6. Закрепите все шаги в **Pipeline** и **ColumnTransformer**.
7. Валидируйте через подходящую схему CV, фиксируйте `random_state` и логируйте метрики.

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

num_pipe = Pipeline([("imp", SimpleImputer(strategy="median")), ("sc", StandardScaler())])
cat_pipe = Pipeline([("imp", SimpleImputer(strategy="most_frequent")), ("ohe", OneHotEncoder(handle_unknown="ignore"))])

ct = ColumnTransformer([("num", num_pipe, num_cols), ("cat", cat_pipe, cat_cols)])
model = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)
sel = SelectFromModel(model, threshold="median")

pipe = Pipeline([("ct", ct), ("sel", sel), ("clf", model)])
```

---

## 8) Дополнительно: текст, время, гео и числовые трюки

- **Datetime** извлекайте календарные признаки, циклические кодировки для часа или дня недели: $x_{sin}=\sin(2\pi t/T)$, $x_{cos}=\cos(2\pi t/T)$.
- **Text** TF-IDF, n-граммы, эмбеддинги. Для коротких текстов работает hashing trick плюс стоп-слова и нормализация.
- **Geo** биннинг широты долготы, расстояния до POI, H3 индексация.
- **Интеракции** полиномиальные признаки и продуктовые взаимодействия, но контролируйте размерность и регуляризацию.
- **Стабилизация масштабов** лог трансформации для правых хвостов, робастные скейлеры.

```python
import numpy as np

def cyclical_encode(x, T):
    return np.sin(2*np.pi*x/T), np.cos(2*np.pi*x/T)
```

> **Tip.** Для задач поиска ближайших и рекомендаций храните уменьшенные и заквантованные эмбеддинги. Комбинируйте PCA плюс Product Quantization, храните индексы в FAISS или HNSW для быстрых запросов.

---

## 9) Частые ошибки

> **Warning.** Обучение энкодеров и имьютеров на всем датасете приводит к утечке. Всегда используйте Pipeline и CV.

> **Warning.** Перемешивание во временных задачах разрушает причинность. Для временных рядов используйте TimeSeriesSplit и лаговые признаки.

> **Note.** Боритесь с коллинеарностью через регуляризацию и отбор признаков. Следите за стабильностью важностей признаков на бутстрапе.

---

## 10) Контрольные вопросы

1. Чем опасен target encoding и как избежать утечки
2. Когда hashing trick предпочтительнее one-hot
3. В чем разница между MCAR, MAR и MNAR и почему это важно для выбора имьютера
4. Зачем использовать permutation importance вместе с встроенными важностями
5. Как организовать честный отбор фич внутри кросс-валидации

---

**Ключевой вывод.** Качественный feature engineering и аккуратная очистка данных это основа надежных моделей. Совмещайте здравый смысл, строгие процедуры валидации и автоматизированные пайплайны. Используйте комбинацию фильтров, встроенных и оберточных методов отбора, следите за утечками и стабильностью решений.
