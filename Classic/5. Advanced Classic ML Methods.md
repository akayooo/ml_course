# Классификация в машинном обучении

Классификация — одна из самых важных задач в машинном обучении. Она ответает на вопрос: "К какой категории относится этот объект?" Примеры повсюду: определить спам ли письмо, предсказать вернёт ли клиент кредит, узнать язык текста, распознать рукописную цифру.

В жизни мы постоянно классифицируем. Смотрим на фрукт и решаем: это яблоко, груша или апельсин. Смотрим на человека и определяем примерный возраст. Слышим речь и понимаем язык. Все эти решения основаны на паттернах, которые мы заметили раньше.

Машинное обучение делает ровно то же самое, но формально: алгоритм учится на примерах (обучающей выборке), выделяет важные закономерности, а затем применяет их к новым, невиданным раньше объектам.

Существует множество алгоритмов классификации. Каждый подходит для своих задач, имеет свои сильные стороны и ограничения. В этом руководстве мы разберём четыре фундаментальных метода, которые покрывают большинство практических сценариев.

---

## Метод k-ближайших соседей (k-NN)

### Концептуальное объяснение

Представьте, что вы новичок в городе и хотите найти хороший ресторан. Вы не знаете город, но можете спросить у местных. Если вы спросите $k$ случайных прохожих о ближайшем к вам ресторане, скорее всего, большинство порекомендует один и тот же. Потому что они близко живут и знают окрестности.

k-NN работает ровно так же. Чтобы классифицировать новый объект, алгоритм ищет $k$ похожих объектов из обучающей выборки и смотрит, к какому классу они принадлежат. Если большинство похожих объектов — это спам, то и новое письмо, вероятно, спам.

Основная гипотеза простая: схожие вещи часто имеют схожие свойства. Если два документа содержат одинаковые слова, вероятно, они об одной теме. Если две клиентские записи похожи (один и тот же возраст, доход, история платежей), скорее всего, они ведут себя одинаково.

### Как работает k-NN практически

Предположим, у вас есть датасет с ирисами. Каждый ирис описан четырьмя признаками: длина и ширина чашелистика, длина и ширина лепестка. У вас есть 150 известных примеров (обучающая выборка) и вы хотите классифицировать новый ирис.

1. Вычислите расстояние от нового ириса до каждого из 150 примеров
2. Отсортируйте по расстоянию и возьмите топ-k (например, k=5)
3. Посчитайте, сколько каждого вида среди этих 5 соседей
4. Назначьте новому ирису класс большинства

Если из 5 ближайших соседей 4 — это разновидность Setosa и 1 — Versicolor, то новый ирис классифицируется как Setosa.

### Варианты расчёта расстояний

Когда мы говорим "похожесть" или "близость", мы имеем в виду расстояние между объектами в пространстве признаков. Существует несколько способов его вычислить.

**Евклидово расстояние** — самое интуитивное:

$d(x_1, x_2) = \sqrt{\sum_{i=1}^n (x_{1i} - x_{2i})^2}$

Это обычное геометрическое расстояние, как в школьной геометрии. Если у вас есть две точки в двумерном пространстве (координаты (1,1) и (4,5)), евклидово расстояние будет $\sqrt{(4-1)^2 + (5-1)^2} = \sqrt{9+16} = 5$.

**Манхэттенское расстояние** — сумма абсолютных разностей:

$d(x_1, x_2) = \sum_{i=1}^n |x_{1i} - x_{2i}|$

Называется так, потому что напоминает движение по сетке улиц Манхэттена. Вместо прямой диагонали вы можете идти только горизонтально или вертикально. Для тех же точек (1,1) и (4,5): $|4-1| + |5-1| = 3 + 4 = 7$.

Также существуют расстояние Чебышёва (максимальная разность), косинусная близость (угол между векторами) и расстояние Минковского (обобщение предыдущих).

Выбор метрики зависит от ваших данных и задачи. Для большинства числовых данных подходит Евклидово расстояние.

### Выбор оптимального k

Параметр $k$ — это количество соседей, которых мы рассматриваем. И это очень важный параметр.

**Малые значения (k=1, 3):**
- Модель очень чувствительна к шуму и выбросам
- Отдельный неправильно классифицированный пример может полностью изменить результат
- Модель переобучается: запоминает обучающие данные, но плохо обобщается

**Большие значения (k=100):**
- Модель становится более гладкой и устойчивой к шуму
- Но теряет способность находить локальные закономерности
- Может не различить тонкие различия между классами

Как выбрать оптимальное значение?

**Elbow method:** Строите график качества модели (например, точность на тестовой выборке) в зависимости от $k$. Ищете "локоть" — точку, где улучшение качества замедляется. Перед локтем увеличение $k$ даёт существенное улучшение, после локтя улучшение минимальное.

**Кросс-валидация с Grid Search:** Перебираете разные значения $k$ (например, 1, 3, 5, 7, ..., 21) и для каждого вычисляете среднее качество на кросс-валидации. Выбираете $k$, который дал лучший результат.

**Правило большого пальца:** $k = \sqrt{N}$, где $N$ — размер обучающей выборки. Для датасета из 100 примеров это даст $k \approx 10$.

> [!IMPORTANT]
> k-NN — "ленивый" алгоритм. Он не строит явную модель при обучении. Все вычисления происходят только при предсказании. Это значит обучение быстрое (просто запоминаем данные), но предсказание медленное (нужно сравнить с каждым примером). Для больших датасетов это становится узким местом.

> [!WARNING]
> k-NN крайне чувствителен к масштабу признаков! Если один признак измеряется в километрах (0-1000), а другой в килограммах (0-100), расстояние будет доминироваться первым признаком. Всегда используйте стандартизацию или нормализацию перед применением k-NN.

> [!TIP]
> Для больших датасетов используйте аппроксимирующие структуры данных (KD-tree, Ball tree, LSH), которые ускоряют поиск ближайших соседей с $O(N)$ до $O(\log N)$. Это позволит применять k-NN на миллионах примеров вместо тысяч.

### Когда использовать k-NN

k-NN работает хорошо, когда:
- Датасет относительно малый (до миллиона примеров на современных компьютерах)
- Решение зависит от локальных закономерностей
- Данные уже предварительно обработаны и масштабированы
- Нужна простая и интерпретируемая модель

k-NN работает плохо, когда:
- Очень много признаков (проклятие размерности)
- Классы сильно несбалансированы
- Данные содержат много шума

---

## Метод опорных векторов (SVM)

### Концептуальное объяснение

Представьте, что вам нужно отделить два вида фруктов: красные яблоки и жёлтые бананы. Одни признаки: цвет и форма. На плоскости можно нарисовать линию, которая разделит яблоки с одной стороны и бананы с другой.

Но не любая линия подойдёт. Если провести линию слишком близко к яблокам, то новое яблоко, которое чуть ближе к банану, может быть неправильно классифицировано. Нужна линия, которая находится "в середине" — максимально далеко от обоих классов.

SVM ищет ровно такую линию (в двумерном случае) или гиперплоскость (в многомерном). И ищет её так, чтобы зазор (margin) между классами был максимален.

### От одномерного к многомерному

В одномерном случае (один признак), разделитель — это точка. Банана от яблока отделяет число типа "красность > 0.7".

В двумерном случае (два признака, например цвет и размер), разделитель — это линия. Уравнение: $w_1 \cdot \text{цвет} + w_2 \cdot \text{размер} + b = 0$.

В трёхмерном — плоскость. В четырёхмерном и выше — гиперплоскость. Интуиция одна и та же.

### Ключевая идея: максимизация зазора

SVM решает: "Найди гиперплоскость, которая максимизирует расстояние до ближайших точек обоих классов".

Зачем максимизировать зазор? Интуитивно: если граница между классами с большим запасом, то шумовые точки и выбросы с меньшей вероятностью будут неправильно классифицированы. Модель более устойчива.

Представьте двух боксёров в ринге. Лучше встать в центре ринга, максимально далеко от обоих краёв, чем прижаться к краю.

### Hard margin vs Soft margin

**Hard Margin (жёсткий зазор):**

Требует, чтобы все точки были идеально разделены гиперплоскостью. Ни одна точка не должна находиться в зазоре или по неправильную сторону.

Проблема: если данные не идеально разделимы (а в реальности они никогда не разделимы идеально), алгоритм не может найти решение. На практике это непрактично.

**Soft Margin (мягкий зазор):**

Позволяет некоторым точкам нарушить границу. Некоторые точки могут находиться в зазоре или даже по неправильную сторону от гиперплоскости. За это добавляется штраф.

Параметр $C$ контролирует баланс:
- Большой $C$: жёсткий штраф за ошибки, модель стремится минимизировать неправильные классификации (риск переобучения)
- Малый $C$: мягкий штраф за ошибки, модель терпимо относится к ошибкам, ищет максимальный зазор (риск недообучения)

### SVM: математическое описание

Разделяющая гиперплоскость задается уравнением:

$\mathbf{w} \cdot \mathbf{x} + b = 0$

где $\mathbf{w}$ — вектор весов (нормаль к гиперплоскости), $b$ — смещение (bias).

Для классификации: объект относится к классу 1, если $\mathbf{w} \cdot \mathbf{x} + b > 0$, иначе к классу -1.

Формально, задача SVM с жёстким зазором:

$y_i(\mathbf{w} \cdot \mathbf{x_i} + b) \geq 1 \quad \forall i = 1, \ldots, n$

где $y_i \in \{-1, +1\}$ — метки классов.

С мягким зазором добавляются переменные slack $\epsilon_i \geq 0$:

$y_i(\beta_0 + \sum_{j=1}^{p} \beta_j x_{ij}) \geq M(1 - \epsilon_i)$

где $\sum_{i=1}^{n} \epsilon_i \leq C$ — ограничение на суммарное нарушение зазора.

> [!IMPORTANT]
> Параметр регуляризации $C$ в SVM работает "наоборот" по сравнению с другими алгоритмами. Большой $C$ означает жёсткий штраф за ошибки (малая регуляризация, риск переобучения). Малый $C$ означает мягкий штраф (сильная регуляризация, риск недообучения).

### Ядерные функции: решение нелинейных задач

Часто данные не разделимы линией. Например, спираль: центр одного класса, периметр другого. Линия не поможет.

SVM использует математический трюк: Kernel Trick. Вместо того чтобы явно преобразовывать данные в пространство большей размерности, можно вычислить "скалярное произведение" в высокомерном пространстве, используя функцию (ядро) исходного пространства.

**Линейное ядро:**

$K(x_i, x_j) = \sum_{k=1}^{p} x_{ik} x_{jk}$

Это просто скалярное произведение. Используется, когда данные уже линейно разделимы.

**Полиномиальное ядро:**

$K(x_i, x_j) = \left(1 + \sum_{k=1}^{p} x_{ik} x_{jk}\right)^d$

Параметр $d$ — степень полинома. При $d=2$ это ядро эквивалентно добавлению всех quadratic признаков. При $d=3$ — кубических и т.д.

**Радиально-базисное (RBF/Gaussian) ядро:**

$K(x_i, x_j) = \exp\left(-\gamma \sum_{k=1}^{p} (x_{ik} - x_{jk})^2\right)$

Это "универсальное" ядро. Параметр $\gamma$ контролирует радиус влияния:
- Малый $\gamma$ (широкое ядро): гладкая, простая граница решения
- Большой $\gamma$ (узкое ядро): сложная, извилистая граница

> [!NOTE]
> Kernel Trick — это математический "хак", который позволяет работать в бесконечномерном пространстве без явного вычисления координат. Вместо преобразования $\phi(x)$ и вычисления $\phi(x_i) \cdot \phi(x_j)$, мы сразу вычисляем $K(x_i, x_j)$. Это экономит огромное количество вычислений.

> [!TIP]
> RBF-ядро с параметром $\gamma$ — самое универсальное и часто первый выбор. Начинайте с $\gamma = \frac{1}{\text{количество признаков}}$. Если модель переобучается, уменьшайте $\gamma$ (делайте границу проще). Если недообучается, увеличивайте $\gamma$ (делайте границу сложнее).

### Практический пример

Предположим, вы классифицируете электронные письма. Один класс — спам, другой — нормальная переписка.

Признаки: количество восклицательных знаков, количество ЗАГЛАВНЫХ букв, наличие ссылки, количество слов.

SVM с RBF-ядром при малом $\gamma$ найдёт гладкую границу: "если много восклицательных знаков И много ссылок, то, скорее всего, спам".

SVM с большим $\gamma$ может найти более сложную границу, которая "запоминает" особые спамовые письма.

### Когда использовать SVM

SVM работает хорошо, когда:
- Высокоразмерные данные (сотни или тысячи признаков)
- Хорошее разделение между классами
- Нужна точность, готовы потратить время на подбор параметров

SVM работает плохо, когда:
- Очень большие датасеты (обучение может быть медленным)
- Сильно несбалансированные классы
- Нужна интерпретируемость (SVM — это "чёрный ящик")

---

## Деревья решений

### Концептуальное объяснение

Представьте, что вы врач и нужно определить, есть ли у пациента болезнь. Вы задаёте последовательность вопросов:

1. "Есть ли высокая температура?" (Да/Нет)
2. Если да: "Болит ли горло?" (Да/Нет)
3. Если да: "Есть ли насморк?" (Да/Нет)
4. На основе ответов ставите диагноз

Дерево решений работает ровно так же. Каждый внутренний узел содержит условие (вопрос). В зависимости от ответа вы идёте по левой или правой ветке. В конце достигаете листового узла, который содержит предсказание (диагноз).

Дерево "учится" на примерах: на обучающей выборке выясняет, какие вопросы (условия на признаках) лучше всего разделяют классы. Первый (корневой) вопрос — это тот, который даёт максимальное разделение.

### Построение дерева решений

Алгоритм CART (Classification and Regression Trees) работает рекурсивно:

1. Начинаем с корневого узла, содержащего все обучающие примеры
2. Для каждого признака и для каждого возможного порога вычисляем качество разбиения
3. Выбираем признак и порог, которые максимизируют качество разбиения (минимизируют неоднородность)
4. Разбиваем узел на два подузла согласно условию
5. Рекурсивно повторяем для каждого подузла

Процесс останавливается, когда:
- Узел "чистый" (все примеры одного класса)
- Достигнута максимальная глубина дерева
- Узел содержит меньше минимального количества примеров

### Мера Джини (Gini impurity)

Чтобы определить качество разбиения, нужна метрика "чистоты" узла. Mера Джини показывает, насколько "загрязнен" узел смешиванием классов.

Формула:

$G(Q) = \sum_{c\in C} p_c(1-p_c)$

где $p_c$ — доля объектов класса $c$ во множестве $Q$.

Интерпретация:
- Если в узле только один класс: $G(Q) = 0$ (чистый узел)
- Если классы распределены поровну: $G(Q)$ максимальна

Пример: узел с 60% класса A и 40% класса B:

$G = 0.6 \cdot (1 - 0.6) + 0.4 \cdot (1 - 0.4) = 0.6 \cdot 0.4 + 0.4 \cdot 0.6 = 0.48$

Лучшее разбиение — то, которое минимизирует взвешенную Gini после разбиения:

$G_{\text{avg}} = \frac{|Q_{\text{left}}|}{|Q|} \cdot G(Q_{\text{left}}) + \frac{|Q_{\text{right}}|}{|Q|} \cdot G(Q_{\text{right}})$

### Пример вычисления Gini

Узел с 100 примерами: 70 спама и 30 не спама.

$G = 0.7 \cdot (1 - 0.7) + 0.3 \cdot (1 - 0.3) = 0.7 \cdot 0.3 + 0.3 \cdot 0.7 = 0.42$

Рассмотрим условие: "Количество восклицательных знаков > 5".

Левый подузел (< 5): 10 спама, 25 не спама (35 примеров)
$G_{\text{left}} = \frac{10}{35} \cdot (1 - \frac{10}{35}) + \frac{25}{35} \cdot (1 - \frac{25}{35}) = \frac{10}{35} \cdot \frac{25}{35} + \frac{25}{35} \cdot \frac{10}{35} \approx 0.408$

Правый подузел (>= 5): 60 спама, 5 не спама (65 примеров)
$G_{\text{right}} = \frac{60}{65} \cdot (1 - \frac{60}{65}) + \frac{5}{65} \cdot (1 - \frac{5}{65}) = \frac{60}{65} \cdot \frac{5}{65} + \frac{5}{65} \cdot \frac{60}{65} \approx 0.237$

Взвешенная Gini:
$G_{\text{avg}} = \frac{35}{100} \cdot 0.408 + \frac{65}{100} \cdot 0.237 = 0.1428 + 0.1541 = 0.297$

Исходная Gini была 0.42, после разбиения стала 0.297. Это хорошее разбиение!

### Интерпретируемость

Главное преимущество деревьев решений — интерпретируемость. Вы можете объяснить любое предсказание:

"Письмо классифицировано как спам потому, что:
- Количество восклицательных знаков > 5 (спам обычно кричит)
- И содержит слово 'lottery' (частое в спаме)"

Это важно в регулируемых областях (финансы, медицина), где нужно объяснить решение модели.

### Проблема переобучения

Дерево без ограничений может растить до тех пор, пока каждый листовой узел содержит только один пример. На обучающей выборке это даст 100% точность, но на новых данных модель будет плохо работать.

Решение: ограничить рост дерева.

Параметры:
- `max_depth`: максимальная глубина дерева
- `min_samples_split`: минимальное количество примеров, чтобы разделять узел
- `min_samples_leaf`: минимальное количество примеров в листовом узле
- `max_features`: максимальное количество признаков рассматривать при поиске разбиения

또 альтернатива: Pruning (усечение). Сначала вырастить большое дерево, затем удалить слабые ветки.

> [!NOTE]
> Gini impurity — ключевой критерий чистоты в построении деревьев решений (CART). Альтернатива — энтропия (Information Gain). Используют логарифм вместо квадрата. На практике разница минимальна.

> [!WARNING]
> Деревья без ограничений склонны к сильному переобучению! Всегда устанавливайте `max_depth`, `min_samples_split`, `min_samples_leaf`. Это критически важно для хорошей обобщающей способности.

> [!TIP]
> Если один признак "доминирует" в дереве (появляется много раз), это может указывать на мультиколлинеарность (корреляцию признаков). Рассмотрите удаление одного из коррелирующих признаков.

### Когда использовать деревья решений

Деревья решений хороши, когда:
- Нужна интерпретируемость
- Есть категориальные признаки
- Важна скорость предсказания (очень быстро)
- Есть сложные нелинейные взаимодействия между признаками

Деревья решений плохи, когда:
- Классы сильно несбалансированы (без особых мер)
- Нужна высокая точность (лучше использовать ensemble)

Совет: Используйте ансамбли деревьев (Random Forest, Gradient Boosting) для лучшей точности вместо одного дерева.

---

## Наивный байес и NLP

### Концептуальное объяснение

Представьте, что вы получаете письмо. Вы хотите определить, это спам или нет. Вы смотрите на слова в письме и думаете:

"Если это спам, то в нём часто встречаются слова 'lottery', 'winner', 'click here'. Если это обычное письмо, то обычно слова 'meeting', 'deadline', 'report'".

Наивный байес делает ровно то же самое, но формально. Он вычисляет: "Какова вероятность того, что это спам, учитывая эти слова?"

Использует для этого теорему Байеса, формализованную на более чем 200 лет назад.

### Теорема Байеса

$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$

Это уравнение решает обратную задачу. Если мы знаем:
- $P(B|A)$ — вероятность видеть доказательства B, когда гипотеза A истинна
- $P(A)$ — априорная вероятность гипотезы A
- $P(B)$ — вероятность наблюдать доказательства B вообще

Тогда мы можем вычислить:
- $P(A|B)$ — апостериорная вероятность гипотезы A после наблюдения B

Пример: врач видит пациента с высокой температурой. Какова вероятность, что это грипп?

- $P(\text{грипп}|\text{высокая температура})$ = ?
- $P(\text{высокая температура}|\text{грипп}) = 0.95$ (у 95% больных гриппом высокая температура)
- $P(\text{грипп}) = 0.1$ (1% населения болеет гриппом в этом сезоне)
- $P(\text{высокая температура}) = 0.05$ (5% людей имеет высокую температуру, по разным причинам)

Подставляем:
$P(\text{грипп}|\text{высокая температура}) = \frac{0.95 \cdot 0.1}{0.05} = \frac{0.095}{0.05} = 1.9$

Стоп, вероятность не может быть > 1! Это значит, нужны дополнительные доказательства для нормализации.

### Применение к классификации текстов

$X = (x_1, \ldots, x_n)$ — признаки (слова в тексте)

$C_k$ — класс (спам/не спам)

Формула для классификации:

$P(C_k|X) = \frac{P(X|C_k) \cdot P(C_k)}{P(X)}$

Наиболее вероятный класс:

$\hat{C} = \arg\max_{C_k} P(C_k|X)$

Для спам-фильтра:
- $P(C_k = \text{спам}|X)$ — вероятность спама, учитывая слова в письме
- $P(X|C_k = \text{спам})$ — вероятность увидеть эти слова, если письмо спам
- $P(C_k = \text{спам})$ — общая доля спама в обучающей выборке

### Наивное предположение

Назван "наивным", потому что предполагает независимость признаков.

$P(X|C_k) = P(x_1|C_k) \cdot P(x_2|C_k) \cdot ... \cdot P(x_n|C_k)$

Для текстов это означает: вероятность слова "bank" независит от наличия слова "money". Это явно неверно! Если рядом есть "money", то слово "bank" вероятнее означает финансовое учреждение, чем берег реки.

Но здесь парадокс: несмотря на неверное предположение, алгоритм работает удивительно хорошо на практике, особенно для текстов!

> [!IMPORTANT]
> Почему "наивный" байес работает? Это философский вопрос в ML. Несмотря на грубое предположение о независимости, алгоритм часто даёт хорошие результаты. Теория предполагает, что ошибки компенсируют друг друга, или что для классификации не нужна абсолютная точность в вероятностях, важен только правильный порядок классов.

### Варианты модели Наивного байеса

**Мультиномиальная модель:**

Подсчитывает частоту слов в документе. Слово "money" может появиться 3 раза, "lottery" 2 раза и т.д.

$P(x_i|C_k) = \frac{\text{count}(x_i, C_k)}{\sum_{j} \text{count}(x_j, C_k)}$

Хорошо работает на текстовой классификации с большим словарём.

**Модель Гаусса:**

Предполагает, что признаки распределены нормально.

$P(x_i|C_k) = \frac{1}{\sqrt{2\pi\sigma_k^2}} \exp\left(-\frac{(x_i - \mu_k)^2}{2\sigma_k^2}\right)$

Используется для непрерывных числовых признаков.

**Модель Бернулли:**

Для бинарных признаков (слово есть в документе или нет).

$P(x_i|C_k) = P(\text{слово в классе}) \text{ или } (1 - P(\text{слово в классе}))$

**Комплементарная и категориальная модели:**

Специализированные варианты для особых случаев.

### Практический пример: классификация фильмов

Обучающая выборка:
- Позитивные отзывы: часто слова "great", "amazing", "love", "movie"
- Негативные отзывы: часто слова "bad", "terrible", "hate", "waste"

Новый отзыв: "I love this movie!"

Вычисляем:
$P(\text{positive}) \times P(\text{love}|\text{positive}) \times P(\text{movie}|\text{positive}) = 0.5 \times 0.15 \times 0.10 = 0.0075$

$P(\text{negative}) \times P(\text{love}|\text{negative}) \times P(\text{movie}|\text{negative}) = 0.5 \times 0.02 \times 0.05 = 0.0005$

Первое выше, значит предсказание: позитивный отзыв.

> [!WARNING]
> Проблема нулевых вероятностей: если слово никогда не встречалось в обучающей выборке для класса спам, то $P(\text{слово}|\text{спам}) = 0$, и всё произведение обнулится. Решение — сглаживание Лапласа: добавляем малую константу $\alpha$ (обычно 1) к каждому счётчику:

$P(w|c) = \frac{\text{count}(w,c) + \alpha}{\text{count}(c) + \alpha \cdot |V|}$

где $|V|$ — размер словаря.

> [!TIP]
> Наивный Байес особенно хорош для текстовой классификации с большим количеством признаков и относительно малым количеством данных. Он быстрый, простой и часто служит хорошим baseline. Для сравнения: обучение занимает миллисекунды даже на миллионах слов.

> [!NOTE]
> Для работы с редкими словами используйте Stop Words Removal (удаление часто встречающихся слов типа "the", "a") и Stemming/Lemmatization (приведение слов к базовой форме). Это улучшит качество без кардинальных изменений алгоритма.

### Когда использовать Naive Bayes

Хорош для:
- Текстовой классификации (спам, сентимент, категоризация)
- Быстрого базового решения (baseline)
- Данных с большим количеством признаков
- Когда нужна интерпретируемость (можно посмотреть вероятности слов)

Плох для:
- Сложных зависимостей между признаками
- Данных, где нарушается предположение о независимости
- Задач, где нужна максимальная точность (ensemble методы лучше)

---

## Практическое руководство: как выбрать алгоритм

### Таблица сравнения

| Алгоритм | Скорость обучения | Скорость предсказания | Точность | Интерпретируемость | Масштабируемость |
|---|---|---|---|---|---|
| k-NN | Очень быстро | Медленно | Средняя-Хорошая | Хорошая | Плохая |
| SVM | Медленно | Быстро | Хорошая-Отличная | Плохая | Средняя |
| Decision Tree | Быстро | Очень быстро | Средняя | Отличная | Хорошая |
| Naive Bayes | Быстро | Быстро | Средняя | Отличная | Отличная |

### Алгоритм выбора

1. **Начните с простого:** Используйте Naive Bayes или Decision Tree как baseline. Они быстрые, интерпретируемые, часто дают приличные результаты.

2. **Если нужна точность:** Используйте SVM или Ensemble методы (Random Forest, Gradient Boosting).

3. **Если нужна интерпретируемость:** Выбирайте Decision Tree или Naive Bayes, избегайте SVM и нейронных сетей.

4. **Если данные малого размера:** k-NN часто работает неплохо.

5. **Если данные большие и высокоразмерные:** SVM или Ensemble методы.

6. **Если много текста:** Обязательно Naive Bayes.

7. **Если есть много категориальных признаков:** Decision Tree лучше справляется.

### Практические советы

**Подготовка данных:**
- Всегда масштабируйте признаки (стандартизация или нормализация) для k-NN и SVM
- Обработайте пропущенные значения (удалите строки или заполните средним)
- Удалите дублирующиеся признаки
- Кодируйте категориальные переменные (One-Hot Encoding для SVM, деревья справляются сами)

**Подбор гиперпараметров:**
- Используйте кросс-валидацию (обычно 5-fold)
- Используйте Grid Search или Random Search
- Всегда проверяйте на отдельном тестовом наборе

**Работа с несбалансированными данными:**
- Используйте метрики F1-score, Recall, Precision вместо Accuracy
- Балансируйте классы: oversampling, undersampling, SMOTE
- Выставляйте class weights в алгоритме

**Интерпретация результатов:**
- Всегда смотрите на Confusion Matrix
- Рисуйте ROC-кривую для бинарной классификации
- Проверяйте важность признаков (Feature Importance)

---

Каждый алгоритм — это инструмент в вашем арсенале. Нет универсально лучшего алгоритма, есть наиболее подходящий для вашей задачи. Понимание математики и интуиции за каждым алгоритмом поможет вам делать правильный выбор.