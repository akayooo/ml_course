# Production ML: инструменты, деплой, эксплуатация и A/B тестирование

> Финальная лекция блока Classic ML. Разберем инструменты данных и MLOps, паттерны деплоя, мониторинг и контроль качества, а также строгие принципы A/B тестирования производственных моделей.

---

## 1) Картина целиком: от ноутбука к продакшну

**Цель.** Превратить исследовательский прототип в надежный сервис с воспроизводимостью, безопасностью и измеримым эффектом.

**Ключевые аспекты:**
- Управление окружениями: conda, venv, poetry, Docker.
- Управление данными и метаданными: DVC, LakeFS, Delta Lake, таблицы фичей.
- Трекинг экспериментов: MLflow, Weights & Biases.
- Регистры моделей: MLflow Model Registry, SageMaker Model Registry, KServe/Seldon.
- Оркестрация: Airflow, Argo Workflows, Kubeflow Pipelines.
- Хранилища и вычисления: S3, GCS, BigQuery, Snowflake, Spark, Flink, Databricks.
- Серверсайд: FastAPI, gRPC, KServe, BentoML, TF Serving, TorchServe.
- Мониторинг и алерты: Prometheus, Grafana, OpenTelemetry, Sentry, пользовательские дашборды.

> **Tip.** С самого начала фиксируйте версии данных, кода и параметров. Минимальный стандарт: Git + MLflow + DVC + Docker.

---

## 2) Управление окружениями и зависимостями

- **Docker** как база для воспроизводимости и идентичности окружений.
- **Poetry/conda** для декларативных зависимостей.
- **Makefile** или tox/nox для скриптов сборки, тестов и линтинга.
- **CI**: GitHub Actions, GitLab CI, Jenkins. Прогон тестов, линтеров, mypy, сборка образа, публикация артефактов.

```dockerfile
# Пример лёгкого образа для sklearn сервиса
FROM python:3.11-slim
WORKDIR /app
COPY pyproject.toml poetry.lock ./
RUN pip install --no-cache-dir poetry && poetry config virtualenvs.create false \
 && poetry install --no-interaction --no-ansi
COPY . .
CMD ["uvicorn", "service:app", "--host", "0.0.0.0", "--port", "8080"]
```

> **Warning.** Убедитесь, что версия glibc и BLAS на проде совместима с тем, как вы обучали. Иначе можно получить неожиданные различия в числах и скорость ниже ожидаемой.

---

## 3) Пайплайны данных и фичи

- **Feature Store**: централизованное определение и материализация фичей онлайн и оффлайн. Примеры: Feast, Tecton, Vertex FS.
- **Согласованность оффлайн и онлайн**: одна функция для расчета в обоих контурах, контроль задержек и латентности.
- **Пайплайны**: Airflow для оффлайна, Flink или Spark Structured Streaming для стриминга.

```python
# Пример определения фичей в Feast (в духе)
from datetime import timedelta
from feast import Entity, Feature, FeatureView, ValueType, FileSource

user = Entity(name="user_id", value_type=ValueType.INT64)
source = FileSource(path="data/events.parquet", timestamp_field="event_ts")
fview = FeatureView(
    name="user_agg",
    entities=["user_id"],
    ttl=timedelta(days=7),
    features=[Feature(name="views_7d", dtype=ValueType.INT64),
              Feature(name="mean_spend_7d", dtype=ValueType.FLOAT)],
    online=True,
    batch_source=source,
)
```

> **Note.** Для real-time скоринга критичны кэширование последних агрегатов в Redis и атомарная запись признаков по событиям.

---

## 4) Паттерны деплоя моделей

### 4.1 Сервис онлайнового скоринга
- REST или gRPC API. В процессе запроса извлекаем фичи из Feature Store или внутреннего кэша и считаем скор.
- Важные метрики: P50, P95, P99 задержки, RPS, доля ошибок.

```python
# FastAPI сервис для модели sklearn
import joblib
import numpy as np
from fastapi import FastAPI

app = FastAPI()
model = joblib.load("model.pkl")

@app.post("/predict")
def predict(payload: dict):
    x = np.array(payload["features"], dtype=np.float32).reshape(1, -1)
    y = model.predict_proba(x)[0, 1]
    return {"score": float(y)}
```

### 4.2 Batch scoring
- Периодический расчет скорингов для больших массивов и запись результатов в таблицу.
- Используется для ранжирования, отчетности, предподсчета рекомендаций.

### 4.3 Stream scoring
- Потоковые фреймворки: Flink, Kafka Streams, Spark Streaming.
- Применяется там, где важна низкая задержка и высокая пропускная способность.

### 4.4 Способы раскатки
- **Shadow**: новая модель получает трафик копией и не влияет на пользователя.
- **Canary**: доля трафика идет на новую версию, постепенно увеличивается.
- **Blue-green**: параллельная среда, быстрое переключение.
- **Rollback**: четкая процедура отката версии по сигналам деградации.

> **Tip.** Отделите бизнес-логику отбора кандидатов от модели. Это повышает гибкость и упрощает A/B эксперименты.

---

## 5) Регистры моделей и контроль версий

- Версионирование артефактов: веса, конфиги препроцессинга, схемы входов.
- Статусы: Staging, Production, Archived. Политика промоута через проверки качества.
- Сигнатуры и схемы: pydantic или OpenAPI схемы для контрактов запроса и ответа.
- Хранение метрик, данных валидации и воспроизводимых ссылок на датасеты.

```python
# Логирование в MLflow
import mlflow
with mlflow.start_run():
    mlflow.log_params({"model": "logreg", "C": 0.5})
    mlflow.log_metrics({"auc": 0.812, "f1": 0.67})
    mlflow.sklearn.log_model(model, "model", registered_model_name="churn_logreg")
```

> **Warning.** Не храните произвольный pickle без фиксированных версий зависимостей. Предпочитайте формат с явной средой: MLflow, ONNX, Docker образ.

---

## 6) Мониторинг качества и здоровья модели

- **Технические метрики**: латентность P50 P95 P99, RPS, ошибки 4xx 5xx, использование CPU RAM GPU, очереди.
- **Поведенческие**: распределения входов и выходов, доли классов, доверительные интервалы.
- **Data drift**: сдвиг входных признаков. Метрики: PSI, Jensen-Shannon, KL, Hellinger.
- **Concept drift**: изменение истинной зависимости. Признаки: падение метрик на отложенной разметке.
- **Алерты**: пороги и канареечные проверки, SLO и SLI.

```python
# Пример PSI для одного признака
import numpy as np
def psi(expected, actual, bins=10):
    q = np.linspace(0, 1, bins+1)
    e = np.histogram(expected, bins=np.quantile(expected, q))[0] + 1e-6
    a = np.histogram(actual,   bins=np.quantile(expected, q))[0] + 1e-6
    e, a = e / e.sum(), a / a.sum()
    return float(np.sum((a - e) * np.log(a / e)))
```

> **Tip.** Логируйте не только предсказания, но и входные фичи, идентификаторы и версию модели. Это критично для диагностики проблем.

---

## 7) Оптимизация инференса

- Векторизация и батчинг запросов.
- Параллелизм: uvicorn workers, gunicorn, asyncio, gRPC.
- Аппаратные ускорители при необходимости.
- Квантизация, прунинг, ONNX Runtime для ускорения.
- Кэширование и предвычисление признаков, запросов, кандидатов.
- Автоскейлинг HPA в Kubernetes по метрикам.

> **Note.** Задайте бюджет задержки, например 100 ms. Спроектируйте путь запроса и разбейте бюджет между шагами: извлечение фичей, модель, постпроцессинг, сеть.

---

## 8) Безопасность, приватность и комплаенс

- Контроль PII: маскирование, токенизация, минимизация доступа.
- Шифрование на диске и в канале, управление секретами.
- Разделение ролей и журналирование действий.
- Datasheets и Model Cards: документация данных и моделей.
- Справедливость и bias: мониторинг метрик по сегментам, процедуры эскалации.

> **Warning.** Не используйте чувствительные признаки для принятия решений без юридической оценки и явного бизнес-обоснования.

---

## 9) A/B тестирование моделей

**Цель.** Доказать, что новая версия дает статистически значимый прирост бизнес-метрик при контролируемых рисках.

### 9.1 Дизайн эксперимента
- Определите бизнес-метрики: конверсия, удержание, LTV, NDCG, время ответа.
- Единица рандомизации: пользователь, сессия, запрос. Исключайте пересечения между группами.
- Предварительные проверки: SRM тест на несоответствие размеров групп, инвариантные метрики.

### 9.2 Размер выборки и мощность
Для сравнения средних при известной дисперсии:
$$
n \approx \frac{2\sigma^2 \left(z_{1-\alpha/2} + z_{1-\beta}\right)^2}{\Delta^2},
$$
где $n$ размер на группу, $\sigma^2$ дисперсия метрики, $\Delta$ минимально значимый эффект, $\alpha$ уровень значимости, $\beta$ ошибка второго рода.

Для долей используйте дисперсию биномиальной метрики с ожидаемой долей $p$.

> **Tip.** Применяйте CUPED для снижения дисперсии: вычтите линейную комбинацию из предэкспериментальной метрики.

### 9.3 Проведение и анализ
- Блокируйте по сегментам и сезонам, рандомизируйте честно.
- Избегайте преждевременных остановок. Если нужны последовательные тесты, используйте корректные правила останова.
- Основной анализ: разница средних, бутстреп доверительных интервалов, поправки за множественные сравнения.
- Диагностика: SRM, инвариантные метрики, санити чек производительности.

### 9.4 Трафик-стратегии
- Shadow и canary до полноразмерного A/B.
- Ramping: 1% 5% 10% 25% 50% 100% при соблюдении SLO.
- Быстрый откат по алертам деградации.

> **Warning.** Не меняйте одновременно много компонентов. Изолируйте эффект модели от логики ранжирования, правил, UI.

---

## 10) Паттерны Big Tech

- Kubernetes повсюду: сервисы, джобы, автоскейлинг, сервис меши.
- Многоуровневые стореджи: data lake для сырья, lakehouse для аналитики, feature store для онлайна.
- Учет стоимости: профилирование запросов, кэширование, компрессия, холодное хранение.
- Строгие SLO и error budgets, SRE практики, oncall ротации.
- Шлюзы и контракты: gRPC схемы, версионирование API, обратная совместимость.

> **Tip.** Считайте TCO: время разработчиков, стоимость инференса, хранение и трафик. Иногда простая модель выигрывает у сложной по суммарной ценности.

---

## 11) Практический чеклист продакшна

- Репозиторий с кодом, тестами и инфраструктурой.
- Docker образ и зафиксированная среда.
- MLflow артефакты и метрики, промоут из Staging в Production по критериям.
- Сервис предикта с healthcheck, логами и трассировкой.
- Мониторинг качества, алерты и планы отката.
- План A/B, метрики, размер выборки, SRM и инварианты.
- Документация: схемы данных, Model Card, runbook инцидентов.

---

**Ключевой вывод.** Продакшн это не только качество модели оффлайн. Это архитектура фичей и данных, надежный деплой, измеримый эффект, мониторинг и дисциплина процессов. Чем раньше вы проектируете эти компоненты, тем быстрее и безопаснее пройдете путь от ноутбука до продукта.
