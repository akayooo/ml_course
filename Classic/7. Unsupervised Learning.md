# Обучение без учителя (Unsupervised Learning)

> Эта лекция продолжает курс из предыдущих материалов и вводит ключевые методы обучения без учителя: кластеризацию (k-means, иерархическая, DBSCAN) и уменьшение размерности (PCA). Мы чередуем интуитивные объяснения с математикой и практическими советами.

---

## 1) Зачем нужно обучение без учителя?

**Цель.** Имея *неразмеченные* данные (без целевой переменной), обнаружить скрытые структуры и закономерности: группы похожих объектов, а также более компактные представления признаков.

- **Кластеризация** - группировка объектов по сходству.
- **Уменьшение размерности** - сжатие признакового пространства при максимальном сохранении информации (вариации).

> **Интуиция.** Представьте шкаф с беспорядком: без ярлыков (классов) мы всё равно можем разложить вещи по кучкам (кластерам) и сложить в меньшее число коробок (уменьшить размерность), не потеряв сути.

---

## 2) k-means кластеризация

### 2.1 Принцип работы

k-means пытается разделить данные на $k$ групп так, чтобы **внутрикластерная дисперсия** была минимальной. Каждый кластер описывается **центроидом** - средним своих точек.

**Алгоритм (итеративно):**
1. Выберите число кластеров $k$.
2. Инициализируйте $k$ центроидов (обычно случайно или k-means++).
3. **E-шаг (assignment):** присвойте каждую точку ближайшему центроиду по метрике расстояния (часто евклидовой).
4. **M-шаг (update):** пересчитайте центроид как среднее точек своего кластера.
5. Повторяйте 3–4, пока назначение точек перестанет меняться или улучшение станет мало́.

**Целевая функция (минимизируется):**
$$\min_{\{C_j\}, \{\mu_j\}} \sum_{j=1}^k \sum_{x_i \in C_j} \|x_i - \mu_j\|_2^2$$
где $C_j$ - множество точек кластера $j$, $\mu_j$ - его центроид.

> **Note.** k-means предполагает примерно *сферические*, сопоставимых размеров кластеры и чувствителен к масштабам признаков - *обязательно нормируйте/стандартизируйте признаки!*

### 2.2 Как выбрать $k$?

**Метод локтя (Elbow):**
- Считайте $\text{SSD}(k) = \sum_{j=1}^k \sum_{x_i \in C_j} \|x_i - \mu_j\|_2^2$.
- Постройте график SSD по $k$ и ищите «локоть» - точку, после которой снижение SSD замедляется.

> **Warning.** При $k = N$ (число точек) SSD=0 - но это переобучение и бессмысленная модель.

**Метод силуэта (Silhouette):**
$$s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$$
- $a(i)$ - среднее расстояние точки $i$ до точек **своего** кластера.
- $b(i)$ - минимальное среднее расстояние до **соседнего** кластера.
- Диапазон $s(i)\in[-1,1]$. Ближе к 1 - лучше.

Оценивайте **среднее** $\bar{s}$ по всем точкам и выбирайте $k$ с наибольшим $\bar{s}$.

> **Tip.** Если локоть неявен, используйте силуэт, Calinski–Harabasz, Davies–Bouldin - сравнивайте несколько метрик одновременно.

### 2.3 Практический пайплайн (Python, scikit-learn)

```python
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

X = ...  # ваш массив признаков (n_samples, n_features)

X_scaled = StandardScaler().fit_transform(X)  # масштабирование важно!
best_k, best_s = None, -1

for k in range(2, 11):
    km = KMeans(n_clusters=k, n_init='auto', random_state=42)
    labels = km.fit_predict(X_scaled)
    s = silhouette_score(X_scaled, labels)
    if s > best_s:
        best_k, best_s = k, s

print(f"Лучшее k={best_k}, silhouette={best_s:.3f}")
```

> **Tip (инициализация).** Используйте `k-means++` (в sklearn по умолчанию) - он стабилизирует сходимость и снижает риск плохих локальных минимумов.

---

## 3) Иерархическая кластеризация

Когда $k$ заранее неизвестно, и хочется увидеть **структуру слияния/деления**.

- **Агломеративная** (снизу вверх): каждая точка - отдельный кластер, кластеры попарно объединяются.
- **Дивизионная** (сверху вниз): один кластер - все точки, затем он рекурсивно делится.

**Linkage (критерии слияния):**
- **single** - минимальная дистанция между кластерами (тяготеет к «цепочкам»).
- **complete** - максимальная дистанция (более компактные кластеры).
- **average/ward** - усреднение; *Ward* минимизирует прирост дисперсии (часто хороший дефолт).

**Дендрограмма** отображает последовательность слияний и высоты (дистанции).

```python
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering

X_scaled = StandardScaler().fit_transform(X)
cl = AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage='ward')
labels = cl.fit_predict(X_scaled)  # для дендрограммы используйте scipy.cluster.hierarchy
```

> **Note.** Для больших $n$ дендрограмма становится тяжёлой. Используйте выборки или предварительное уменьшение размерности (например, PCA или UMAP).

---

## 4) DBSCAN: кластеризация на основе плотности

**Идея.** Кластеры - области повышенной плотности, разделённые зонами низкой плотности. Автоматически определяет число кластеров, устойчив к шуму/выбросам.

**Параметры:**
- $\varepsilon$ (*eps*) - радиус окрестности точки.
- `min_samples` - минимальное число точек в $\varepsilon$-окрестности для **core**-точки.

**Типы точек:**
- **Core** - достаточно соседей в радиусе $\varepsilon$.
- **Border** - в радиусе core-точки, но сама не core.
- **Outlier (noise)** - не относится ни к core, ни к border.

**Алгоритм (упрощённо):**
1. Возьмите непосещённую точку, проверьте её плотность.
2. Если core - разрастите кластер на всех достижимых соседей.
3. Повторяйте, пока все точки не обработаны.

```python
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
import numpy as np

X_scaled = StandardScaler().fit_transform(X)

# Быстрый хак подбора eps: локальная k-dist кривая (k=min_samples)
k = 10
nbrs = NearestNeighbors(n_neighbors=k).fit(X_scaled)
distances, _ = nbrs.kneighbors(X_scaled)
k_dist = np.sort(distances[:, -1])  # расстояние до k-го соседа
# визуально ищем "локоть" на графике k_dist -> eps

db = DBSCAN(eps=0.7, min_samples=k).fit(X_scaled)
labels = db.labels_  # -1 = шум
```

> **Warning.** Масштабирование критично. Плохой выбор $\varepsilon$ ломает результат. В неоднородных плотностях рассмотрите **HDBSCAN** (не в sklearn из коробки).

> **Tip.** Для данных с разными масштабами плотностей HDBSCAN часто выигрывает (автовыбор устойчивых кластеров).

---

## 5) Уменьшение размерности: PCA (Метод главных компонент)

**Цель.** Найти ортонормированное базисное пространство меньшей размерности, в котором сохраняется максимум дисперсии данных.

### 5.1 Математическая формулировка

Пусть $X \in \mathbb{R}^{n \times d}$ - центрированные данные. Ищем матрицу проекции $W \in \mathbb{R}^{d \times m}$ ($m<d$), максимизирующую дисперсию проекций:
$$
\max_W {Tr}\left(W^\top S W\right) \quad \text{s.t. } W^\top W = I_m,
$$
где $S = \frac{1}{n-1} X^\top X$ - ковариационная матрица. Решение - **собственные векторы** $S$, соответствующие $m$ наибольшим собственным значениям (дисперсиям).

**Проекция:** $Z = X W$, где строки $Z$ - точки в новом пространстве главных компонент.

### 5.2 Практика и интерпретация

- **Обязательно центрируйте** данные; масштабирование по признакам часто полезно (особенно если разные единицы измерения).
- **Объяснённая дисперсия** (`explained_variance_ratio_`) помогает выбрать $m$. Ищите кумулятивный порог (например, 90–99%).

```python
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import numpy as np

X_scaled = StandardScaler().fit_transform(X)
pca = PCA(n_components=None, random_state=42)
Z = pca.fit_transform(X_scaled)

cum = np.cumsum(pca.explained_variance_ratio_)
m = np.searchsorted(cum, 0.95) + 1  # число компонент для 95% дисперсии
print(f"Компонент для 95% дисперсии: m={m}")
```

> **Tip.** Перед кластеризацией применяйте PCA для шумных/высокомерных данных - это ускоряет расчёт расстояний и часто улучшает качество кластеров.

> **Note.** PCA - *линейный* метод. Структуры, лежащие на нелинейных многообразиях, лучше раскрывают t-SNE/UMAP (для визуализации) или автоэнкодеры (для обучения представлений).

### 5.3 Связь PCA и SVD

Центрируйте $X$ и возьмите SVD: $X = U \Sigma V^\top$. Тогда столбцы $V$ - главные направления, $\Sigma^2/(n-1)$ - собственные значения ковариации. Численно SVD часто устойчивее прямой спектральной декомпозиции $S$.

---

## 6) Паттерны и рецепты

- **Масштабирование - сначала.** `StandardScaler()` почти всегда перед k-means/DBSCAN/PCA.
- **k-means ⇄ PCA.** Сначала PCA (сохранив 90–99% дисперсии), затем k-means на сокращённом пространстве.
- **DBSCAN для выбросов.** Неплохо выделяет шум/аномалии ($label=-1$).
- **Иерархическая кластеризация** даёт дендрограмму → удобно выбирать число кластеров *после* просмотра структуры.
- **Оценка качества без меток.** Силуэт, Calinski–Harabasz, Davies–Bouldin, стабилизация кластеров бутстрепом.
- **Повторяемость.** Фиксируйте `random_state` и логируйте гиперпараметры/метрики.

---

## 7) Частые ошибки и анти-паттерны

> **Warning.** Не интерпретируйте кластеры как «истинные классы» без доменной проверки. Кластеры - гипотезы, а не факты.

> **Warning.** Нельзя сравнивать расстояния, если признаки в разных шкалах и не нормированы.

> **Warning.** Слишком большое $k$ в k-means вырождает кластеры и ведёт к переобучению; слишком маленькое - сливает разные группы.

> **Note.** В DBSCAN выбор $\varepsilon$ должен быть осмысленным: используйте $k$-distance график, grid search, экспертный масштаб.

---

## 8) Мини-кейс: от сырых данных к кластерам

1. **Очистка и кодирование.** Обработка пропусков, One-Hot Encoding категорий.
2. **Масштабирование.**
3. **PCA → m компонент (например, до 95% дисперсии).**
4. **Быстрый просмотр кластеризуемости:** k-means (несколько $k$), силуэт/локоть.
5. **Тонкая настройка:** сравнить k-means/DBSCAN/агломеративную с Ward, выбрать по метрикам + здравому смыслу.
6. **Интерпретация кластеров:** описательные статистики по признакам, профильные названия («покупатели-новички», «частые большие транзакции» и т.п.).
7. **Валидация стабилизации:** переобучение на бутстреп-выборках, сравнение устойчивости меток (например, Adjusted Rand Index между прогонами).

---

## 9) Шпаргалка по API (scikit-learn)

```python
# KMeans
from sklearn.cluster import KMeans
KMeans(n_clusters=k, n_init='auto', random_state=42).fit(X_scaled)

# Agglomerative
from sklearn.cluster import AgglomerativeClustering
AgglomerativeClustering(n_clusters=k, linkage='ward').fit(X_scaled)

# DBSCAN
from sklearn.cluster import DBSCAN
DBSCAN(eps=0.5, min_samples=10).fit(X_scaled)

# PCA
from sklearn.decomposition import PCA
PCA(n_components=m, random_state=42).fit_transform(X_scaled)
```

---

## 10) Контрольные вопросы

1. Почему k-means плохо работает на вытянутых или неоднородных по плотности кластерах?
2. Как интерпретировать значения силуэта, близкие к нулю?
3. Чем linkage='single' отличается от 'complete' и когда это важно?
4. Почему перед DBSCAN важно масштабирование?
5. Чем PCA через SVD предпочтительнее спектрального разложения ковариации на практике?
6. Какие метрики использовать для сравнения нескольких кластеризаций на одних данных?

---

**Ключевой вывод.** Обучение без учителя - это набор инструментов для поиска структуры в данных, где нет меток. Правильная предобработка (масштабирование, фильтрация шума), разумный выбор метода (k-means/иерархическая/DBSCAN) и адекватная оценка качества - залог полезных кластеров и интерпретируемых компонент.


---

## 5.4 Более продвинутые методы сжатия векторов и представлений

**Когда PCA недостаточно линейной:** используйте методы, которые улавливают нелинейные многообразия, а также алгоритмы явного уменьшения размера и компрессии для продакшна.

### Kernel PCA
Идея: применить PCA в пространстве признаков после нелинейного отображения $\phi(x)$ с помощью ядра $k(x,x')=\langle \phi(x),\phi(x')\rangle$.
- Плюсы: выявляет искривленные структуры.
- Минусы: $O(n^3)$ по времени, хранение матрицы ядра $n\times n$.
- **Tip:** используйте разреженные/аппроксимирующие ядра (Nystroem) для больших $n$.

### t-SNE и UMAP (для визуализации)
- **t-SNE:** сохраняет локальные расстояния благодаря вероятностному соседству; отличен для 2D/3D-визуализации кластеров, но не для дальнейшей модели.
- **UMAP:** быстрее, масштабируемее, лучше сохраняет глобальную структуру, даёт осмысленные эмбеддинги для последующей кластеризации.
- **Warning:** не интерпретируйте расстояния между далеко расположенными кластерами буквально; используйте эти методы как инструмент разведочного анализа.

### Нелинейные факторизации: NMF, ICA, Sparse PCA
- **NMF (неотрицательные матричные факторизации):** $X \approx WH,\; W,H\ge0$. Даёт аддитивно-интерпретируемые компоненты (темы, базисы).
- **ICA:** ищет статистически независимые источники (сигналы), полезно для аудио/EEG.
- **Sparse PCA:** компоненты с разреженными нагрузками - легче интерпретировать признаки.

### Случайные проекции (Johnson-Lindenstrauss)
- Проекция $X \in \mathbb{R}^{n\times d}$ на $m \ll d$ случайной матрицей $R$ с сохранением расстояний с малыми искажениями:
$$
Z = X R,\quad R_{ij} \sim \mathcal{N}(0, 1/m).
$$
- **Плюсы:** очень быстро и просто, хорошие гарантии JL.
- **Note:** применимы для больших разреженных данных (тексты, логи).

### Автоэнкодеры (AEs)
Нейросеть обучается кодировать $x \mapsto z$ и восстанавливать $x \approx \hat{x}(z)$, где $z$ - сжатое представление.
- **Denoising AE:** обучается восстанавливать чистый сигнал из зашумленного - устойчивее к шуму.
- **Variational AE (VAE):** оптимизирует нижнюю оценку правдоподобия с регуляризацией к латентному распределению $p(z)$, удобно для генерации и интерполяций.
- **Contractive AE / Sparse AE:** поощряют устойчивость/разреженность латентного кода.
- **Tip:** используйте батч-нормализацию и раннюю остановку; подбирайте размер латентного пространства по целевой задаче (кластеризация/поиск ближайших/генерация).

### Компрессия эмбеддингов в продакшне
- **PCA/OPQ (Optimized Product Quantization):** уменьшение размерности + квантование под ANN-поиск.
- **Scalar/Vector Quantization:** храните коды вместо float32; снижает память и ускоряет поиск.
- **Hashing (LSH, SimHash):** быстрый поиск по сходству, особенно для высокого $d$.
- **Low-rank SVD:** $X \approx U_m \Sigma_m V_m^\top$ - храните только топ-$m$ сингулярных компонент.
- **Note:** для индексов ANN рассмотрите FAISS/HNSW; для онлайна - баланс памяти, точности и латентности.

```python
# Пример: уменьшение размерности перед PQ/ANN
from sklearn.decomposition import PCA
X32 = X.astype("float32")
pca = PCA(n_components=64, random_state=42)
Z = pca.fit_transform(X32)  # затем индексируйте Z в ANN (FAISS/HNSWlib)
```

