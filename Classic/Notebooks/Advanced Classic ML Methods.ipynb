{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Классификация в машинном обучении: практическое руководство\n",
        "\n",
        "В этом notebook мы разберём четыре фундаментальных алгоритма классификации:\n",
        "- k-ближайших соседей (k-NN)\n",
        "- Метод опорных векторов (SVM)\n",
        "- Деревья решений\n",
        "- Наивный байес\n",
        "\n",
        "Для каждого алгоритма покажем:\n",
        "- Как работает на синтетических данных\n",
        "- Визуализацию границ решения\n",
        "- Оценку качества\n",
        "- Подбор гиперпараметров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорты\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_moons, load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Алгоритмы\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Настройки визуализации\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "# Фиксируем random seed для воспроизводимости\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 1: k-ближайших соседей (k-NN)\n",
        "\n",
        "### Теория\n",
        "\n",
        "k-NN — один из самых простых алгоритмов машинного обучения. Идея: чтобы классифицировать новый объект, найдём $k$ ближайших к нему точек из обучающей выборки и выберем наиболее частый класс среди них.\n",
        "\n",
        "**Ключевые параметры:**\n",
        "- `n_neighbors` (k): количество соседей\n",
        "- `metric`: метрика расстояния (euclidean, manhattan, etc.)\n",
        "- `weights`: веса соседей (uniform или distance)\n",
        "\n",
        "**Особенности:**\n",
        "- Требует масштабирования признаков\n",
        "- Медленное предсказание на больших данных\n",
        "- Чувствителен к выбору k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создаём синтетические данные\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
        "                          n_informative=2, n_clusters_per_class=1, \n",
        "                          class_sep=1.5, random_state=42)\n",
        "\n",
        "# Разделяем на train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Масштабирование (критически важно для k-NN!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Размер обучающей выборки: {X_train.shape}\")\n",
        "print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
        "print(f\"Распределение классов: {np.bincount(y_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Функция для визуализации границ решения\n",
        "def plot_decision_boundary(X, y, model, title, scaler=None):\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', s=50)\n",
        "    plt.xlabel('Признак 1')\n",
        "    plt.ylabel('Признак 2')\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучаем k-NN с разными k\n",
        "k_values = [1, 3, 5, 15]\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    print(f\"\\nk = {k}\")\n",
        "    print(f\"Точность на тестовой выборке: {accuracy:.3f}\")\n",
        "    \n",
        "    # Визуализация границ решения\n",
        "    plot_decision_boundary(X_train_scaled, y_train, knn, \n",
        "                          f'k-NN (k={k}), accuracy={accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Подбор оптимального k с помощью кросс-валидации\n",
        "k_range = range(1, 31)\n",
        "k_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    k_scores.append(scores.mean())\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(k_range, k_scores, marker='o')\n",
        "plt.xlabel('Значение k')\n",
        "plt.ylabel('Средняя точность (5-fold CV)')\n",
        "plt.title('Выбор оптимального k (Elbow method)')\n",
        "plt.axvline(x=k_range[np.argmax(k_scores)], color='r', linestyle='--', \n",
        "           label=f'Оптимальное k={k_range[np.argmax(k_scores)]}')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Оптимальное k: {k_range[np.argmax(k_scores)]}\")\n",
        "print(f\"Максимальная точность: {max(k_scores):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Выводы по k-NN:\n",
        "\n",
        "- При малом k (1-3): модель переобучается, граница сложная\n",
        "- При большом k (15+): модель становится более гладкой\n",
        "- Оптимальное k находится кросс-валидацией\n",
        "- Масштабирование критически важно!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 2: Метод опорных векторов (SVM)\n",
        "\n",
        "### Теория\n",
        "\n",
        "SVM ищет гиперплоскость, которая максимизирует зазор (margin) между классами.\n",
        "\n",
        "**Ключевые параметры:**\n",
        "- `C`: параметр регуляризации (большой C = жёсткий штраф за ошибки)\n",
        "- `kernel`: тип ядра (linear, poly, rbf, sigmoid)\n",
        "- `gamma`: параметр RBF-ядра (малый = гладкая граница)\n",
        "\n",
        "**Особенности:**\n",
        "- Хорошо работает с высокоразмерными данными\n",
        "- Kernel trick позволяет решать нелинейные задачи\n",
        "- Требует масштабирования признаков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создаём нелинейно разделимые данные\n",
        "X_moons, y_moons = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
        "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_moons, y_moons, \n",
        "                                                            test_size=0.3, random_state=42)\n",
        "\n",
        "# Масштабирование\n",
        "scaler_m = StandardScaler()\n",
        "X_train_m_scaled = scaler_m.fit_transform(X_train_m)\n",
        "X_test_m_scaled = scaler_m.transform(X_test_m)\n",
        "\n",
        "# Визуализация данных\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_train_m[:, 0], X_train_m[:, 1], c=y_train_m, cmap='coolwarm', edgecolors='k')\n",
        "plt.title('Нелинейно разделимые данные (две луны)')\n",
        "plt.xlabel('Признак 1')\n",
        "plt.ylabel('Признак 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сравнение разных ядер SVM\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "\n",
        "for kernel in kernels:\n",
        "    if kernel == 'poly':\n",
        "        svm = SVC(kernel=kernel, degree=3, C=1.0, random_state=42)\n",
        "    else:\n",
        "        svm = SVC(kernel=kernel, C=1.0, random_state=42)\n",
        "    \n",
        "    svm.fit(X_train_m_scaled, y_train_m)\n",
        "    y_pred = svm.predict(X_test_m_scaled)\n",
        "    accuracy = accuracy_score(y_test_m, y_pred)\n",
        "    \n",
        "    print(f\"\\nКерnel: {kernel}\")\n",
        "    print(f\"Точность: {accuracy:.3f}\")\n",
        "    print(f\"Количество опорных векторов: {len(svm.support_vectors_)}\")\n",
        "    \n",
        "    # Визуализация\n",
        "    plot_decision_boundary(X_train_m_scaled, y_train_m, svm, \n",
        "                          f'SVM ({kernel} kernel), accuracy={accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Влияние параметра C на RBF-ядро\n",
        "C_values = [0.1, 1.0, 10.0, 100.0]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, C in enumerate(C_values):\n",
        "    svm = SVC(kernel='rbf', C=C, gamma='scale', random_state=42)\n",
        "    svm.fit(X_train_m_scaled, y_train_m)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test_m, svm.predict(X_test_m_scaled))\n",
        "    \n",
        "    # Визуализация границ\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_train_m_scaled[:, 0].min() - 1, X_train_m_scaled[:, 0].max() + 1\n",
        "    y_min, y_max = X_train_m_scaled[:, 1].min() - 1, X_train_m_scaled[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "    axes[idx].scatter(X_train_m_scaled[:, 0], X_train_m_scaled[:, 1], \n",
        "                     c=y_train_m, cmap='coolwarm', edgecolors='k')\n",
        "    axes[idx].set_title(f'C={C}, accuracy={accuracy:.3f}\\nОпорных векторов: {len(svm.support_vectors_)}')\n",
        "    axes[idx].set_xlabel('Признак 1')\n",
        "    axes[idx].set_ylabel('Признак 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid Search для подбора гиперпараметров\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train_m_scaled, y_train_m)\n",
        "\n",
        "print(f\"Лучшие параметры: {grid_search.best_params_}\")\n",
        "print(f\"Лучшая точность (CV): {grid_search.best_score_:.3f}\")\n",
        "\n",
        "# Оценка на тестовой выборке\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred_best = best_svm.predict(X_test_m_scaled)\n",
        "print(f\"Точность на тесте: {accuracy_score(y_test_m, y_pred_best):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Выводы по SVM:\n",
        "\n",
        "- Линейное ядро плохо работает на нелинейных данных\n",
        "- RBF-ядро универсально и часто лучший выбор\n",
        "- Малый C: гладкая граница, риск недообучения\n",
        "- Большой C: сложная граница, риск переобучения\n",
        "- Grid Search критически важен для хороших результатов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 3: Деревья решений\n",
        "\n",
        "### Теория\n",
        "\n",
        "Дерево решений — последовательность условий (if-then), которые разделяют данные на классы.\n",
        "\n",
        "**Ключевые параметры:**\n",
        "- `max_depth`: максимальная глубина дерева\n",
        "- `min_samples_split`: минимум примеров для разбиения узла\n",
        "- `min_samples_leaf`: минимум примеров в листовом узле\n",
        "- `criterion`: критерий разбиения (gini или entropy)\n",
        "\n",
        "**Особенности:**\n",
        "- Очень интерпретируемы\n",
        "- Не требуют масштабирования\n",
        "- Склонны к переобучению без ограничений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Используем датасет Iris\n",
        "iris = load_iris()\n",
        "X_iris, y_iris = iris.data[:, :2], iris.target  # берём только первые 2 признака для визуализации\n",
        "\n",
        "# Оставим только 2 класса для упрощения\n",
        "mask = y_iris != 2\n",
        "X_iris, y_iris = X_iris[mask], y_iris[mask]\n",
        "\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Признаки: {iris.feature_names[:2]}\")\n",
        "print(f\"Классы: {[iris.target_names[i] for i in [0, 1]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Деревья с разной глубиной\n",
        "depths = [1, 2, 3, 10]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, depth in enumerate(depths):\n",
        "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    tree.fit(X_train_iris, y_train_iris)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test_iris, tree.predict(X_test_iris))\n",
        "    \n",
        "    # Визуализация границ\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_train_iris[:, 0].min() - 0.5, X_train_iris[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_train_iris[:, 1].min() - 0.5, X_train_iris[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "    axes[idx].scatter(X_train_iris[:, 0], X_train_iris[:, 1], \n",
        "                     c=y_train_iris, cmap='coolwarm', edgecolors='k')\n",
        "    axes[idx].set_title(f'max_depth={depth}, accuracy={accuracy:.3f}')\n",
        "    axes[idx].set_xlabel(iris.feature_names[0])\n",
        "    axes[idx].set_ylabel(iris.feature_names[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация самого дерева\n",
        "tree_best = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_best.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(tree_best, feature_names=iris.feature_names[:2], \n",
        "         class_names=[iris.target_names[i] for i in [0, 1]], \n",
        "         filled=True, rounded=True, fontsize=12)\n",
        "plt.title('Визуализация дерева решений (max_depth=3)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Вычисление Gini impurity вручную\n",
        "def gini_impurity(y):\n",
        "    \"\"\"Вычисляет Gini impurity для массива меток\"\"\"\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / len(y)\n",
        "    gini = 1 - np.sum(probabilities ** 2)\n",
        "    return gini\n",
        "\n",
        "# Пример\n",
        "print(\"Примеры вычисления Gini impurity:\\n\")\n",
        "print(f\"Чистый узел [0,0,0,0,0]: Gini = {gini_impurity([0,0,0,0,0]):.3f}\")\n",
        "print(f\"Сбалансированный [0,0,1,1]: Gini = {gini_impurity([0,0,1,1]):.3f}\")\n",
        "print(f\"Несбалансированный [0,0,0,1]: Gini = {gini_impurity([0,0,0,1]):.3f}\")\n",
        "print(f\"\\nGini всей выборки: {gini_impurity(y_train_iris):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Важность признаков\n",
        "tree_full = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "tree_full.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "importances = tree_full.feature_importances_\n",
        "feature_names = iris.feature_names[:2]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_names, importances)\n",
        "plt.xlabel('Важность признака')\n",
        "plt.title('Feature Importance в дереве решений')\n",
        "plt.show()\n",
        "\n",
        "for name, imp in zip(feature_names, importances):\n",
        "    print(f\"{name}: {imp:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Выводы по деревьям решений:\n",
        "\n",
        "- max_depth=1: слишком простая модель (недообучение)\n",
        "- max_depth=10: переобучение, сложные границы\n",
        "- max_depth=2-3: оптимальный баланс\n",
        "- Дерево легко интерпретировать\n",
        "- Feature importance показывает важность признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 4: Наивный байес и текстовая классификация\n",
        "\n",
        "### Теория\n",
        "\n",
        "Наивный байес использует теорему Байеса для классификации. Особенно эффективен для текстовой классификации.\n",
        "\n",
        "**Варианты:**\n",
        "- `MultinomialNB`: для подсчёта частот (текст)\n",
        "- `GaussianNB`: для непрерывных признаков\n",
        "- `BernoulliNB`: для бинарных признаков\n",
        "\n",
        "**Особенности:**\n",
        "- Очень быстрый\n",
        "- Хорош для текстов\n",
        "- Предполагает независимость признаков (наивное предположение)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Простой пример: классификация коротких текстов\n",
        "texts = [\n",
        "    \"I love this movie\",\n",
        "    \"This film is great\",\n",
        "    \"Amazing movie, best ever\",\n",
        "    \"Wonderful film experience\",\n",
        "    \"I hate this movie\",\n",
        "    \"Terrible film, worst ever\",\n",
        "    \"Bad movie, waste of time\",\n",
        "    \"Awful film, very boring\"\n",
        "]\n",
        "\n",
        "labels = [1, 1, 1, 1, 0, 0, 0, 0]  # 1 = положительный, 0 = отрицательный\n",
        "\n",
        "# Векторизация текста (Count Vectorization)\n",
        "vectorizer = CountVectorizer()\n",
        "X_text = vectorizer.fit_transform(texts)\n",
        "\n",
        "print(f\"Словарь: {vectorizer.get_feature_names_out()}\")\n",
        "print(\"\\nМатрица признаков (разреженная):\")\n",
        "print(X_text.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучение Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_text, labels)\n",
        "\n",
        "# Предсказания\n",
        "test_texts = [\n",
        "    \"I love this film\",\n",
        "    \"This movie is terrible\",\n",
        "    \"Great experience\"\n",
        "]\n",
        "\n",
        "X_test_text = vectorizer.transform(test_texts)\n",
        "predictions = nb.predict(X_test_text)\n",
        "probs = nb.predict_proba(X_test_text)\n",
        "\n",
        "print(\"Предсказания:\\n\")\n",
        "for text, pred, prob in zip(test_texts, predictions, probs):\n",
        "    sentiment = \"Положительный\" if pred == 1 else \"Отрицательный\"\n",
        "    print(f\"Текст: '{text}'\")\n",
        "    print(f\"Класс: {sentiment}\")\n",
        "    print(f\"Вероятности: Отриц={prob[0]:.3f}, Полож={prob[1]:.3f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Вероятности слов в каждом классе\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "log_probs_neg = nb.feature_log_prob_[0]\n",
        "log_probs_pos = nb.feature_log_prob_[1]\n",
        "\n",
        "print(\"Наиболее вероятные слова для каждого класса:\\n\")\n",
        "\n",
        "# Топ-5 слов для отрицательного класса\n",
        "top_neg = np.argsort(log_probs_neg)[-5:]\n",
        "print(\"Отрицательный класс:\")\n",
        "for idx in top_neg:\n",
        "    print(f\"  {feature_names[idx]}: {np.exp(log_probs_neg[idx]):.3f}\")\n",
        "\n",
        "# Топ-5 слов для положительного класса\n",
        "top_pos = np.argsort(log_probs_pos)[-5:]\n",
        "print(\"\\nПоложительный класс:\")\n",
        "for idx in top_pos:\n",
        "    print(f\"  {feature_names[idx]}: {np.exp(log_probs_pos[idx]):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gaussian Naive Bayes на числовых данных (Iris)\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "y_pred_gnb = gnb.predict(X_test_iris)\n",
        "accuracy_gnb = accuracy_score(y_test_iris, y_pred_gnb)\n",
        "\n",
        "print(\"Gaussian Naive Bayes на Iris:\")\n",
        "print(f\"Точность: {accuracy_gnb:.3f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_iris, y_pred_gnb)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "           xticklabels=[iris.target_names[i] for i in [0,1]], \n",
        "           yticklabels=[iris.target_names[i] for i in [0,1]])\n",
        "plt.title('Confusion Matrix для Gaussian NB')\n",
        "plt.ylabel('Истинный класс')\n",
        "plt.xlabel('Предсказанный класс')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Выводы по Naive Bayes:\n",
        "\n",
        "- Очень быстрый и простой алгоритм\n",
        "- MultinomialNB отлично подходит для текстов\n",
        "- GaussianNB для непрерывных признаков\n",
        "- Можно интерпретировать вероятности слов\n",
        "- Несмотря на наивное предположение, работает хорошо"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 5: Сравнение всех алгоритмов\n",
        "\n",
        "Давайте сравним все 4 алгоритма на одном датасете."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Используем исходные данные\n",
        "models = {\n",
        "    'k-NN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
        "    'SVM (RBF)': SVC(kernel='rbf', C=1.0, gamma='scale'),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'Gaussian NB': GaussianNB()\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # k-NN и SVM требуют масштабирования\n",
        "    if name in ['k-NN (k=5)', 'SVM (RBF)']:\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append({'Алгоритм': name, 'Точность': accuracy})\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Точность: {accuracy:.3f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Класс 0', 'Класс 1']))\n",
        "\n",
        "# Визуализация сравнения\n",
        "results_df = pd.DataFrame(results)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(results_df['Алгоритм'], results_df['Точность'])\n",
        "plt.xlabel('Точность')\n",
        "plt.title('Сравнение алгоритмов классификации')\n",
        "plt.xlim(0, 1)\n",
        "for i, v in enumerate(results_df['Точность']):\n",
        "    plt.text(v + 0.01, i, f\"{v:.3f}\", va='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Итоговые выводы\n",
        "\n",
        "### Когда использовать каждый алгоритм:\n",
        "\n",
        "**k-NN:**\n",
        "- ✅ Малые датасеты\n",
        "- ✅ Простая интерпретация\n",
        "- ✅ Локальные зависимости\n",
        "- ❌ Большие данные (медленно)\n",
        "- ❌ Высокоразмерные данные\n",
        "\n",
        "**SVM:**\n",
        "- ✅ Высокоразмерные данные\n",
        "- ✅ Чёткое разделение классов\n",
        "- ✅ Нелинейные задачи (RBF kernel)\n",
        "- ❌ Большие датасеты (медленное обучение)\n",
        "- ❌ Трудная интерпретация\n",
        "\n",
        "**Decision Trees:**\n",
        "- ✅ Интерпретируемость\n",
        "- ✅ Категориальные признаки\n",
        "- ✅ Нелинейные взаимодействия\n",
        "- ❌ Склонность к переобучению\n",
        "- ❌ Нестабильность (малые изменения данных → разные деревья)\n",
        "\n",
        "**Naive Bayes:**\n",
        "- ✅ Текстовая классификация\n",
        "- ✅ Очень быстрый\n",
        "- ✅ Малые данные\n",
        "- ❌ Предположение независимости\n",
        "- ❌ Непрерывные признаки с корреляциями\n",
        "\n",
        "### Практические советы:\n",
        "\n",
        "1. **Всегда начинайте с простого**: baseline модель (Naive Bayes или Decision Tree)\n",
        "2. **Масштабируйте данные** для k-NN и SVM\n",
        "3. **Используйте кросс-валидацию** для оценки\n",
        "4. **Grid Search** для подбора гиперпараметров\n",
        "6. **Смотрите на разные метрики**: не только accuracy, но и precision, recall, F1\n",
        "7. **Визуализируйте** границы решения и confusion matrix"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
