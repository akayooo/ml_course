{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Регрессия: «своими руками» и с помощью библиотек\n",
        "\n",
        "В этом ноутбуке вы:\n",
        "1. Сгенерируете синтетические данные с линейной зависимостью.\n",
        "2. Оцените линейную регрессию **с нуля**:\n",
        "   - нормальное уравнение (аналитическое решение);\n",
        "   - градиентный спуск.\n",
        "3. Построите регрессию **с помощью библиотек** (`scikit-learn`).\n",
        "4. Попробуете **полиномиальную регрессию** и регуляризацию **Ridge**.\n",
        "5. Сравните метрики (MSE, R²) на train/test и визуализируете результаты.\n",
        "\n",
        "> Примечания по графикам: используем только `matplotlib`, каждый график — отдельный рисунок, без заданных цветов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Базовые импорты\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Настройки печати\n",
        "np.set_printoptions(suppress=True, precision=4)\n",
        "pd.set_option(\"display.float_format\", lambda v: f\"{v:.4f}\")\n",
        "\n",
        "# Функции метрик\n",
        "def mse(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def r2_score(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true).reshape(-1)\n",
        "    y_pred = np.asarray(y_pred).reshape(-1)\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return 1 - ss_res / ss_tot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1) Синтетические данные\n",
        "\n",
        "Создадим данные по формуле:\n",
        "\n",
        "$\n",
        "y = 3x + 2 + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\,1)\n",
        "$\n",
        "\n",
        "А затем разделим на train/test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "rng = np.random.default_rng(42)\n",
        "n = 120\n",
        "X = rng.uniform(-3, 3, size=(n, 1))\n",
        "noise = rng.normal(0, 1, size=(n, 1))\n",
        "y = 3 * X + 2 + noise\n",
        "\n",
        "# Train/Test split\n",
        "idx = np.arange(n)\n",
        "rng.shuffle(idx)\n",
        "train_size = int(0.75 * n)\n",
        "train_idx, test_idx = idx[:train_size], idx[train_size:]\n",
        "\n",
        "X_train, X_test = X[train_idx], X[test_idx]\n",
        "y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "\n",
        "# Быстрый scatter\n",
        "plt.figure()\n",
        "plt.scatter(X_train, y_train, label=\"train\")\n",
        "plt.title(\"Данные (train)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2) Линейная регрессия «с нуля»: нормальное уравнение\n",
        "\n",
        "Добавим столбец единиц и решим $(\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def add_intercept(X):\n",
        "    X = np.asarray(X)\n",
        "    return np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "\n",
        "Xb_train = add_intercept(X_train)\n",
        "Xb_test  = add_intercept(X_test)\n",
        "\n",
        "# Закрытая форма (псевдообратная на случай вырожденности)\n",
        "beta_hat = np.linalg.pinv(Xb_train.T @ Xb_train) @ (Xb_train.T @ y_train)\n",
        "\n",
        "y_pred_train = Xb_train @ beta_hat\n",
        "y_pred_test  = Xb_test  @ beta_hat\n",
        "\n",
        "print(\"Оценки коэффициентов [beta0, beta1]:\", beta_hat.reshape(-1))\n",
        "\n",
        "print(\"MSE train:\", mse(y_train, y_pred_train))\n",
        "print(\"MSE test :\", mse(y_test, y_pred_test))\n",
        "print(\"R^2  train:\", r2_score(y_train, y_pred_train))\n",
        "print(\"R^2  test :\", r2_score(y_test, y_pred_test))\n",
        "\n",
        "# Визуализация прямой\n",
        "xs = np.linspace(X.min(), X.max(), 200).reshape(-1,1)\n",
        "ys = add_intercept(xs) @ beta_hat\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_train, y_train, label=\"train\")\n",
        "plt.plot(xs, ys, label=\"fit\")\n",
        "plt.title(\"Линейная регрессия (нормальное уравнение)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3) Линейная регрессия «с нуля»: градиентный спуск\n",
        "\n",
        "Минимизируем $J(\\beta)=\\frac{1}{n}\\sum (y-\\hat{y})^2$ по $\\beta$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def gradient_descent_linreg(X, y, lr=0.05, epochs=1000):\n",
        "    Xb = add_intercept(X)\n",
        "    y = y.reshape(-1, 1)\n",
        "    n, d = Xb.shape\n",
        "    beta = np.zeros((d, 1))\n",
        "    history = []\n",
        "    for t in range(epochs):\n",
        "        y_hat = Xb @ beta\n",
        "        grad = (-2.0 / n) * (Xb.T @ (y - y_hat))\n",
        "        beta -= lr * grad\n",
        "        if t % 50 == 0 or t == epochs - 1:\n",
        "            history.append(float(mse(y, y_hat)))\n",
        "    return beta, history\n",
        "\n",
        "beta_gd, hist = gradient_descent_linreg(X_train, y_train, lr=0.05, epochs=1000)\n",
        "\n",
        "y_pred_train_gd = add_intercept(X_train) @ beta_gd\n",
        "y_pred_test_gd  = add_intercept(X_test)  @ beta_gd\n",
        "\n",
        "print(\"GD коэффициенты [beta0, beta1]:\", beta_gd.reshape(-1))\n",
        "print(\"MSE train:\", mse(y_train, y_pred_train_gd))\n",
        "print(\"MSE test :\", mse(y_test, y_pred_test_gd))\n",
        "print(\"R^2  train:\", r2_score(y_train, y_pred_train_gd))\n",
        "print(\"R^2  test :\", r2_score(y_test, y_pred_test_gd))\n",
        "\n",
        "# Кривая обучения (значение функции потерь по эпохам)\n",
        "plt.figure()\n",
        "plt.plot(np.arange(len(hist)) * 50, hist)\n",
        "plt.title(\"Градиентный спуск: убывание MSE (каждые 50 эпох)\")\n",
        "plt.xlabel(\"эпоха\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.show()\n",
        "\n",
        "# Линия регрессии после GD\n",
        "xs = np.linspace(X.min(), X.max(), 200).reshape(-1,1)\n",
        "ys_gd = add_intercept(xs) @ beta_gd\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_train, y_train, label=\"train\")\n",
        "plt.plot(xs, ys_gd, label=\"fit\")\n",
        "plt.title(\"Линейная регрессия (градиентный спуск)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 4) Регрессия с библиотеками (`scikit-learn`)\n",
        "\n",
        "Используем `LinearRegression` и `Ridge`. Затем сравним метрики.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_tr_lr = lr.predict(X_train)\n",
        "y_te_lr = lr.predict(X_test)\n",
        "\n",
        "ridge = Ridge(alpha=1.0, fit_intercept=True, random_state=42)\n",
        "ridge.fit(X_train, y_train)\n",
        "y_tr_rg = ridge.predict(X_train)\n",
        "y_te_rg = ridge.predict(X_test)\n",
        "\n",
        "import pandas as pd\n",
        "res = pd.DataFrame({\n",
        "    \"model\": [\"LinearRegression\", \"Ridge(alpha=1)\"],\n",
        "    \"MSE_train\": [mse(y_train, y_tr_lr), mse(y_train, y_tr_rg)],\n",
        "    \"MSE_test\":  [mse(y_test,  y_te_lr), mse(y_test,  y_te_rg)],\n",
        "    \"R2_train\":  [r2_score(y_train, y_tr_lr), r2_score(y_train, y_tr_rg)],\n",
        "    \"R2_test\":   [r2_score(y_test,  y_te_lr), r2_score(y_test,  y_te_rg)],\n",
        "})\n",
        "res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Выводы\n",
        "- **С нуля**: нормальное уравнение даёт ту же прямую, что и `LinearRegression`, при хорошей обусловленности;\n",
        "- **Градиентный спуск** сходится к тем же коэффициентам при адекватной скорости обучения и числе эпох;\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
