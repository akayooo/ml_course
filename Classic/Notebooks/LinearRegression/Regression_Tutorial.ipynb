{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n# Регрессия: «своими руками» и с помощью библиотек\n\nВ этом ноутбуке вы:\n1. Сгенерируете синтетические данные с линейной зависимостью.\n2. Оцените линейную регрессию **с нуля**:\n   - нормальное уравнение (аналитическое решение);\n   - градиентный спуск.\n3. Построите регрессию **с помощью библиотек** (`scikit-learn`).\n4. Попробуете **полиномиальную регрессию** и регуляризацию **Ridge**.\n5. Сравните метрики (MSE, R²) на train/test и визуализируете результаты.\n\n> Примечания по графикам: используем только `matplotlib`, каждый график — отдельный рисунок, без заданных цветов.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Базовые импорты\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Настройки печати\nnp.set_printoptions(suppress=True, precision=4)\npd.set_option(\"display.float_format\", lambda v: f\"{v:.4f}\")\n\n# Функции метрик\ndef mse(y_true, y_pred):\n    y_true = np.asarray(y_true).reshape(-1)\n    y_pred = np.asarray(y_pred).reshape(-1)\n    return np.mean((y_true - y_pred) ** 2)\n\ndef r2_score(y_true, y_pred):\n    y_true = np.asarray(y_true).reshape(-1)\n    y_pred = np.asarray(y_pred).reshape(-1)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    return 1 - ss_res / ss_tot\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 1) Синтетические данные\n\nСоздадим данные по формуле:  \n\\[\ny = 3x + 2 + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\,1)\n\\]\n\nА затем разделим на train/test.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nrng = np.random.default_rng(42)\nn = 120\nX = rng.uniform(-3, 3, size=(n, 1))\nnoise = rng.normal(0, 1, size=(n, 1))\ny = 3 * X + 2 + noise\n\n# Train/Test split\nidx = np.arange(n)\nrng.shuffle(idx)\ntrain_size = int(0.75 * n)\ntrain_idx, test_idx = idx[:train_size], idx[train_size:]\n\nX_train, X_test = X[train_idx], X[test_idx]\ny_train, y_test = y[train_idx], y[test_idx]\n\nprint(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n\n# Быстрый scatter\nplt.figure()\nplt.scatter(X_train, y_train, label=\"train\")\nplt.title(\"Данные (train)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 2) Линейная регрессия «с нуля»: нормальное уравнение\n\nДобавим столбец единиц и решим \\(\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\\).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef add_intercept(X):\n    X = np.asarray(X)\n    return np.hstack([np.ones((X.shape[0], 1)), X])\n\nXb_train = add_intercept(X_train)\nXb_test  = add_intercept(X_test)\n\n# Закрытая форма (псевдообратная на случай вырожденности)\nbeta_hat = np.linalg.pinv(Xb_train.T @ Xb_train) @ (Xb_train.T @ y_train)\n\ny_pred_train = Xb_train @ beta_hat\ny_pred_test  = Xb_test  @ beta_hat\n\nprint(\"Оценки коэффициентов [beta0, beta1]:\", beta_hat.reshape(-1))\n\nprint(\"MSE train:\", mse(y_train, y_pred_train))\nprint(\"MSE test :\", mse(y_test, y_pred_test))\nprint(\"R^2  train:\", r2_score(y_train, y_pred_train))\nprint(\"R^2  test :\", r2_score(y_test, y_pred_test))\n\n# Визуализация прямой\nxs = np.linspace(X.min(), X.max(), 200).reshape(-1,1)\nys = add_intercept(xs) @ beta_hat\n\nplt.figure()\nplt.scatter(X_train, y_train, label=\"train\")\nplt.plot(xs, ys, label=\"fit\")\nplt.title(\"Линейная регрессия (нормальное уравнение)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 3) Линейная регрессия «с нуля»: градиентный спуск\n\nМинимизируем \\(J(\\beta)=\\frac{1}{n}\\sum (y-\\hat{y})^2\\) по \\(\\beta\\).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef gradient_descent_linreg(X, y, lr=0.05, epochs=1000):\n    Xb = add_intercept(X)\n    y = y.reshape(-1, 1)\n    n, d = Xb.shape\n    beta = np.zeros((d, 1))\n    history = []\n    for t in range(epochs):\n        y_hat = Xb @ beta\n        grad = (-2.0 / n) * (Xb.T @ (y - y_hat))\n        beta -= lr * grad\n        if t % 50 == 0 or t == epochs - 1:\n            history.append(float(mse(y, y_hat)))\n    return beta, history\n\nbeta_gd, hist = gradient_descent_linreg(X_train, y_train, lr=0.05, epochs=1000)\n\ny_pred_train_gd = add_intercept(X_train) @ beta_gd\ny_pred_test_gd  = add_intercept(X_test)  @ beta_gd\n\nprint(\"GD коэффициенты [beta0, beta1]:\", beta_gd.reshape(-1))\nprint(\"MSE train:\", mse(y_train, y_pred_train_gd))\nprint(\"MSE test :\", mse(y_test, y_pred_test_gd))\nprint(\"R^2  train:\", r2_score(y_train, y_pred_train_gd))\nprint(\"R^2  test :\", r2_score(y_test, y_pred_test_gd))\n\n# Кривая обучения (значение функции потерь по эпохам)\nplt.figure()\nplt.plot(np.arange(len(hist)) * 50, hist)\nplt.title(\"Градиентный спуск: убывание MSE (каждые 50 эпох)\")\nplt.xlabel(\"эпоха\")\nplt.ylabel(\"MSE\")\nplt.show()\n\n# Линия регрессии после GD\nxs = np.linspace(X.min(), X.max(), 200).reshape(-1,1)\nys_gd = add_intercept(xs) @ beta_gd\n\nplt.figure()\nplt.scatter(X_train, y_train, label=\"train\")\nplt.plot(xs, ys_gd, label=\"fit\")\nplt.title(\"Линейная регрессия (градиентный спуск)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 4) Регрессия с библиотеками (`scikit-learn`)\n\nИспользуем `LinearRegression` и `Ridge`. Затем сравним метрики.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_tr_lr = lr.predict(X_train)\ny_te_lr = lr.predict(X_test)\n\nridge = Ridge(alpha=1.0, fit_intercept=True, random_state=42)\nridge.fit(X_train, y_train)\ny_tr_rg = ridge.predict(X_train)\ny_te_rg = ridge.predict(X_test)\n\nimport pandas as pd\nres = pd.DataFrame({\n    \"model\": [\"LinearRegression\", \"Ridge(alpha=1)\"],\n    \"MSE_train\": [mse(y_train, y_tr_lr), mse(y_train, y_tr_rg)],\n    \"MSE_test\":  [mse(y_test,  y_te_lr), mse(y_test,  y_te_rg)],\n    \"R2_train\":  [r2_score(y_train, y_tr_lr), r2_score(y_train, y_tr_rg)],\n    \"R2_test\":   [r2_score(y_test,  y_te_lr), r2_score(y_test,  y_te_rg)],\n})\nres\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 5) Полиномиальная регрессия и переобучение\n\nСконструируем признаки \\(x, x^2, \\dots, x^d\\) и сравним качества при разных степенях \\(d\\).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef evaluate_poly(degree):\n    pipe = Pipeline([\n        (\"poly\", PolynomialFeatures(degree=degree, include_bias=True)),\n        (\"lr\", LinearRegression())\n    ])\n    pipe.fit(X_train, y_train)\n    y_tr = pipe.predict(X_train)\n    y_te = pipe.predict(X_test)\n    return {\n        \"degree\": degree,\n        \"MSE_train\": mse(y_train, y_tr),\n        \"MSE_test\": mse(y_test, y_te),\n        \"R2_train\": r2_score(y_train, y_tr),\n        \"R2_test\": r2_score(y_test, y_te),\n        \"model\": pipe\n    }\n\nrows = [evaluate_poly(d) for d in range(1, 10)]\ndf_poly = pd.DataFrame(rows)[[\"degree\",\"MSE_train\",\"MSE_test\",\"R2_train\",\"R2_test\"]]\ndf_poly\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Визуализация для пары степеней\ndeg_list = [1, 3, 8]\nxs = np.linspace(X.min(), X.max(), 400).reshape(-1,1)\n\nfor d in deg_list:\n    pipe = Pipeline([\n        (\"poly\", PolynomialFeatures(degree=d, include_bias=True)),\n        (\"lr\", LinearRegression())\n    ])\n    pipe.fit(X_train, y_train)\n    ys_hat = pipe.predict(xs)\n\n    plt.figure()\n    plt.scatter(X_train, y_train, label=\"train\")\n    plt.plot(xs, ys_hat, label=f\"degree={d}\")\n    plt.title(f\"Полиномиальная регрессия (степень {d})\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.legend()\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 6) Ridge-регуляризация в полиномиальной регрессии\n\nС ростом степени возможен взрыв дисперсии (переобучение). Регуляризация стабилизирует веса.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef evaluate_poly_ridge(degree, alpha):\n    pipe = Pipeline([\n        (\"poly\", PolynomialFeatures(degree=degree, include_bias=True)),\n        (\"ridge\", Ridge(alpha=alpha, fit_intercept=True, random_state=42))\n    ])\n    pipe.fit(X_train, y_train)\n    y_tr = pipe.predict(X_train)\n    y_te = pipe.predict(X_test)\n    return {\n        \"degree\": degree,\n        \"alpha\": alpha,\n        \"MSE_train\": mse(y_train, y_tr),\n        \"MSE_test\": mse(y_test, y_te),\n        \"R2_train\": r2_score(y_train, y_tr),\n        \"R2_test\": r2_score(y_test, y_te),\n    }\n\nrows = [evaluate_poly_ridge(d, a) for d in [5,8,12] for a in [0.0, 0.1, 1.0, 10.0]]\ndf_ridge = pd.DataFrame(rows)[[\"degree\",\"alpha\",\"MSE_train\",\"MSE_test\",\"R2_train\",\"R2_test\"]]\ndf_ridge\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Визуализация кривых для степени 8 при разных alpha\ndegree = 8\nxs = np.linspace(X.min(), X.max(), 400).reshape(-1,1)\n\nfor alpha in [0.0, 0.1, 1.0, 10.0]:\n    pipe = Pipeline([\n        (\"poly\", PolynomialFeatures(degree=degree, include_bias=True)),\n        (\"ridge\", Ridge(alpha=alpha, fit_intercept=True, random_state=42))\n    ])\n    pipe.fit(X_train, y_train)\n    ys_hat = pipe.predict(xs)\n\n    plt.figure()\n    plt.scatter(X_train, y_train, label=\"train\")\n    plt.plot(xs, ys_hat, label=f\"alpha={alpha}\")\n    plt.title(f\"Полиномиальная регрессия (d={degree}) с Ridge\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.legend()\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## Выводы\n- **С нуля**: нормальное уравнение даёт ту же прямую, что и `LinearRegression`, при хорошей обусловленности;\n- **Градиентный спуск** сходится к тем же коэффициентам при адекватной скорости обучения и числе эпох;\n- **Полиномиальная регрессия** может переобучаться на высоких степенях;\n- **Ridge** помогает контролировать переобучение, особенно на высоких степенях полинома.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}