{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ансамблевые методы: Random Forest, AdaBoost и Gradient Boosting\n",
        "\n",
        "В этом notebook мы разберём три главных ансамблевых метода:\n",
        "- Random Forest (параллельное обучение)\n",
        "- AdaBoost (адаптивный бустинг)\n",
        "- Gradient Boosting (градиентный бустинг)\n",
        "\n",
        "Для каждого метода покажем:\n",
        "- Как работает на синтетических и реальных данных\n",
        "- Влияние гиперпараметров\n",
        "- Важность признаков\n",
        "- Сравнение скорости и точности"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорты\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Ансамбли\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Визуализация\n",
        "import time\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 1: Random Forest\n",
        "\n",
        "### Теория\n",
        "\n",
        "Random Forest использует два механизма для снижения переобучения:\n",
        "1. **Bagging:** каждое дерево обучается на bootstrap выборке\n",
        "2. **Случайные признаки:** на каждом разбиении рассматривается только подмножество признаков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создаём синтетические данные\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                          n_redundant=5, n_classes=2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Размер обучающей выборки: {X_train.shape}\")\n",
        "print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
        "print(f\"Распределение классов: {np.bincount(y_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Влияние количества деревьев (n_estimators)\n",
        "n_trees_range = np.array([1, 5, 10, 25, 50, 100, 200, 500])\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for n_trees in n_trees_range:\n",
        "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    \n",
        "    train_score = rf.score(X_train, y_train)\n",
        "    test_score = rf.score(X_test, y_test)\n",
        "    \n",
        "    train_scores.append(train_score)\n",
        "    test_scores.append(test_score)\n",
        "    \n",
        "    print(f\"n_trees={n_trees:3d}: train={train_score:.3f}, test={test_score:.3f}\")\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(n_trees_range, train_scores, marker='o', label='Обучающая выборка')\n",
        "plt.plot(n_trees_range, test_scores, marker='s', label='Тестовая выборка')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Количество деревьев')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Влияние количества деревьев на Random Forest')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Влияние max_depth на переобучение\n",
        "depths = [1, 2, 3, 5, 10, 20, None]\n",
        "train_scores_depth = []\n",
        "test_scores_depth = []\n",
        "\n",
        "for depth in depths:\n",
        "    rf = RandomForestClassifier(n_estimators=100, max_depth=depth, \n",
        "                              random_state=42, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    \n",
        "    train_score = rf.score(X_train, y_train)\n",
        "    test_score = rf.score(X_test, y_test)\n",
        "    \n",
        "    train_scores_depth.append(train_score)\n",
        "    test_scores_depth.append(test_score)\n",
        "    \n",
        "    depth_label = 'None' if depth is None else str(depth)\n",
        "    print(f\"max_depth={depth_label:4s}: train={train_score:.3f}, test={test_score:.3f}\")\n",
        "\n",
        "# Визуализация\n",
        "depths_str = [str(d) if d is not None else 'None' for d in depths]\n",
        "x_pos = np.arange(len(depths_str))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x_pos, train_scores_depth, marker='o', label='Обучающая выборка')\n",
        "plt.plot(x_pos, test_scores_depth, marker='s', label='Тестовая выборка')\n",
        "plt.xticks(x_pos, depths_str)\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Влияние max_depth на Random Forest (переобучение)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Out-of-Bag (OOB) ошибка\n",
        "rf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_oob.fit(X_train, y_train)\n",
        "\n",
        "oob_score = rf_oob.oob_score_\n",
        "test_score = rf_oob.score(X_test, y_test)\n",
        "\n",
        "print(f\"OOB Score (на обучающей выборке): {oob_score:.3f}\")\n",
        "print(f\"Test Score (на отдельной тестовой): {test_score:.3f}\")\n",
        "print(f\"Разница: {abs(oob_score - test_score):.3f}\")\n",
        "\n",
        "print(\"\\nOOB даёт хорошую оценку обобщающей способности без отдельной тестовой выборки!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance\n",
        "rf_best = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_best.fit(X_train, y_train)\n",
        "\n",
        "importances = rf_best.feature_importances_\n",
        "indices = np.argsort(importances)[-10:]  # Топ-10 признаков\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(range(len(indices)), importances[indices])\n",
        "plt.yticks(range(len(indices)), [f'Feature {i}' for i in indices])\n",
        "plt.xlabel('Важность признака')\n",
        "plt.title('Топ-10 важных признаков в Random Forest')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Сумма важностей: {importances.sum():.3f}\")\n",
        "print(f\"Топ-3 признака: {indices[-3:]} с важностями {importances[indices[-3:]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 2: AdaBoost\n",
        "\n",
        "### Теория\n",
        "\n",
        "AdaBoost использует адаптивные веса: примеры, которые сложно классифицировать, получают больший вес на следующей итерации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сравнение AdaBoost с базовой моделью\n",
        "base_clf = DecisionTreeClassifier(max_depth=1)  # \"пень\" (weak learner)\n",
        "\n",
        "adaboost = AdaBoostClassifier(estimator=base_clf, n_estimators=100, \n",
        "                             random_state=42)\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "base_score = base_clf.fit(X_train, y_train).score(X_test, y_test)\n",
        "ada_score = adaboost.score(X_test, y_test)\n",
        "\n",
        "print(f\"Базовая модель (пень): {base_score:.3f}\")\n",
        "print(f\"AdaBoost (100 пней): {ada_score:.3f}\")\n",
        "print(f\"Улучшение: {(ada_score - base_score):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Влияние количества итераций на AdaBoost\n",
        "n_estimators_range = np.array([1, 5, 10, 20, 50, 100, 200])\n",
        "ada_train_scores = []\n",
        "ada_test_scores = []\n",
        "\n",
        "for n_est in n_estimators_range:\n",
        "    base_clf = DecisionTreeClassifier(max_depth=1)\n",
        "    ada = AdaBoostClassifier(estimator=base_clf, n_estimators=n_est, \n",
        "                           random_state=42)\n",
        "    ada.fit(X_train, y_train)\n",
        "    \n",
        "    train_score = ada.score(X_train, y_train)\n",
        "    test_score = ada.score(X_test, y_test)\n",
        "    \n",
        "    ada_train_scores.append(train_score)\n",
        "    ada_test_scores.append(test_score)\n",
        "    \n",
        "    print(f\"n_est={n_est:3d}: train={train_score:.3f}, test={test_score:.3f}\")\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(n_estimators_range, ada_train_scores, marker='o', label='Обучающая выборка')\n",
        "plt.plot(n_estimators_range, ada_test_scores, marker='s', label='Тестовая выборка')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Количество итераций')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Влияние n_estimators на AdaBoost')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance в AdaBoost\n",
        "base_clf = DecisionTreeClassifier(max_depth=1)\n",
        "ada_fi = AdaBoostClassifier(estimator=base_clf, n_estimators=100, random_state=42)\n",
        "ada_fi.fit(X_train, y_train)\n",
        "\n",
        "importances_ada = ada_fi.feature_importances_\n",
        "indices_ada = np.argsort(importances_ada)[-10:]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(range(len(indices_ada)), importances_ada[indices_ada])\n",
        "plt.yticks(range(len(indices_ada)), [f'Feature {i}' for i in indices_ada])\n",
        "plt.xlabel('Важность признака')\n",
        "plt.title('Топ-10 важных признаков в AdaBoost')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 3: Gradient Boosting\n",
        "\n",
        "### Теория\n",
        "\n",
        "Gradient Boosting каждое новое дерево учится предсказывать остатки (residuals) предыдущей модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Базовое Gradient Boosting\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
        "                              max_depth=3, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "gb_train_score = gb.score(X_train, y_train)\n",
        "gb_test_score = gb.score(X_test, y_test)\n",
        "\n",
        "print(\"Gradient Boosting:\")\n",
        "print(f\"Train accuracy: {gb_train_score:.3f}\")\n",
        "print(f\"Test accuracy: {gb_test_score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Влияние learning_rate\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
        "gb_lr_train = []\n",
        "gb_lr_test = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=lr, \n",
        "                                  max_depth=3, random_state=42)\n",
        "    gb.fit(X_train, y_train)\n",
        "    \n",
        "    train_score = gb.score(X_train, y_train)\n",
        "    test_score = gb.score(X_test, y_test)\n",
        "    \n",
        "    gb_lr_train.append(train_score)\n",
        "    gb_lr_test.append(test_score)\n",
        "    \n",
        "    print(f\"lr={lr:.3f}: train={train_score:.3f}, test={test_score:.3f}\")\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(learning_rates, gb_lr_train, marker='o', label='Обучающая выборка')\n",
        "plt.plot(learning_rates, gb_lr_test, marker='s', label='Тестовая выборка')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Влияние learning_rate на Gradient Boosting')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Влияние max_depth (глубина деревьев)\n",
        "depths_gb = [1, 2, 3, 4, 5, 8, 10]\n",
        "gb_depth_train = []\n",
        "gb_depth_test = []\n",
        "\n",
        "for depth in depths_gb:\n",
        "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
        "                                  max_depth=depth, random_state=42)\n",
        "    gb.fit(X_train, y_train)\n",
        "    \n",
        "    train_score = gb.score(X_train, y_train)\n",
        "    test_score = gb.score(X_test, y_test)\n",
        "    \n",
        "    gb_depth_train.append(train_score)\n",
        "    gb_depth_test.append(test_score)\n",
        "    \n",
        "    print(f\"depth={depth:2d}: train={train_score:.3f}, test={test_score:.3f}\")\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(depths_gb, gb_depth_train, marker='o', label='Обучающая выборка')\n",
        "plt.plot(depths_gb, gb_depth_test, marker='s', label='Тестовая выборка')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Влияние max_depth на Gradient Boosting')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Кривая обучения: как ошибка меняется с каждой итерацией\n",
        "gb_learn = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, \n",
        "                                     max_depth=3, random_state=42)\n",
        "gb_learn.fit(X_train, y_train)\n",
        "\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for i, y_pred in enumerate(gb_learn.staged_predict(X_train)):\n",
        "    train_error = 1 - accuracy_score(y_train, y_pred)\n",
        "    train_errors.append(train_error)\n",
        "\n",
        "for i, y_pred in enumerate(gb_learn.staged_predict(X_test)):\n",
        "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
        "    test_errors.append(test_error)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_errors, label='Ошибка на обучении', alpha=0.7)\n",
        "plt.plot(test_errors, label='Ошибка на тесте', alpha=0.7)\n",
        "plt.xlabel('Номер итерации (дерева)')\n",
        "plt.ylabel('Ошибка (1 - Accuracy)')\n",
        "plt.title('Кривая обучения Gradient Boosting')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 4: Сравнение всех трёх методов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Обучение всех моделей\n",
        "models = {\n",
        "    'Single Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'AdaBoost': AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                                 n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n",
        "                                                   max_depth=3, random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Время обучения\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start\n",
        "    \n",
        "    # Время предсказания\n",
        "    start = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    pred_time = time.time() - start\n",
        "    \n",
        "    # Метрики\n",
        "    train_acc = model.score(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Train Acc': train_acc,\n",
        "        'Test Acc': test_acc,\n",
        "        'Overfit': train_acc - test_acc,\n",
        "        'Train Time (s)': train_time,\n",
        "        'Pred Time (ms)': pred_time * 1000\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train accuracy: {train_acc:.3f}\")\n",
        "    print(f\"  Test accuracy: {test_acc:.3f}\")\n",
        "    print(f\"  Overfit gap: {train_acc - test_acc:.3f}\")\n",
        "    print(f\"  Train time: {train_time:.3f}s\")\n",
        "    print(f\"  Pred time: {pred_time*1000:.2f}ms\")\n",
        "\n",
        "results_df = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация сравнения\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Accuracy\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.35\n",
        "axes[0, 0].bar(x - width/2, results_df['Train Acc'], width, label='Train', alpha=0.8)\n",
        "axes[0, 0].bar(x + width/2, results_df['Test Acc'], width, label='Test', alpha=0.8)\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_title('Точность моделей')\n",
        "axes[0, 0].set_xticks(x)\n",
        "axes[0, 0].set_xticklabels(results_df['Model'], rotation=45)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Overfit gap\n",
        "axes[0, 1].bar(results_df['Model'], results_df['Overfit'], color='salmon')\n",
        "axes[0, 1].set_ylabel('Train Acc - Test Acc')\n",
        "axes[0, 1].set_title('Переобучение (зазор между train/test)')\n",
        "axes[0, 1].set_xticklabels(results_df['Model'], rotation=45)\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Время обучения\n",
        "axes[1, 0].bar(results_df['Model'], results_df['Train Time (s)'], color='skyblue')\n",
        "axes[1, 0].set_ylabel('Время (сек)')\n",
        "axes[1, 0].set_title('Время обучения')\n",
        "axes[1, 0].set_xticklabels(results_df['Model'], rotation=45)\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Время предсказания\n",
        "axes[1, 1].bar(results_df['Model'], results_df['Pred Time (ms)'], color='lightgreen')\n",
        "axes[1, 1].set_ylabel('Время (мс)')\n",
        "axes[1, 1].set_title('Время предсказания')\n",
        "axes[1, 1].set_xticklabels(results_df['Model'], rotation=45)\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nСводная таблица:\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Часть 5: Grid Search для оптимизации гиперпараметров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid Search для Gradient Boosting\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [2, 3, 4],\n",
        "    'subsample': [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "print(\"Начинаем Grid Search для Gradient Boosting...\")\n",
        "print(f\"Всего комбинаций: {np.prod([len(v) for v in param_grid_gb.values()])}\")\n",
        "\n",
        "grid_search = GridSearchCV(GradientBoostingClassifier(random_state=42),\n",
        "                         param_grid_gb, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "start = time.time()\n",
        "grid_search.fit(X_train, y_train)\n",
        "search_time = time.time() - start\n",
        "\n",
        "print(f\"\\nЛучшие параметры: {grid_search.best_params_}\")\n",
        "print(f\"Лучшая CV точность: {grid_search.best_score_:.3f}\")\n",
        "\n",
        "best_gb = grid_search.best_estimator_\n",
        "test_score = best_gb.score(X_test, y_test)\n",
        "print(f\"Тестовая точность: {test_score:.3f}\")\n",
        "print(f\"Время Grid Search: {search_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Анализ результатов Grid Search\n",
        "results_cv = pd.DataFrame(grid_search.cv_results_)\n",
        "results_cv['rank_test_score'] = results_cv['rank_test_score']\n",
        "\n",
        "# Топ-5 комбинаций\n",
        "top5 = results_cv.nsmallest(5, 'rank_test_score')[['param_n_estimators', 'param_learning_rate', \n",
        "                                                   'param_max_depth', 'param_subsample', \n",
        "                                                   'mean_test_score']]\n",
        "print(\"Топ-5 комбинаций гиперпараметров:\")\n",
        "print(top5.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Итоговые выводы\n",
        "\n",
        "### Когда использовать каждый метод:\n",
        "\n",
        "**Random Forest:**\n",
        "- ✅ Быстро обучается\n",
        "- ✅ Хорошие результаты без подбора\n",
        "- ✅ Легко интерпретировать (Feature Importance)\n",
        "- ❌ Может быть медленнее на очень больших данных\n",
        "\n",
        "**AdaBoost:**\n",
        "- ✅ Исторически важна (основа современного бустинга)\n",
        "- ✅ Хороша на чистых данных\n",
        "- ✅ Часто работает из коробки\n",
        "- ❌ Чувствительна к выбросам\n",
        "- ❌ Последовательная (не параллелизируется)\n",
        "\n",
        "**Gradient Boosting:**\n",
        "- ✅ Обычно лучшая точность\n",
        "- ✅ Early stopping предотвращает переобучение\n",
        "- ✅ Гибкая (любая дифференцируемая функция потерь)\n",
        "- ❌ Требует подбора гиперпараметров\n",
        "- ❌ Медленнее обучается\n",
        "\n",
        "### Практический совет:\n",
        "\n",
        "1. Начните с Random Forest (быстро, хорошо)\n",
        "2. Если нужна лучше точность → Gradient Boosting\n",
        "3. Используйте Grid Search для оптимизации\n",
        "4. Проверяйте на кросс-валидации, не переобучайтесь!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
